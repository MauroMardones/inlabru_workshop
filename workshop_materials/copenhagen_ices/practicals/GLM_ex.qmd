---
title: "Practical"
format: 
  html:
    theme:
      light: flatly
      dark: darkly
  PrettyPDF-pdf:
    keep-tex: true
    number-sections: true
embed-resources: true
editor_options: 
  chunk_output_type: console
---

## Generalized Linear Model {#sec-genlinmodel}

In this practical we will:

-   Simulate non-Gaussian data
-   Learn how to fit a generalised linear model with `inlabru`
-   Generate predictions from the model

```{r}
#| warning: false
#| message: false
#| echo: false
#| purl: false
library(dplyr)
library(INLA)
library(ggplot2)
library(patchwork)
library(inlabru)     
library(scico)
library(webexercises)
```

A generalised linear model allows for the data likelihood to be non-Gaussian. In this example we have a discrete response variable which we model using a Poisson distribution. Thus, we assume that our data $$
y_i \sim \text{Poisson}(\lambda_i)
$$ with rate parameter $\lambda_i$ which, using a log link, has associated predictor $$
\eta_i = \log \lambda_i = \beta_0 + \beta_1 x_i 
$$ with parameters $\beta_0$ and $\beta_1$, and covariate $x$.

### **Simulate example data**

This code generates 100 samples of covariate `x` and data `y`.

```{r}
#| code-summary: "Simulate Data from a GLM"
set.seed(123)
n = 100
beta = c(1,1)
x = rnorm(n)
lambda = exp(beta[1] + beta[2] * x)
y = rpois(n, lambda  = lambda)
df = data.frame(y = y, x = x)  
```

### Fitting a GLM in `inlabru`

------------------------------------------------------------------------

**Define model components**

The predictor here only contains only 2 components (Intercept and Slope).


:::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}}Task

Define an object called `cmp` that includes and (i) intercept `beta_0` and (ii) a covariate `x` linear effect `beta_1`.


```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false

cmp =  ~ -1 + beta_0(1) + beta_1(x, model = "linear")

```
::::

**Define linear predictor**

:::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}}Task

Define a linear predictor `eta` using the component labels you have defined on the previous task.


```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false

eta = y ~ beta_0 + beta_1

```
::::



**Build observational model**


When building the observation model likelihood we must now specify the Poisson likelihood using the `family` argument (the default link function for this family is the $\log$ link).

```{r}
#| code-summary: "GLM likelihood"

lik =  bru_obs(formula = eta,
            family = "poisson",
            data = df)
```

**Fit the model**

Once the likelihood object is constructed, fitting the model is exactly the same process, we just need to specify the model components and the observational model, and pass this on to the `bru` function:

```{r}
#| code-summary: "Fit a GLM"
fit_glm = bru(cmp, lik)
```

And model summaries can be viewed using

```{r }
#| code-summary: "GLM summaries"
summary(fit_glm)
```

### Generate model predictions

------------------------------------------------------------------------

To generate new predictions we must provide a data frame that contains the covariate values for $x$ at which we want to predict.

This code block generates predictions for the data we used to fit the model (contained in `df$x`) as well as 10 new covariate values sampled from a uniform distribution `runif(10)`.

```{r get_predictions_glm}
#| code-summary: "Predcited values for Poisson GLM"
 
# Define new data, set to NA the values for prediction

new_data = data.frame(x = c(df$x, runif(10)),
                      y = c(df$y, rep(NA,10)))

# Define predictor formula
pred_fml <- ~ exp(beta_0 + beta_1)

# Generate predictions
pred_glm <- predict(fit_glm, new_data, pred_fml)
```

Since we used a log link (which is the default for `family = "poisson"`), we want to predict the exponential of the predictor. We specify this using a general `R` expression using the formula syntax.

::: callout-note
Note that the `predict` function will call the component names (i.e. the "labels") that were decided when defining the model.
:::

Since the component definition is looking for a covariate named $x$, all we need to provide is a data frame that contains one, and the software does the rest.

::: panel-tabset
## Plot

```{r}
#| echo: false
#| code-summary: "Plot GLM predicted values"
#| fig-cap: Data and 95% credible intervals
#| fig-align: center
#| fig-width: 4
#| fig-height: 4
#| warning: false

pred_glm %>% ggplot() + 
  geom_point(aes(x,y), alpha = 0.3) +
  geom_line(aes(x,mean)) +
    geom_ribbon(aes(x = x, ymax = q0.975, ymin = q0.025),fill = "tomato", alpha = 0.3)+
  geom_line(aes(x, q0.025), linetype = "dashed")+
  geom_line(aes(x, q0.975), linetype = "dashed")+
  xlab("Covariate") + ylab("Observations (counts)")
```

## R Code

```{r}
#| eval: false
#| code-summary: "Plot GLM predicted values"
#| purl: false

pred_glm %>% ggplot() + 
  geom_point(aes(x,y), alpha = 0.3) +
  geom_line(aes(x,mean)) +
    geom_ribbon(aes(x = x, ymax = q0.975, ymin = q0.025),fill = "tomato", alpha = 0.3)+
  xlab("Covariate") + ylab("Observations (counts)")
```
:::

:::: {.callout-warning icon="false"}
## {{< bi pencil-square color=#c8793c >}}Task

Suppose a binary response such that

$$
    \begin{aligned}
y_i &\sim \mathrm{Bernoulli}(\psi_i)\\
\eta_i &= \mathrm{logit}(\psi_i) = \alpha_0 +\alpha_1 \times w_i 
\end{aligned}
$$ Using the following simulated data, use `inlabru` to fit the logistic regression above. Then, plot the predictions for the data used to fit the model along with 10 new covariate values

```{r}
#| code-summary: "GLM Task"
set.seed(123)
n = 100
alpha = c(0.5,1.5)
w = rnorm(n)
psi = plogis(alpha[1] + alpha[2] * w)
y = rbinom(n = n, size = 1, prob =  psi) # set size = 1 to draw binary observations
df_logis = data.frame(y = y, w = w)  
```

Here we use the logit link function $\mathrm{logit}(x) = \log\left(\frac{x}{1-x}\right)$ (`plogis()` function in R) to link the linear predictor to the probabilities $\psi$.

`r hide("Take hint")`

You can set `family = "binomial"` for binary responses and the `plogis()` function for computing the predicted values.

::: callout-note
The Bernoulli distribution is equivalent to a $\mathrm{Binomial}(1, \psi)$ pmf. If you have proportional data (e.g. no. successes/no. trials) you can specify the number of events as your response and then the number of trials via the `Ntrials = n` argument of the `bru_obs` function (where `n` is the known vector of trials in your data set).
:::

`r unhide()`

```{r}
#| webex.hide: "Click here to see the solution"
#| code-fold: show
#| purl: false
#| warning: false
#| message: false
#| fig-align: center
#| fig-width: 4
#| fig-height: 4


# Model components
cmp_logis =  ~ -1 + alpha_0(1) + alpha_1(w, model = "linear")
# Model likelihood
lik_logis =  bru_obs(formula = y ~.,
            family = "binomial",
            data = df_logis)
# fit the model
fit_logis <- bru(cmp_logis,lik_logis)

# Define data for prediction
new_data = data.frame(w = c(df_logis$w, runif(10)),
                      y = c(df_logis$y, rep(NA,10)))
# Define predictor formula
pred_fml <- ~ plogis(alpha_0 + alpha_1)

# Generate predictions
pred_logis <- predict(fit_logis, new_data, pred_fml)

# Plot predictions
pred_logis %>% ggplot() + 
  geom_point(aes(w,y), alpha = 0.3) +
  geom_line(aes(w,mean)) +
    geom_ribbon(aes(x = w, ymax = q0.975, ymin = q0.025),fill = "tomato", alpha = 0.3)+
  xlab("Covariate") + ylab("Observations")

```
::::
