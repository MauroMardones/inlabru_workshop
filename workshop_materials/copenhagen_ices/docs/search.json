[
  {
    "objectID": "slides/slides_13.html#overview-of-distance-sampling",
    "href": "slides/slides_13.html#overview-of-distance-sampling",
    "title": "Lecture 13",
    "section": "Overview of Distance Sampling",
    "text": "Overview of Distance Sampling\n\nDistance sampling is a family of related methods for estimating the abundance and spatial distribution of wild populations.\nDistance sampling is based on the idea that animals further away from observers are harder to detect than animals that are nearer.\n\n\n\n\nThis idea is implemented in the model as a detection function that depends on distance.\n\nAnimals at greater distances are harder to detect and the detection function therefore declines as distance increases.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#density-surface-models",
    "href": "slides/slides_13.html#density-surface-models",
    "title": "Lecture 13",
    "section": "Density surface models",
    "text": "Density surface models\nCoupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.\n\nTraditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#density-surface-models-1",
    "href": "slides/slides_13.html#density-surface-models-1",
    "title": "Lecture 13",
    "section": "Density surface models",
    "text": "Density surface models\nCoupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.\n\nTraditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data.\nThis requires binning the data into counts based on some discretisation of space.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#density-surface-models-2",
    "href": "slides/slides_13.html#density-surface-models-2",
    "title": "Lecture 13",
    "section": "Density surface models",
    "text": "Density surface models\nCoupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.\n\nTraditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data.\nA major downside to this approach is the propagation of uncertainty from the detection model to the second-stage spatial model.\n\n\n\nThe goal: one-stage distance sampling model, simultaneously estimating the detectability and the spatial distribution of animals using a thinned point process framework.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-1",
    "href": "slides/slides_13.html#thinned-point-process-1",
    "title": "Lecture 13",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\n\n\nThe LGCP is a flexible approach that can include spatial covariates to model the mean intensity and a mean-zero spatially structured random effect to account for unexplained heterogeneity not captured by the covariates.\n\nTo account for the imperfect detection of points we specify a thinning probability function \\[\ng(s) = \\text{Prob}(\\text{a point at s is detected}|\\text{a point is at s})\n\\]\nA key property of LGCP is that a realisation of a point process with intensity \\(\\lambda(s)\\) that is thinned by probability function \\(g(s)\\), follows also a LGCP with intensity:\n\n\\[\n\\underbrace{\\tilde{\\lambda}(s)}_{\\text{observed process}} = \\underbrace{\\lambda(s)}_{\\text{true process}} \\times \\underbrace{g(s)}_{\\text{thinning probability}}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-2",
    "href": "slides/slides_13.html#thinned-point-process-2",
    "title": "Lecture 13",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\nLets visualize this on 1D: Intensity function with points",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-3",
    "href": "slides/slides_13.html#thinned-point-process-3",
    "title": "Lecture 13",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\nIntensity (density) function with points and transect locations",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-4",
    "href": "slides/slides_13.html#thinned-point-process-4",
    "title": "Lecture 13",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\nDetection function \\(\\color{red}{g(s)}\\)\nHere \\(\\color{red}{g(s) =1}\\) on the transects (at x = 10,30 and 50).",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-5",
    "href": "slides/slides_13.html#thinned-point-process-5",
    "title": "Lecture 13",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\nDetection function \\(\\color{red}{g(s)}\\) and detected points",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-6",
    "href": "slides/slides_13.html#thinned-point-process-6",
    "title": "Lecture 13",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-7",
    "href": "slides/slides_13.html#thinned-point-process-7",
    "title": "Lecture 13",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n The detection function describes the probability \\(\\color{red}{p(s)}\\) that an point is detected",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-8",
    "href": "slides/slides_13.html#thinned-point-process-8",
    "title": "Lecture 13",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#thinned-point-process-9",
    "href": "slides/slides_13.html#thinned-point-process-9",
    "title": "Lecture 13",
    "section": "Thinned Point Process",
    "text": "Thinned Point Process\n\nObservations are from a thinned Poisson process with intensity \\(\\lambda(s) \\color{red}{p(s)}\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#detection-function",
    "href": "slides/slides_13.html#detection-function",
    "title": "Lecture 13",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#detection-function-1",
    "href": "slides/slides_13.html#detection-function-1",
    "title": "Lecture 13",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#detection-function-2",
    "href": "slides/slides_13.html#detection-function-2",
    "title": "Lecture 13",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects\n\nThe thinning probability function is specified as a parametric family of functions.\n\n\n\nHalf-normal: \\(g(\\mathbf{s}|\\sigma) = \\exp(-0.5 (d(\\mathbf{s})/\\sigma)^2)\\)\nHazard-rate :\\(g(\\mathbf{s}|\\sigma) = 1 - \\exp(-(d(\\mathbf{s})/\\sigma)^{-1})\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#detection-function-3",
    "href": "slides/slides_13.html#detection-function-3",
    "title": "Lecture 13",
    "section": "Detection Function",
    "text": "Detection Function\n\n\n\n\nStandard distance sampling approaches specify \\(g(s)\\) as a function that declines with increasing distance\n\nhorizontal distance to the observer for point transects\nperpendicular distance to the transect line for line transects\n\nThe thinning probability function is specified as a parametric family of functions.\nThe thinned-LGCP likelihood is given by:\n\n\\[\n\\pi(\\mathbf{s_1},\\ldots,\\mathbf{s_m}) = \\exp\\left( |\\Omega| - \\int_{\\mathbf{s}\\in\\Omega}\\lambda(s)g(s)\\text{d}s \\right) \\prod_{i=1}^m \\lambda(\\mathbf{s}_i)g(\\mathbf{s}_i)\n\\]\n\nTo make \\(g(s)\\) and \\(\\lambda(s)\\) identifiable, we assume intensity is constant with respect to distance from the observer.\n\nIn practice this means we assume animals are uniformly distributed with respect to distance from the line",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#putting-all-the-pieces-together",
    "href": "slides/slides_13.html#putting-all-the-pieces-together",
    "title": "Lecture 13",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#putting-all-the-pieces-together-1",
    "href": "slides/slides_13.html#putting-all-the-pieces-together-1",
    "title": "Lecture 13",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#putting-all-the-pieces-together-2",
    "href": "slides/slides_13.html#putting-all-the-pieces-together-2",
    "title": "Lecture 13",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects\ndetected points are generated from the thinned PP with intensity \\(\\color{red}{\\tilde{\\lambda}(s)}= \\lambda(s)\\color{red}{g(d(s))}\\)\n\nThe log intensity \\(\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\mathbf{x}'\\beta + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#putting-all-the-pieces-together-3",
    "href": "slides/slides_13.html#putting-all-the-pieces-together-3",
    "title": "Lecture 13",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects\ndetected points are generated from the thinned PP with intensity \\(\\color{red}{\\tilde{\\lambda}(s)}= \\lambda(s)\\color{red}{g(d(s))}\\)\n\nThe log intensity \\(\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\mathbf{x}'\\beta + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\)\nThe encounter rate, i.e.¬†the number of observed animals within a distance \\(W\\) follows \\(m \\sim \\text{Poisson} \\left(\\int_0^W \\tilde{\\lambda}(d)\\text{d}d\\right)\\)\n\n\n\n\nThe pdf of detected distances is \\(\\pi(d_1,\\ldots,d_m|m) \\propto \\prod_{i=1}^m\\dfrac{\\tilde{\\lambda}(d_i)}{\\int_0^W \\tilde{\\lambda}(d)\\text{d}d}\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#putting-all-the-pieces-together-4",
    "href": "slides/slides_13.html#putting-all-the-pieces-together-4",
    "title": "Lecture 13",
    "section": "Putting all the pieces together",
    "text": "Putting all the pieces together\n\n\n\n\n\n\n\n\n\nThe true point pattern \\(Y = \\mathbf{s}_1,\\ldots,\\mathbf{s}_n\\) are a realization of a Point process with intensity \\(\\lambda(s)\\)\nWe design a sampling survey to collect the data along transects\ndetected points are generated from the thinned PP with intensity \\(\\color{red}{\\tilde{\\lambda}(s)}= \\lambda(s)\\color{red}{g(d(s))}\\)\n\nThe log intensity \\(\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\mathbf{x}'\\beta + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\\)\nThe encounter rate, i.e.¬†the number of observed animals within a distance \\(W\\) follows \\(m \\sim \\text{Poisson} \\left(\\int_0^W \\tilde{\\lambda}(d)\\text{d}d\\right)\\)\n\n\n\n\nThe pdf of detected distances is \\(\\pi(d_1,\\ldots,d_m|m) \\propto \\prod_{i=1}^m\\dfrac{ g(d_i)}{\\int_o^W g(d) \\text{d}d}\\) if \\(\\color{red}{\\tilde{\\lambda}(d_i)} = \\lambda \\color{red}{g(d_i)}\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#an-approximation-strips-as-lines",
    "href": "slides/slides_13.html#an-approximation-strips-as-lines",
    "title": "Lecture 13",
    "section": "An approximation: Strips as lines",
    "text": "An approximation: Strips as lines\n\nIf the strips width ( \\(2W\\) ) is narrow compared to study region (\\(\\Omega\\)) we can treat them as lines.\n\nWe need to adjust the intensity at a point \\(\\mathbf{s}\\) on the line to take account of the actual width of the strip\nAdjust the thinning probability to account for having collapsed all points onto the line.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#an-approximation-strips-as-lines-1",
    "href": "slides/slides_13.html#an-approximation-strips-as-lines-1",
    "title": "Lecture 13",
    "section": "An approximation: Strips as lines",
    "text": "An approximation: Strips as lines\nThe intensity at a point \\(\\mathbf{s}\\) on the line becomes \\(2W\\lambda(s)\\) instead of \\(\\lambda(s)\\).\n\nLet \\(\\pi(d)\\) be the probability that the point is at a distance \\(d\\) from the line.\nLet \\(p(d)\\) be the probability that is detected given it is at \\(d\\).\n\nThen, the thinning probability becomes \\(\\pi(d)\\times p(d)\\), assuming the points are uniformly distributed within the strip then \\(\\pi(d) = 1/W\\) (the density of distances is assumed to be constant on the interval \\([0,W]\\)).\nThis updates our thinning intensity to\n\\[\n\\log \\tilde{\\lambda}(s) = \\underbrace{\\mathbf{x}'\\beta + \\xi(s)}_{\\log \\lambda(s)} + \\log p(d) + \\log \\times(2/2W)\n\\]\n\nTypically \\(p(d)\\) is a non-linear function, that is where inlabru can help via a Fixed point iteration scheme (further details available in this vignette)\n\n\n\\[\n\\mathbb{E}[N_{\\text{det}}] = 2W \\lambda(s) \\times \\frac{1}{W} \\int_0^W p(r)  dr = 2\\lambda(s) \\int_0^W p(r)  dr\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico",
    "href": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico",
    "title": "Lecture 13",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\nIn the next example, we will explore data from a combination of several NOAA shipboard surveys conducted on pan-tropical spotted dolphins in the Gulf of Mexico.\n\nA total of 47 observations of groups of dolphins were detected. The group size was recorded, as well as the Beaufort sea state at the time of the observation.\nTransect width is 16 km, i.e.¬†maximal detection distance 8 km (transect half-width 8 km).",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh",
    "title": "Lecture 13",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\nFirst, we need to create the mesh used to approximate the random field. We can either:\n\nCreate a nonconvex extension of the points using the fm_mesh_2d and fm_nonconvex_hull functions from the fmesher package:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmax.edge for maximum triangle edge lengths\ncutoff to avoid overly small triangles in clustered areas",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-1",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-1",
    "title": "Lecture 13",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\nFirst, we need to create the mesh used to approximate the random field. We can either:\n\nUse a pre-define sf boundary and specify this directly into the mesh construction via the fm_mesh_2d function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmax.edge for maximum triangle edge lengths\ncutoff to avoid overly small triangles in clustered areas",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-2",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-2",
    "title": "Lecture 13",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\n\nAll random field models need to be discretised for practical calculations.\nThe SPDE models were developed to provide a consistent model definition across a range of discretisations.\nWe use finite element methods with local, piecewise linear basis functions defined on a triangulation of a region of space containing the domain of interest.\nDeviation from stationarity is generated near the boundary of the region.\nThe choice of region and choice of triangulation affects the numerical accuracy.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-3",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-3",
    "title": "Lecture 13",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\n\nToo fine meshes \\(\\rightarrow\\) heavy computation\nToo coarse mesh \\(\\rightarrow\\) not accurate enough",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-4",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-4",
    "title": "Lecture 13",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\nSome guidelines\n\nCreate triangulation meshes with fm_mesh_2d():\nedge length should be around a third to a tenth of the spatial range\nMove undesired boundary effects away from the domain of interest by extending to a smooth external boundary:\nUse a coarser resolution in the extension to reduce computational cost (max.edge=c(inner, outer)), i.e., add extra, larger triangles around the border",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-5",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-mesh-5",
    "title": "Lecture 13",
    "section": "Step 1: Define the SPDE representation: The mesh",
    "text": "Step 1: Define the SPDE representation: The mesh\n\nUse a fine resolution (subject to available computational resources) for the domain of interest (inner correlation range) and avoid small edges ,i.e., filter out small input point clusters (0 \\(&lt;\\) cutoff \\(&lt;\\) inner)\nCoastlines and similar can be added to the domain specification in fm_mesh_2d() through the boundary argument.\nsimplify the border",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#step-1-define-the-spde-representation-the-spde",
    "href": "slides/slides_13.html#step-1-define-the-spde-representation-the-spde",
    "title": "Lecture 13",
    "section": "Step 1: Define the SPDE representation: The SPDE",
    "text": "Step 1: Define the SPDE representation: The SPDE\nWe use the inla.spde2.pcmatern to define the SPDE model using PC priors through the following probability statements\n\n\n\n\\(P(\\rho &lt; 50) = 0.1\\)\n\\(P(\\sigma &gt; 2) = 0.1\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#step-2-define-the-detection-function",
    "href": "slides/slides_13.html#step-2-define-the-detection-function",
    "title": "Lecture 13",
    "section": "Step 2: Define the Detection function",
    "text": "Step 2: Define the Detection function\nWe start by plotting the distances and histogram of frequencies in distance intervals.\n\nThen, we need to define a half-normal detection probability function. This must take distance as its first argument and the linear predictor of the sigma parameter as its second:",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-1",
    "href": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-1",
    "title": "Lecture 13",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\eta(s) &  = \\color{#FF6B6B}{\\boxed{\\beta_0}} + \\color{#FF6B6B}{\\boxed{ \\omega(s)}} + \\color{#FF6B6B}{\\boxed{ \\log p(s)}} \\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)\n\n\n\n\n\n\n\n\n\n\n\n\nThe samplers in this dataset are lines, not polygons, so we need to tell inlabru about the strip half-width, W, which in the case of these data is 8.\nTo control the prior distribution for the \\(\\sigma\\) parameter, we use a transformation mapper that converts a latent variable into an exponentially distributed variable with expectation 8\nThe marginal argument in the sigma component specifies the transformation function taking N(0,1) to Exponential(1/8).",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-2",
    "href": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-2",
    "title": "Lecture 13",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\color{#FF6B6B}{\\boxed{\\eta(s)}} &  = \\color{#FF6B6B}{\\boxed{\\beta_0 +  \\omega(s) +  \\log p(s)}}\\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)\n\n\n\n\n\n\n\n\n\n\n\n\nwe need an offset due to the unknown direction of the detections",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-3",
    "href": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-3",
    "title": "Lecture 13",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\n\\color{#FF6B6B}{\\boxed{p(\\mathbf{y} | \\lambda)}} & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\eta(s) &  = \\beta_0 +  \\omega(s) +  \\log p(s) \\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-4",
    "href": "slides/slides_13.html#example-dolphins-in-the-gulf-of-mexico-4",
    "title": "Lecture 13",
    "section": "Example: Dolphins in the Gulf of Mexico",
    "text": "Example: Dolphins in the Gulf of Mexico\n\n\nThe LGCP Model\n\\[\n\\begin{aligned}\n\\color{#FF6B6B}{\\boxed{p(\\mathbf{y} | \\lambda)}} & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) p(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) p(\\mathbf{s}_i)) \\\\\n\\eta(s) &  = \\beta_0 +  \\omega(s) +  \\log p(s) \\\\\n\\end{aligned}\n\\]\nThe code\n\n# define model component\ncmp = ~ Intercept(1) + \n  space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) \n\n# define model predictor\neta  = geometry + distance ~ space +\n  log(hn(distance, sigma)) +\n  Intercept + log(2)\n\n# build the observation model\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\n# fit the model\nfit = bru(cmp, lik)\n\n\nThe integration scheme\n\n# build integration scheme\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\nips = fm_int(list(geometry = mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#results-posterior-summaries",
    "href": "slides/slides_13.html#results-posterior-summaries",
    "title": "Lecture 13",
    "section": "Results: posterior summaries",
    "text": "Results: posterior summaries\n\n\nWe can use the fit$summary.fixed and summary.hyperpar to obtain posterior summaries of the mdoel parameters.\n\n\n\n\n\n\n\n\n\nmean\n0.025quant\n0.975quant\n\n\n\n\nIntercept\n‚àí8.41\n‚àí9.47\n‚àí7.62\n\n\nsigma\n‚àí0.05\n‚àí0.46\n0.36\n\n\nRange for space\n131.74\n41.79\n320.28\n\n\nStdev for space\n1.17\n0.72\n1.78\n\n\n\n\n\n\n\n\nThe spde.posterior allow us to plot the posterior density of the Matern field parameters",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#results-posterior-summaries-1",
    "href": "slides/slides_13.html#results-posterior-summaries-1",
    "title": "Lecture 13",
    "section": "Results: posterior summaries",
    "text": "Results: posterior summaries\n\n\nWe can use the fit$summary.fixed and summary.hyperpar to obtain posterior summaries of the model parameters.\n\n\n\n\n\n\n\n\n\nmean\n0.025quant\n0.975quant\n\n\n\n\nIntercept\n‚àí8.41\n‚àí9.47\n‚àí7.62\n\n\nsigma\n‚àí0.05\n‚àí0.46\n0.36\n\n\nRange for space\n131.74\n41.79\n320.28\n\n\nStdev for space\n1.17\n0.72\n1.78\n\n\n\n\n\n\n\n\nThe spde.posterior allow us to plot the posterior density of the Matern field parameters",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#results-predicted-densities",
    "href": "slides/slides_13.html#results-predicted-densities",
    "title": "Lecture 13",
    "section": "Results: predicted densities",
    "text": "Results: predicted densities\n\n\nTo map the spatial intensity we first need to define a grid of points where we want to predict.\n\nWe do this using the function fm_pixel() which creates a regular grid of points covering the mesh\nThen, we use the predict function which takes as input\n\nthe fitted model (fit)\nthe prediction points (pxl)\nthe model components we want to predict (e.g., \\(e^{\\beta_0 + \\xi(s)}\\))",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#results-predicted-densities-1",
    "href": "slides/slides_13.html#results-predicted-densities-1",
    "title": "Lecture 13",
    "section": "Results: predicted densities",
    "text": "Results: predicted densities\n\n\nWe can also use the predict the detection function:\n\ndistdf &lt;- data.frame(distance = seq(0, 8, length.out = 100))\ndfun &lt;- predict(fit, distdf, ~ hn(distance, sigma))",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#results-predicted-expected-counts",
    "href": "slides/slides_13.html#results-predicted-expected-counts",
    "title": "Lecture 13",
    "section": "Results: predicted expected counts",
    "text": "Results: predicted expected counts\nWe can look at the posterior for the mean expected number of dolphins. Remember, the number of dolphins over the whole domain \\(\\Omega\\) is \\[\nN(\\Omega)\\sim\\text{Poisson}(E_{\\Omega}), \\text{ with } E_{\\Omega} = \\int_{\\Omega}\\lambda(s)ds\n\\] so the expected mean number of dolphins is \\[\nE_{\\Omega} = \\int_{\\Omega}\\lambda(s)ds = \\int_{\\Omega}\\exp\\{\\beta_0+\\omega(s)\\}ds\n\\]\n\npredpts &lt;- fm_int(mexdolphin$mesh, mexdolphin$ppoly)\nLambda &lt;- generate(fit, predpts, ~ sum(weight * exp(space + Intercept)),\n                   n.samples = 3000)\n\nEstimate:",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_13.html#results-predicted-expected-counts-1",
    "href": "slides/slides_13.html#results-predicted-expected-counts-1",
    "title": "Lecture 13",
    "section": "Results: predicted expected counts",
    "text": "Results: predicted expected counts\nIf want to predict the expected counts We can also get Monte Carlo samples for the expected number of dolphins as follows:\n\n\n\n\n[1] 601  10",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 13"
    ]
  },
  {
    "objectID": "slides/slides_10.html#motivation",
    "href": "slides/slides_10.html#motivation",
    "title": "Lecture 10",
    "section": "Motivation",
    "text": "Motivation\n\n‚ÄúAll models are wrong, some models are useful‚Äù ‚Äî George Box\n\n\n\nHow do we check that our model fits our data? - Model Validation\n\n\n\n\nHow do we choose the best model? - Model Comparison\n\n\n\nBut..it is not always easy to distinguish between the two!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#bayesian-model-comparison-1",
    "href": "slides/slides_10.html#bayesian-model-comparison-1",
    "title": "Lecture 10",
    "section": "Bayesian Model Comparison",
    "text": "Bayesian Model Comparison\n\nThere is no golden standard\nIt really depends what you want to do!\nBasically two types\n\nOnes that look at the posterior probability of the data under the model\nOnes that look at how model the data fits the data\n\nIn general it is not an easy task\ninlabru provides some options available",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#model-comparisonvalidation-in-inlabru",
    "href": "slides/slides_10.html#model-comparisonvalidation-in-inlabru",
    "title": "Lecture 10",
    "section": "Model Comparison/Validation in inlabru",
    "text": "Model Comparison/Validation in inlabru\n\nCriteria of fit\n\nMarginal likelihood ‚áíBayes factors\nDeviance information criterion (DIC)\nWidely applicable information criterion (WAIC)\n\nThere are also some predictive checks for the model:\n\nConditional predictive ordinate (CPO)\nProbability integral transform (PIT)\n\nYou can sample from the posterior using generate() and compute\n\nlog-scores\nCRPS\n‚Ä¶",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#marginal-likelihood",
    "href": "slides/slides_10.html#marginal-likelihood",
    "title": "Lecture 10",
    "section": "Marginal likelihood",
    "text": "Marginal likelihood\n\n# tell inlabru you want to compute mlik\nbru_options_set(control.compute = list(mlik = TRUE))\nfit = bru(cmp, lik)\n# see the results\nfit$mlik\n\n                                           [,1]\nlog marginal-likelihood (integration) -173.3449\nlog marginal-likelihood (Gaussian)    -173.7285\n\n\n\n\nCalculates \\(\\log(\\pi(\\mathbf{y}))\\)\nCan calculate Bayes factors through differences in value\nNB: Problematic for intrinsic models",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#deviance-information-criteria-dic",
    "href": "slides/slides_10.html#deviance-information-criteria-dic",
    "title": "Lecture 10",
    "section": "Deviance Information Criteria (DIC)",
    "text": "Deviance Information Criteria (DIC)\n\n# tell inlabru you want to compute DIC\nbru_options_set(control.compute = list(dic = TRUE))\nfit = bru(cmp, lik)\n# see the results\nfit$dic$dic\n\n[1] 308.5116\n\n\n\n\nMeasure of complexity and fit.\nDefined as: \\(\\text{DIC} = \\bar{D} + p_D\\)\n\n\\(\\bar{D}\\) is the posterior mean of the deviance\n\\(p_D\\) is the effective number of parameters.\n\nSmaller values indicate better trade-off between complexity and fit.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#widely-applicable-information-criterion-waic",
    "href": "slides/slides_10.html#widely-applicable-information-criterion-waic",
    "title": "Lecture 10",
    "section": "Widely applicable information criterion (WAIC)",
    "text": "Widely applicable information criterion (WAIC)\nalso known as Watanabe‚ÄìAkaike information criterion.\n\n# tell inlabru you want to compute WAIC\nbru_options_set(control.compute = list(waic = TRUE))\nfit = bru(cmp, lik)\n# see the results\nfit$waic$waic\n\n[1] 308.316\n\n\n\n\nsimilar to the DIC‚Ä¶but maybe better1\nLinked to the leave-one-out crossvalidation\nSmaller values indicate better trade-off between complexity and fit\n\nSee ‚ÄúUnderstanding predictive information criteria for Bayesian models‚Äù (2013) by Andrew Gelman, Jessica Hwang, and Aki Vehtari",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#posterior-predictive-distribution",
    "href": "slides/slides_10.html#posterior-predictive-distribution",
    "title": "Lecture 10",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nRemember: Our GLM is defined as: \\[\n\\begin{eqnarray}\n\\pi(\\mathbf{y}|\\mathbf{u},\\theta) & =\\prod_i \\pi(y_i|\\mathbf{u},\\theta)& \\ \\text{  likelihood}\\\\\n\\pi(\\mathbf{u}|\\theta)& &\\ \\text{  LGM}\\\\\n\\pi(\\theta )& &\\ \\text{  hyperprior}\\\\\n\\end{eqnarray}\n\\] using inlabru we estimate the posterior distribution \\(\\pi(\\mathbf{u},\\theta|\\mathbf{y})\\).\nThe posterior predictive distribution for a new data \\(\\hat{y}\\) is then: \\[\n\\pi(\\hat{y}|\\mathbf{y}) = \\int\\pi(y_i|\\mathbf{u},\\theta)\\pi(\\mathbf{u},\\theta|\\mathbf{y})\\ d\\mathbf{u}\\ d\\theta\n\\]\nThis ditribution can be used to check the model fit!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#posterior-predictive-distribution-1",
    "href": "slides/slides_10.html#posterior-predictive-distribution-1",
    "title": "Lecture 10",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\n\\[\n\\pi(\\hat{y}|\\mathbf{y}) = \\int\\pi(y_i|\\mathbf{u},\\theta)\\pi(\\mathbf{u},\\theta|\\mathbf{y})\\ d\\mathbf{u}\\ d\\theta\n\\]\nNOTE: In general this is NOT computed by inlabru but needs to be approximated\n\nUse generate to sample from the posterior \\((\\mathbf{u}^*_i,\\theta^*_i)\\sim\\pi(\\mathbf{u},\\theta|\\mathbf{y})\\)\nSimulate a new datapoint \\(y^*_i\\sim(y_i|\\mathbf{u}^*_i,\\theta^*_i)\\)\nUse \\(y^*_1,\\dots, y^*_N\\) to approximate the posterior predictive distribution.\n\n\nBUT inlabru computes automatically two quantities that are useful for model check!\n\nConditional predictive ordinate (CPO)\nProbability integral transform (PIT)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#conditional-predictive-ordinate-cpo",
    "href": "slides/slides_10.html#conditional-predictive-ordinate-cpo",
    "title": "Lecture 10",
    "section": "Conditional predictive ordinate (CPO)",
    "text": "Conditional predictive ordinate (CPO)\nDefinition: \\[\ncpo_i = \\pi(y^{obs}_i|\\mathbf{y}_{-i})\n\\]\n\nIntroduced in Pettit (1990)1\nMeasures fit through the predictive density\nCan be used to compute the log-score as \\[\n\\text{Score} = -\\sum \\log(cpo_i)\n\\] lower score correspond to better models\n\nPettit, L. I. 1990. ‚ÄúThe Conditional Predictive Ordinate for the Normal Distribution.‚Äù Journal of the Royal Statistical Society. Series B (Methodological)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#probability-integral-transform-pit",
    "href": "slides/slides_10.html#probability-integral-transform-pit",
    "title": "Lecture 10",
    "section": "Probability integral transform (PIT)",
    "text": "Probability integral transform (PIT)\nHow to compute:\n\n# tell inlabru you want to compute DIC\nbru_options_set(control.compute = list(cpo = TRUE))\nfit = bru(cmp, lik)\n# see the results\nhead(fit$cpo$cpo)\n\n[1] 0.30893421 0.12762309 0.18682286 0.01053016 0.31926466 0.11970350\n\n\nNote it is possible to check for possible fails in computed CPOs\n\nhead(fit$cpo$failure)\n\n[1] 0 0 0 0 0 0",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#probability-integral-transform-pit-1",
    "href": "slides/slides_10.html#probability-integral-transform-pit-1",
    "title": "Lecture 10",
    "section": "Probability integral transform (PIT)",
    "text": "Probability integral transform (PIT)\nDefinition: \\[\npit_i = \\text{Prob}(\\hat{y}_i&lt;y_i|\\mathbf{y}_{-i})\n\\]\n\nLinked to leave-one-out cross-validation\n\\(pit_i\\) shows how well the ith data point is predicted by the rest of the data\nVery small values indicate ‚Äúsuprising‚Äù observation under the model\nFor well-calibrated, the PIT values should be approximately uniformly distributed.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#probability-integral-transform-pit-2",
    "href": "slides/slides_10.html#probability-integral-transform-pit-2",
    "title": "Lecture 10",
    "section": "Probability integral transform (PIT)",
    "text": "Probability integral transform (PIT)\nHow to compute:\n\n# tell inlabru you want to compute DIC\nbru_options_set(control.compute = list(cpo = TRUE))\nfit = bru(cmp, lik)\n# see the results\nhead(fit$cpo$pit)\n\n[1] 0.69973392 0.92570934 0.87304126 0.99683215 0.31511776 0.06836316",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#good-and-bad-pit-plots",
    "href": "slides/slides_10.html#good-and-bad-pit-plots",
    "title": "Lecture 10",
    "section": "Good and Bad PIT plots",
    "text": "Good and Bad PIT plots",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#other-scores",
    "href": "slides/slides_10.html#other-scores",
    "title": "Lecture 10",
    "section": "Other scores",
    "text": "Other scores\nIn the literature there are many proposed scores for evaluate predictions. For example:\n\nDawid-Sebastian score\nLog-score\nContinuous rank probility score (CRPS)\nBrier score\n‚Ä¶\n\nThey all have their strength and wakness and which one is better depends on the goals of the model.\ninlabru does not provide such scores automatcally, but they can be computed using simulations from the posterior distribution.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#example-crps-for-poisson-data",
    "href": "slides/slides_10.html#example-crps-for-poisson-data",
    "title": "Lecture 10",
    "section": "Example: CRPS for Poisson data",
    "text": "Example: CRPS for Poisson data\nOur model: \\[\n\\begin{eqnarray}\ny_i|\\lambda_i & \\sim \\text{Poisson}(\\lambda_i),&\\ i = 1,\\dots,N_{\\text{data}}\\\\\n\\log(\\lambda_i)  = \\eta_i &= \\beta_0 + \\beta_1 x_i\n\\end{eqnarray}\n\\]\nSimulate data and fit the model:\n\ndf_pois &lt;- data.frame(\n  x = rnorm(50),\n  y = rpois(length(x), exp(2 + 1 * x))\n)\ncmp = ~ Intercept(1) + cov(x, model = \"linear\")\nlik = bru_obs(formula = y~.,\n              family = \"poisson\",\n              data = df_pois)\nfit_pois &lt;- bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#example-crps-for-poisson-data-1",
    "href": "slides/slides_10.html#example-crps-for-poisson-data-1",
    "title": "Lecture 10",
    "section": "Example: CRPS for Poisson data",
    "text": "Example: CRPS for Poisson data\nThe CRPS score is defined as: \\[\n\\text{S}_{\\text{CRPS}}(F_i, y_i) = \\sum_{k=0}^\\infty\\left[\\text{Prob}(Y_i\\leq k|\\mathbf{y})-I(y_i\\leq k)\\right]^2\n\\]\nComputational algorithm:\n\nSimulate \\(\\lambda^{(j)}\\sim p(\\lambda|\\text{data}), j = 1,\\dots, N_{\\text{samples}}\\) using generate() (size \\(N\\times N_\\text{samples}\\)).\nFor each \\(i=1,\\dots,N_{\\text{data}}\\), estimate \\(r_{ik}=\\text{Prob}(Y\\leq k|\\text{data})-I(y_i\\leq k)\\) as \\[\n  \\hat{r}_{ik} = \\frac{1}{N_\\text{samples}} \\sum_{j=1}^{N_\\text{samples}}\n  \\{\n  \\text{Prob}(Y\\leq k|\\lambda^{(j)}_i)-I(y_i\\leq k)\n  \\} .\n\\]\nCompute \\[\n  S_\\text{CRPS}(F_i,y_i) = \\sum_{k=0}^{K} \\hat{r}_{ik}^2\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#example-crps-for-poisson-data-2",
    "href": "slides/slides_10.html#example-crps-for-poisson-data-2",
    "title": "Lecture 10",
    "section": "Example: CRPS for Poisson data",
    "text": "Example: CRPS for Poisson data\nImplementation:\n\n# some large value, so that 1-F(K) is small\nmax_K &lt;- ceiling(max(df_pois$y) + 4 * sqrt(max(df_pois$y)))\nk &lt;- seq(0, max_K)\nkk &lt;- rep(k, times = length(df_pois$y))\ni &lt;- seq_along(df_pois$y)\npred_pois &lt;- generate(fit_pois, df_pois,\n  formula = ~ {\n    lambda &lt;- exp(Intercept + x)\n    ppois(kk, lambda = rep(lambda, each = length(k)))\n  },\n  n.samples = 2000\n)\nresults &lt;- data.frame(\n  i = rep(i, each = length(k)),\n  k = kk,\n  Fpred = rowMeans(pred_pois),\n  residuals =\n    rowMeans(pred_pois) - (rep(df_pois$y, each = length(k)) &lt;= kk)\n)\n\ncrps_scores &lt;-\n  (results %&gt;%\n    group_by(i) %&gt;%\n    summarise(crps = sum(residuals^2), .groups = \"drop\") %&gt;%\n    pull(crps))\nsummary(crps_scores)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  0.5005   3.0537   7.7753  14.2907  18.5087 155.8309",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#residuals-frequentist-vs-bayesian",
    "href": "slides/slides_10.html#residuals-frequentist-vs-bayesian",
    "title": "Lecture 10",
    "section": "Residuals: Frequentist vs Bayesian",
    "text": "Residuals: Frequentist vs Bayesian\n\n\nüéØ Frequentist View\n\nModel parameters are fixed but unknown.\nFitted values (predictions): \\(\\hat{y}_i\\) are point estimates\nResiduals:\n\\[\nr_i = y_i - \\hat{y}_i\n\\]\nSingle number per data point.\nUsed for:\n\nChecking model fit / outliers\n\n\n\nüîÆ Bayesian View",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#residuals-frequentist-vs-bayesian-1",
    "href": "slides/slides_10.html#residuals-frequentist-vs-bayesian-1",
    "title": "Lecture 10",
    "section": "Residuals: Frequentist vs Bayesian",
    "text": "Residuals: Frequentist vs Bayesian\n\n\nüéØ Frequentist View\n\nModel parameters are fixed but unknown.\nFitted values (predictions): \\(\\hat{y}_i\\) are point estimates\nResiduals:\n\\[\nr_i = y_i - \\hat{y}_i\n\\]\nSingle number per data point.\nUsed for:\n\nChecking model fit / outliers\n\n\n\nüîÆ Bayesian View\n\nParameters are random variables with posterior \\(\\pi(\\theta \\mid y)\\).\nPredictions \\(\\tilde{y}_i\\) also have a posterior distribution.\nNo single ‚Äútrue‚Äù fitted value ‚Üí residuals are not uniquely defined.\n\n\\(r_i^{(\\text{mean})} = (y_i - E[\\tilde{y}_i \\mid y])\\) (mean residual)\n\n\\(r_i^{(\\text{sample})} = (y_i - \\tilde{y}^s)\\) for posterior sample \\(s\\)\nDistribution of \\(y_i - \\tilde{y}_i^{(s)}\\) (posterior residuals)\n\n\n\n\n\nA better option is to use posterior predictive checks1\n\nSee Gelman et. al (2020) ‚ÄúBayesian Workflow‚Äù",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#one-example-of-posterior-predictive-checks",
    "href": "slides/slides_10.html#one-example-of-posterior-predictive-checks",
    "title": "Lecture 10",
    "section": "One example of posterior predictive checks",
    "text": "One example of posterior predictive checks\n\n\n\\[\ny_i|\\eta_i\\sim\\mathcal{N}(\\eta_i, \\sigma^2)\n\\]\n\nModel 1 \\[\n\\eta_i = \\beta_0 + \\beta_1 x_i\n\\]\nModel 2 \\[\n\\eta_i = \\beta_0 +  f(x_i)\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#one-example-of-posterior-predictive-checks-1",
    "href": "slides/slides_10.html#one-example-of-posterior-predictive-checks-1",
    "title": "Lecture 10",
    "section": "One example of posterior predictive checks",
    "text": "One example of posterior predictive checks\n\nSample \\(y^{1k}_i\\sim\\pi(y_i|\\mathbf{y})\\) \\(k = 1,\\dots,M\\) using generate()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSamples\n\n\n\n\n1\n2\n3\n4\n...\nM\n\n\n\n\n$y_1$\n2.84\n5.04\n4.89\n3.95\n...\n4.85\n\n\n$y_3$\n2.3\n4.15\n2.52\n1.74\n...\n2.2\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n$y_N$\n3.33\n3.89\n1.46\n4.91\n...\n3.71\n\n\n\n\n\n\nCompare some summaries of the simulated data with the one of the observed one",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#one-example-of-posterior-predictive-checks-2",
    "href": "slides/slides_10.html#one-example-of-posterior-predictive-checks-2",
    "title": "Lecture 10",
    "section": "One example of posterior predictive checks",
    "text": "One example of posterior predictive checks\nHere we compare the estimated posterior densities \\(\\hat{\\pi}^k(y|\\mathbf{y})\\) with the estimated data density\n\n\nThis is just a simple example, but more complex checks can be computed with the same idea!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#leave-group-our-cross-validation",
    "href": "slides/slides_10.html#leave-group-our-cross-validation",
    "title": "Lecture 10",
    "section": "Leave Group Our Cross-Validation",
    "text": "Leave Group Our Cross-Validation\nThis is a new option for cross-validation in inlabru . . .\n\nLeave-one-out cross-validation (LOOCV) is a very common technique to evaluate predictions from models\nWhen data are correlated (as in spatial statistics) LOOCV might be too optimistic and overestimate model performances.\n\n\n\nOne possible solution is to then remove ‚Äúchunck(s)‚Äù of data (for example one station and all its nearest neighbours)\nThis is the solution implemented in the inla.group.cv() function1\n\n\nAdin & al (2024) Automatic cross-validation in structured models: Is it time to leave out leave-one-out?, Spat. Statistics and Liu & al.¬†(2022) Leave-group-out cross-validation for latent Gaussian models",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#model-validation-for-lgcp-1",
    "href": "slides/slides_10.html#model-validation-for-lgcp-1",
    "title": "Lecture 10",
    "section": "Model validation for LGCP",
    "text": "Model validation for LGCP\nPoint processes (and so LGCP) are different from all other spatial models:\n\nThe data are presence and absence of points\nThe likelihood depends on the data location and the integrated intensity over the whole domain\n\n\nThis makes model evaluation especially challenging. Especially cross-validation based measures are hard to define‚Ä¶ why?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#cross-validation-for-point-processes",
    "href": "slides/slides_10.html#cross-validation-for-point-processes",
    "title": "Lecture 10",
    "section": "Cross validation for point processes",
    "text": "Cross validation for point processes\n\n\n\nIn a point process empty areas are also ‚Äúdata‚Äù\nWe cannot just remove points as this will change the underlying intensity\nWe need to remove a whole subdomain in order to cross-validate",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#a-warning-note",
    "href": "slides/slides_10.html#a-warning-note",
    "title": "Lecture 10",
    "section": "A warning note!",
    "text": "A warning note!\nWarning‚ö†Ô∏è\n\nDo not use WAIC and DIC as computed today by inlabru to compare LGCP models\n\n\nWAIC is linked to leave-one-out crossvalidation therefore it is ill-defined for point processes\nDIC is ok ‚Äúin theory‚Äù but not the way it is computed today\n\n\nWork is going on about this and measures of fit for LGCP will soon be available in inlabru",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#some-ideas-on-what-one-can-do",
    "href": "slides/slides_10.html#some-ideas-on-what-one-can-do",
    "title": "Lecture 10",
    "section": "Some ideas on what one can do",
    "text": "Some ideas on what one can do\n\nIt is possible to define residuals for PP, for example as: \\[\n\\hat{R}_B = n(B)- \\int_B\\hat{\\lambda}(s)\\ ds\n\\] where\n\\(n(B)\\) is the number of observed points in \\(B\\)\n\\(\\hat{\\lambda}(s)\\) is the estimated intensity\n\nThese residuals can be used to evaluate the model.\n\nThis is still work in progress and at the moment not easily available‚Ä¶ but it will be üòÄ\nHere you can see some examples of computation and use.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_10.html#take-home-messages",
    "href": "slides/slides_10.html#take-home-messages",
    "title": "Lecture 10",
    "section": "Take home messages",
    "text": "Take home messages\n\nModel check and model comparison are complex topics\nThere are no universal solutions, it all depends on which model characteristics you are interested in.\ninlabru provides some easy to compute alternatives\nLGCP require own tools to validate the model\n\nwork is ongoing here‚Ä¶ stay tuned üòÄ",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 10"
    ]
  },
  {
    "objectID": "slides/slides_5.html#outline",
    "href": "slides/slides_5.html#outline",
    "title": "Lecture 5",
    "section": "Outline",
    "text": "Outline\n\nWhy spatial modelling?\nWhy is spatial modelling computationally expensive?\nDifferent data types?\nModelling in discrete space ‚Äì areal data\nModelling in continuous space ‚Äì geo-referenced data\nModelling continuous space ‚Äì spatial point process data"
  },
  {
    "objectID": "slides/slides_5.html#spatial-modelling",
    "href": "slides/slides_5.html#spatial-modelling",
    "title": "Lecture 5",
    "section": "Spatial modelling",
    "text": "Spatial modelling\nmany natural processes take place in space large amounts of data collected in space; increased resolution large, complex data sets\nHOWEVER:\n\nspatial statistical analysis is often not complex enough\ninaccessible to practitioners as literature written for statisticians\ndevelopment of methodology often not linked to applications (unrealistic assumptions)\ndifficult to apply (unless you are an expert statistician and programmer)\n\nwe will see that inlabru can help with this‚Ä¶ why do we need spatial models in the first place?"
  },
  {
    "objectID": "slides/slides_5.html#an-example",
    "href": "slides/slides_5.html#an-example",
    "title": "Lecture 5",
    "section": "an example",
    "text": "an example\nglobal pm 2.5\nexposure to air pollution; particulate matter &lt; 2.5 microns in diameter (PM 2.5)\n\nlinked to poor health outcomes\nresponsible for three million deaths worldwide each year\nmaybe observations not independent?\nsparsely measured\nheterogeneous spatial coverage\n\nWe need to take account of spatial dependence‚Ä¶ i.e.¬†account for autocorrelation‚Ä¶ and complexity"
  },
  {
    "objectID": "slides/slides_5.html#spatial-modelling-1",
    "href": "slides/slides_5.html#spatial-modelling-1",
    "title": "Lecture 5",
    "section": "Spatial modelling",
    "text": "Spatial modelling\naccounting for spatial dependence\n\nstandard statistical modelling usually assumes independent observations\ndistributional assumptions that are made are only true if the independence assumption hold\nspatio-temporal data, however, are often not independent, but are spatially auto correlated\nindependence assumptions are violated here\ntwo observations taken in close proximity are very similar\ndo not provide two (independent) pieces of information"
  },
  {
    "objectID": "slides/slides_5.html#spatial-modelling-2",
    "href": "slides/slides_5.html#spatial-modelling-2",
    "title": "Lecture 5",
    "section": "Spatial modelling",
    "text": "Spatial modelling\naccounting for spatial dependence - ignoring this = pretend we have as much independent information as we have observations - pretend we have more information than we actually have - spurious inference and ultimately wrong conclusions‚Ä¶\nspatio-temporal models have special model components that explicitly model the dependence structure we need to:\n\nsay what the dependence in our data looks like in general: choose a specific class of spatial models, and\nestimate its specific properties for a specific dataset"
  },
  {
    "objectID": "slides/slides_5.html#spatial-modelling-computations",
    "href": "slides/slides_5.html#spatial-modelling-computations",
    "title": "Lecture 5",
    "section": "Spatial modelling ‚Äì computations",
    "text": "Spatial modelling ‚Äì computations\n\ncomputationally expensive\nin the past: often MCMC, takes forever"
  },
  {
    "objectID": "slides/slides_5.html#types-of-spatial-data",
    "href": "slides/slides_5.html#types-of-spatial-data",
    "title": "Lecture 5",
    "section": "Types of spatial data",
    "text": "Types of spatial data\nWe can distinguish three types of spatial data\nDiscrete space: - data on a spatial grid (areal data)\nContinuous space: - geostatistical (geo-referenced) data - spatial point data"
  },
  {
    "objectID": "slides/slides_5.html#discrete-space-areal-data",
    "href": "slides/slides_5.html#discrete-space-areal-data",
    "title": "Lecture 5",
    "section": "Discrete space: areal data",
    "text": "Discrete space: areal data\n\ndata on a (regular or irregular) spatial grid\nexamples: number of individuals in a region, average rainfall in a province\n(originally geostatistical or point data; gridded for practical reasons)\n\nObserved response(s): Measurement over each grid cell (e.g.¬†number of individuals in cell; rainfall in province)"
  },
  {
    "objectID": "slides/slides_5.html#continuous-space-geostatistical-data",
    "href": "slides/slides_5.html#continuous-space-geostatistical-data",
    "title": "Lecture 5",
    "section": "Continuous space: geostatistical data",
    "text": "Continuous space: geostatistical data\n\nphenomenon that is continuous in space\nexamples: nutrient levels in soil, salinity in the sea measurements at a given set of locations that are determined by surveyor\n\nObserved response(s): measurement(s) taken at given locations"
  },
  {
    "objectID": "slides/slides_5.html#continuous-space-spatial-point-patterns",
    "href": "slides/slides_5.html#continuous-space-spatial-point-patterns",
    "title": "Lecture 5",
    "section": "Continuous space: spatial point patterns",
    "text": "Continuous space: spatial point patterns\n\npatterns formed by locations of objects (individuals) in space (typically 2D)\nexamples: locations of trees in a forest, groups of animals, earthquakes\n\nObserved response(s): x,y coordinates of points (individuals/groups) sometimes also properties of individuals/groups (‚Äúmarks‚Äù)"
  },
  {
    "objectID": "slides/slides_5.html#point-patterns-vs.-geostatistical-data",
    "href": "slides/slides_5.html#point-patterns-vs.-geostatistical-data",
    "title": "Lecture 5",
    "section": "point patterns vs.¬†geostatistical data",
    "text": "point patterns vs.¬†geostatistical data\npoint patterns: - data format : x,y coordinates optional : properties of objects represented by the points (‚Äúmarks‚Äù)\ngeostatistical data: - data format : x,y coordinates not optional : measurement taken in these locations\nThese seem rather similar‚Ä¶"
  },
  {
    "objectID": "slides/slides_5.html#point-patterns-vs.-geostatistical-data-1",
    "href": "slides/slides_5.html#point-patterns-vs.-geostatistical-data-1",
    "title": "Lecture 5",
    "section": "point patterns vs.¬†geostatistical data",
    "text": "point patterns vs.¬†geostatistical data\npoint patterns : - data format : x,y coordinates\noptional : properties of objects represented by the points (‚Äúmarks‚Äù)\n\naim : modelling the locations of objects in continuous space\nlocations are being modelled and are considered random\nmarks only take values in locations where there is a point\n\ngeostatistical data : - data format : x,y coordinates\nnot optional : measurement taken in these locations\n\naim : modelling continuous process observed in finite number of locations\nlocations have typically been deliberately chosen and are fixed\ncontinuous spatial field takes on values in whole subset of \\(\\mathbf R^ 2\\)"
  },
  {
    "objectID": "slides/slides_5.html#take-home-message",
    "href": "slides/slides_5.html#take-home-message",
    "title": "Lecture 5",
    "section": "Take home message!",
    "text": "Take home message!\n\nall spatial models we discuss here are also special cases of the large class of Latent Gaussian models\nto account for spatial dependence in the data, different types of spatial terms have to be included in the model for different spatial data structures, but they are all approximated by an SPDE model\ninlabru provides an efficient and unified way to fit all these models!"
  },
  {
    "objectID": "slides/slides_3.html#main-elements-of-the-inla-methodology",
    "href": "slides/slides_3.html#main-elements-of-the-inla-methodology",
    "title": "Lecture 2",
    "section": "Main elements of the INLA methodology",
    "text": "Main elements of the INLA methodology\n\nLatent Gaussian Models\nSparse matrices\nLaplace approximations\n‚Ä¶other ‚Äútechnical‚Äù details for the special interested!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#repetition",
    "href": "slides/slides_3.html#repetition",
    "title": "Lecture 2",
    "section": "Repetition",
    "text": "Repetition\nEverything in INLA is based on so-called latent Gaussian models\n\n\n\nA few hyperparameters \\(\\theta\\sim\\pi(\\theta)\\) control variances, range and so on\nGiven these hyperparameters we have an underlying Gaussian distribution \\(\\mathbf{u}|\\theta\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{Q}^{-1}(\\theta))\\) that we cannot directly observe\nInstead we make indirect observations \\(\\mathbf{y}|\\mathbf{u},\\theta\\sim\\pi(\\mathbf{y}|\\mathbf{u},\\theta)\\) of the underlying latent Gaussian field",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#repetition-1",
    "href": "slides/slides_3.html#repetition-1",
    "title": "Lecture 2",
    "section": "Repetition",
    "text": "Repetition\nModels of this kind: \\[\n\\begin{aligned}\n\\mathbf{y}|\\mathbf{u},\\theta &\\sim \\prod_i \\pi(y_i|\\eta_i,\\theta)\\\\\n\\mathbf{\\eta} & = A_1\\mathbf{u}_1 + A_2\\mathbf{u}_2+\\dots + A_k\\mathbf{u}_k\\\\\n\\mathbf{u},\\theta &\\sim \\mathcal{N}(0,\\mathbf{Q}(Œ∏)^{‚àí1})\\\\\n\\theta & \\sim \\pi(\\theta)\n\\end{aligned}\n\\]\noccurs in many, seemingly unrelated, statistical models.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#examples",
    "href": "slides/slides_3.html#examples",
    "title": "Lecture 2",
    "section": "Examples",
    "text": "Examples\n\nGeneralised linear (mixed) models\nStochastic volatility\nGeneralised additive (mixed) models\nMeasurement error models\nSpline smoothing\nSemiparametric regression\nSpace-varying (semiparametric) regression models\nDisease mapping\nLog-Gaussian Cox-processes\nModel-based geostatistics (*)\nSpatio-temporal models\nSurvival analysis\n+++",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#main-characteristics",
    "href": "slides/slides_3.html#main-characteristics",
    "title": "Lecture 2",
    "section": "Main Characteristics",
    "text": "Main Characteristics\n\nLatent Gaussian model \\(\\mathbf{u}|\\theta\\sim\\mathcal{N}(0,\\mathbf{Q}^{-1}(\\theta))\\)\nThe data are conditionally independent given the latent field\nThe predictor is linear wrt the elements of \\(\\mathbf{u}\\)1\nThe dimension of \\(\\mathbf{u}\\) can be big (\\(10^3-10^6\\))\nThe dimension of \\(\\theta\\) should be not too big.\n\n\n\nIn this talk we will focus on the characteristics of \\(\\mathbf{u}|\\theta\\)\n\n\nwe will see that this can be partly relaxed üòÉ",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#the-big-n-problem",
    "href": "slides/slides_3.html#the-big-n-problem",
    "title": "Lecture 2",
    "section": "The big n problem!",
    "text": "The big n problem!\nFor many interesting applications it is necessary to solve large problems.\n\n\n\nWith large problems we think of models where the size \\(n\\) of the latent field \\(\\mathbf{u}\\) is between 100 to 100, 000. This includes fixed effects, spatial effects and so on.\n\n\n\n\n\n\nComputations with models of this size are cumbersome!\n\n\n\n\n\n\nThe solution in the INLA world are Gaussian Markov random fields (GMRFs)!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#the-gaussian-distribution",
    "href": "slides/slides_3.html#the-gaussian-distribution",
    "title": "Lecture 2",
    "section": "The Gaussian distribution",
    "text": "The Gaussian distribution\nA Gaussian distribution \\(\\mathbf{u}\\sim\\mathcal{N}(0,\\Sigma)\\) is controlled by:\n‚Äî The mean vector \\(\\mu\\), which we have set equal to 0. This is the centre of the distribution\n‚Äî The matrix \\(\\Sigma\\) describes all pairwise covariances \\(\\Sigma_{ij} = \\text{Cov}(u_i, u_j)\\)\nFormally \\[\n\\pi(\\mathbf{u}) = \\frac{1}{2\\pi|\\Sigma|^{1/2}}\\exp\\left\\{-\\frac{1}{2}\\mathbf{u}^T\\Sigma^{-1}\\mathbf{u}\\right\\}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#the-gaussian-distribution-1",
    "href": "slides/slides_3.html#the-gaussian-distribution-1",
    "title": "Lecture 2",
    "section": "The Gaussian distribution",
    "text": "The Gaussian distribution\n\n\n\n\n\n\n\nAdvantages üòÑüëç\nDisadvantages üòîüëé\n\n\n\n\nmostly theoretical\nmostly computational\n\n\nAnalytically tractable.\n\\(\\Sigma\\) usually is large and dense\n\n\nWe have a good understanding of its properties.\nComputations scale as \\(\\mathcal{O}(n^3)\\)\n\n\nBasically, it‚Äôs easy to do the theory\nNot feasible for large problems\n\n\n\n\nWe need to reduce the computational burden !",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#sparse-matrices",
    "href": "slides/slides_3.html#sparse-matrices",
    "title": "Lecture 2",
    "section": "Sparse matrices",
    "text": "Sparse matrices\n\nA matrix \\(\\mathbf{Q}\\) is called sparse if most of its elements are zero.\n\n\\[\n\\mathbf{Q} = \\begin{bmatrix}\n1 & 0 & 0 & 0\\\\\n0& 2& 0 & 0\\\\\n0& 0& -1 & 0\\\\\n0& 1& 0 & 0.4\\\\\n\\end{bmatrix}\n\\]\n‚Äî There exist very efficient numerical algorithms to deal with sparse matrices:\n‚Äî In order to solve large problems we need to exploit some property to make the computations feasible\n\n\nWhich matrix should be sparse in a Gaussian field?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#two-possible-options",
    "href": "slides/slides_3.html#two-possible-options",
    "title": "Lecture 2",
    "section": "Two possible options",
    "text": "Two possible options\n\\[\n\\mathcal{u}\\sim\\mathcal{N}(0,\\Sigma)\n\\]\n\nForce the covariance matrix \\(\\Sigma\\) to be sparse\nForce the precision matrix \\(\\Sigma^{-1}\\) to be sparse",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#two-possible-options-1",
    "href": "slides/slides_3.html#two-possible-options-1",
    "title": "Lecture 2",
    "section": "Two possible options",
    "text": "Two possible options\n\\[\n\\mathcal{u}\\sim\\mathcal{N}(0,\\Sigma)\n\\]\n\nForce the covariance matrix \\(\\Sigma\\) to be sparse\n\n\\(\\Sigma_{ij} = \\text{Cov}(u_i, u_j)\\) : Covariance between \\(u_i\\) and \\(u_j\\)\n\\(\\Sigma_{ij} = 0\\) \\(\\longrightarrow\\) \\(u_i\\) and \\(u_j\\) are independent\nA sparse covariance matrix implies that many elements of \\(\\mathbf{u}\\) are mutually independent‚Ä¶..is this desirable?\n\nForce the precision matrix \\(\\mathbf{Q} = \\Sigma^{-1}\\) to be sparse",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#two-possible-options-2",
    "href": "slides/slides_3.html#two-possible-options-2",
    "title": "Lecture 2",
    "section": "Two possible options",
    "text": "Two possible options\n\\[\n\\mathcal{u}\\sim\\mathcal{N}(0,\\Sigma)\n\\]\n\nForce the covariance matrix \\(\\Sigma\\) to be sparse\n\n\\(\\Sigma_{ij} = \\text{Cov}(u_i, u_j)\\) : Covariance between \\(u_i\\) and \\(u_j\\)\n\\(\\Sigma_{ij} = 0\\) \\(\\longrightarrow\\) \\(u_i\\) and \\(u_j\\) are independent\nA sparse covariance matrix implies that many elements of \\(\\mathbf{u}\\) are mutually independent‚Ä¶..is this desirable?\n\nForce the precision matrix \\(\\mathbf{Q} = \\Sigma^{-1}\\) to be sparse\n\nWhat does \\(Q_{ij}\\) represents?\nWhat does a sparse precision matrix implies?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#example-the-ar1-proces",
    "href": "slides/slides_3.html#example-the-ar1-proces",
    "title": "Lecture 2",
    "section": "Example: The AR1 proces",
    "text": "Example: The AR1 proces\nDefinition\n\\[\n\\begin{aligned}\n\\mathbf{i=1}&:  u_1 \\sim\\mathcal{N}(0, \\frac{1}{1-\\phi^2})\\\\\n\\mathbf{i=2,\\dots,T}&:  u_i  = \\phi\\  u_{i-1} +\\epsilon_i,\\ \\epsilon_i\\sim\\mathcal{N}(0,1)\n\\end{aligned}\n\\]\n\nVery common to model dependence in time\nThe joint distribution of \\(u_1,u_2,\\dots,u_N\\) is Gaussian\nHow do covariance and precision matrices look?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#covariance-and-precision-matrix-for-ar1",
    "href": "slides/slides_3.html#covariance-and-precision-matrix-for-ar1",
    "title": "Lecture 2",
    "section": "Covariance and Precision Matrix for AR1",
    "text": "Covariance and Precision Matrix for AR1\n\n\nCovariance Matrix\n\\[\n\\Sigma = \\frac{1}{1-\\phi^2}  \\begin{bmatrix}\n1& \\phi & \\phi^2  & \\dots& \\phi^N \\\\\n\\phi & 1& \\phi  & \\dots& \\phi^{N-1} \\\\\n\\phi^2 & \\phi & 1 & \\dots& \\phi^{N-2} \\\\\n\\dots& \\dots& \\dots& \\dots& \\dots& \\\\\n\\phi^{N} & \\phi^{N-1}& \\phi^{N-2}  & \\dots& 1\\\\\n\\end{bmatrix}\n\\]\n\nThis is a dense matrix.\nAll elements of the \\(\\mathbf{u}\\) vector are dependent.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#covariance-and-precision-matrix-for-ar1-1",
    "href": "slides/slides_3.html#covariance-and-precision-matrix-for-ar1-1",
    "title": "Lecture 2",
    "section": "Covariance and Precision Matrix for AR1",
    "text": "Covariance and Precision Matrix for AR1\nPrecision Matrix\n\\[\n\\mathbf{Q} = \\Sigma^{-1} =  \\begin{bmatrix}\n1& -\\phi & 0  & 0 &\\dots& 0 \\\\\n-\\phi & 1 + \\phi^2& -\\phi  & 0 & \\dots& 0 \\\\\n0 & -\\phi & 1-\\phi^2 &-\\phi &  \\dots& 0 \\\\\n0 & 0 & -\\phi &1-\\phi^2 & \\dots & \\dots \\\\\n\\dots& \\dots& \\dots& \\dots& \\dots& \\dots& \\\\\n0 &0 & 0 & \\dots  & -\\phi& 1\\\\\n\\end{bmatrix}\n\\]\n\nThis is a tridiagonal matrix, it is sparse\nThe tridiagonal form of \\(\\mathbf{Q}\\) can be exploited for quick calculations.\n\n\nWhat is the key property of this example that causes \\(\\mathbf{Q}\\) to be sparse?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#conditional-independence",
    "href": "slides/slides_3.html#conditional-independence",
    "title": "Lecture 2",
    "section": "Conditional independence",
    "text": "Conditional independence\nThe key lies in the full conditionals\n\\[\nu_t|\\mathbf{u}_{-t}\\sim\\mathcal{N}\\left(\\frac{\\phi}{1-\\phi^2}(x_{t-1}+x_{t+1}), \\frac{1}{1+\\phi^2}\\right)\n\\]\n\nEach timepoint is only conditionally dependent on the two closest timepoints\nIt is useful to represent the conditional independence structure through a graph",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#conditional-independence-and-the-precision-matrix",
    "href": "slides/slides_3.html#conditional-independence-and-the-precision-matrix",
    "title": "Lecture 2",
    "section": "Conditional independence and the precision matrix",
    "text": "Conditional independence and the precision matrix\n\n\nTheorem: \\(u_i\\perp u_j|\\mathbf{u}_{-ij}\\Longleftrightarrow Q{ij} =0\\)\n\n\nThis is the key property! üòÉ",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#informal-definition-of-a-gmrf",
    "href": "slides/slides_3.html#informal-definition-of-a-gmrf",
    "title": "Lecture 2",
    "section": "(Informal) definition of a GMRF",
    "text": "(Informal) definition of a GMRF\n\n\nA GMRF is a Gaussian distribution where the non-zero elements of the precision matrix are defined by the graph structure.\n\n\n\nIn the previous example the precision matrix is tridiagonal since each variable is connected only to its predecessor and successor.\n\\[\n\\mathbf{Q} = \\Sigma^{-1} =  \\begin{bmatrix}\n1& -\\phi & 0  & 0 &\\dots& 0 \\\\\n-\\phi & 1 + \\phi^2& -\\phi  & 0 & \\dots& 0 \\\\\n0 & -\\phi & 1-\\phi^2 &-\\phi &  \\dots& 0 \\\\\n0 & 0 & -\\phi &1-\\phi^2 & \\dots & \\dots \\\\\n\\dots& \\dots& \\dots& \\dots& \\dots& \\dots& \\\\\n0 &0 & 0 & \\dots  & -\\phi& 1\\\\\n\\end{bmatrix}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#gmrf-and-inla",
    "href": "slides/slides_3.html#gmrf-and-inla",
    "title": "Lecture 2",
    "section": "GMRF and INLA",
    "text": "GMRF and INLA\n\nEvery component of the latent Gaussian field \\(\\mathbf{u}\\) in an INLA model is always a GMRF!!\nWe will see how the idea of GMRF expands also for continuously indexed processes (SPDE approach)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#what-do-we-need-to-be-able-to-compute",
    "href": "slides/slides_3.html#what-do-we-need-to-be-able-to-compute",
    "title": "Lecture 2",
    "section": "What do we need to be able to compute?",
    "text": "What do we need to be able to compute?\n\nWe need to compute determinants of large, sparse matrices. This is needed for likelihood calculations.\nWe need to compute square roots of large, sparse matrices. This is needed for likelihood calculations and simulations.\n\n\nTechnically:\n\nIn the end our computations come down to the Cholesky decomposition, \\(\\mathbf{Q} = \\mathbf{LL}^T\\)\nAfter \\(\\mathbf{L}\\) is available, things are fast\nThe limiting factor is how quickly the Cholesky factorization can be done",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#what-is-the-gain",
    "href": "slides/slides_3.html#what-is-the-gain",
    "title": "Lecture 2",
    "section": "What is the gain?",
    "text": "What is the gain?\n\nTemporal structure uses \\(\\mathcal{O}(n)\\) operations\nSpatial structure uses \\(\\mathcal{O}(n^{3/2})\\) operations\nSpatio-temporal structure uses \\(\\mathcal{O}(n^{2})\\) operations\n\nCompare this with \\(\\mathcal{O}(n^{3})\\) operations in the general case.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#summary",
    "href": "slides/slides_3.html#summary",
    "title": "Lecture 2",
    "section": "Summary",
    "text": "Summary\nGaussian Markov Random Fields (GMRF):\n\nGive faster computations, both in INLA and in MCMC schemes (thanks to sparse precision matrices)\nKeep the analytical tractability of the Gaussian distribution\nAllow modelling through conditional distributions\nNaturally arises from the SPDE approach (that we will talk about ‚Ä¶..)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#formal-definition-of-a-gmrf",
    "href": "slides/slides_3.html#formal-definition-of-a-gmrf",
    "title": "Lecture 2",
    "section": "Formal definition of a GMRF",
    "text": "Formal definition of a GMRF\n\nDefinition\nA random variable \\(\\mathbf{u}\\) is said to be a Gaussian Markov random field (GMRF) with respect to the graph \\(\\mathcal{G}\\), with vertices \\(\\{1, 2,\\dots , n\\}\\) and edges \\(\\mathcal{E}\\), with mean \\(\\mu\\) and precision matrix \\(\\mathbf{Q}\\) if its probability distribution is given by \\[\n\\pi(\\mathbf{u}) = \\frac{|\\mathbf{Q}|^{1/2}}{(2\\pi)^{n/2}}\\exp\\left\\{ -\\frac{1}{2}(\\mathbf{u}-\\mu)^T\\mathbf{Q}(\\mathbf{u}-\\mu)\\right\\}\n\\] and \\(Q_{ij} \\neq 0\\Longleftrightarrow \\{i,j\\}\\in\\mathcal{E}\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#the-inla-recipe",
    "href": "slides/slides_3.html#the-inla-recipe",
    "title": "Lecture 2",
    "section": "The INLA recipe",
    "text": "The INLA recipe\n\nNumerical integration schemes\nThe GMRF-Approximation\nThe Laplace approximation\n‚Ä¶many many many ‚Äútechnical‚Äù details",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#hierarchical-gmrf",
    "href": "slides/slides_3.html#hierarchical-gmrf",
    "title": "Lecture 2",
    "section": "Hierarchical GMRF",
    "text": "Hierarchical GMRF\n\n\\(\\mathbf{y}|\\mathbf{u},\\theta\\), Observation model (likelihood)\n\\(\\mathbf{u}|\\theta\\), Gaussian random field\n\\(\\theta\\) Hyperparameters",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#hierarchical-gmrf-1",
    "href": "slides/slides_3.html#hierarchical-gmrf-1",
    "title": "Lecture 2",
    "section": "Hierarchical GMRF",
    "text": "Hierarchical GMRF\n\n\\(\\mathbf{y}|\\mathbf{u},\\theta\\), Observation model (likelihood)\n\\(\\mathbf{u}|\\theta\\), Gaussian random field\n\\(\\theta\\) Hyperparameters\n\nExtra requirements\n\nThe latent field \\(\\mathbf{u}\\) is a GMRF \\[\n\\pi(\\mathbf{u}|\\theta)\\propto\\exp\\left\\{-\\frac{1}{2}\\mathbf{u}^T\\mathbf{Q}\\mathbf{u}\\right\\}\n\\]\n\nThe precision matrix \\(\\mathbf{Q}\\) is sparse.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#hierarchical-gmrf-2",
    "href": "slides/slides_3.html#hierarchical-gmrf-2",
    "title": "Lecture 2",
    "section": "Hierarchical GMRF",
    "text": "Hierarchical GMRF\n\n\\(\\mathbf{y}|\\mathbf{u},\\theta\\), Observation model (likelihood)\n\\(\\mathbf{u}|\\theta\\), Gaussian random field\n\\(\\theta\\) Hyperparameters\n\nExtra requirements\n\nThe latent field \\(x\\) is a GMRF \\(\\pi(\\mathbf{u}|\\theta)\\propto\\exp\\left\\{-\\frac{1}{2}\\mathbf{u}^T\\mathbf{Q}\\mathbf{u}\\right\\}\\)\nWe can factorize the likelihood as: \\(\\pi(\\mathbf{y}|\\mathbf{u},\\theta) = \\prod_i\\pi(y_i|\\eta_i,\\theta)\\)\n\nData are conditional independent give \\(\\mathbf{u}\\) and \\(\\theta\\)\nEach data point depends on only 1 element of the latent field: the predictor \\(\\eta_i\\)\n\\(\\eta\\) is a linear combination of other elements of \\(\\mathbf{u}\\): \\(\\eta = \\mathbf{A}^T\\mathbf{u}\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#hierarchical-gmrf-3",
    "href": "slides/slides_3.html#hierarchical-gmrf-3",
    "title": "Lecture 2",
    "section": "Hierarchical GMRF",
    "text": "Hierarchical GMRF\n\n\\(\\mathbf{y}|\\mathbf{u},\\theta\\), Observation model (likelihood)\n\\(\\mathbf{u}|\\theta\\), Gaussian random field\n\\(\\theta\\) Hyperparameters\n\nExtra requirements\n\nThe latent field \\(\\mathbf{u}\\) is a GMRF \\(\\pi(\\mathbf{u}|\\theta)\\propto\\exp\\left\\{-\\frac{1}{2}\\mathbf{u}^T\\mathbf{Q}\\mathbf{u}\\right\\}\\)\nWe can factorize the likelihood as: \\(\\pi(\\mathbf{y}|\\mathbf{u},\\theta) = \\prod_i\\pi(y_i|\\eta_i,\\theta)\\)\nThe vector of hyperparameters \\(\\theta\\) is low dimensional.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#inference",
    "href": "slides/slides_3.html#inference",
    "title": "Lecture 2",
    "section": "Inference",
    "text": "Inference\nMain Inferential Goal:\n\\[\\begin{aligned}\n\\overbrace{\\pi(\\mathbf{u}, {\\theta}\\mid\\mathbf{y})}^{{\\text{Posterior}}} &\\propto \\overbrace{\\pi({\\theta}) \\pi(\\mathbf{u}\\mid{\\theta})}^{{\\text{Prior}}} \\overbrace{\\prod_{i}\\pi(y_i \\mid \\eta_i, {\\theta})}^{{\\text{Likelihood}}}\n\\end{aligned}\\]\n\nThe posterior distribution is, in general, not available in closed form‚Ä¶\n..what to do then?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#inla-strategy-in-short",
    "href": "slides/slides_3.html#inla-strategy-in-short",
    "title": "Lecture 2",
    "section": "INLA strategy in short",
    "text": "INLA strategy in short\nAssumption\nWe are mainly interested in posterior margianals \\(\\pi(u_i|\\mathbf{y})\\) and \\(\\pi(\\theta_j|\\mathbf{y})\\)\nStrategy\n\nWe use numerical integration in a smart way\nWe approximate what we do not know analitically exploiting the Gaussian structure of \\(\\mathbf{u}|\\theta\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#numerical-integration",
    "href": "slides/slides_3.html#numerical-integration",
    "title": "Lecture 2",
    "section": "Numerical Integration",
    "text": "Numerical Integration\nWe want to compute \\(\\pi(u_i|\\mathbf{y})\\)\n\n\n\nHard way üëé\n\\[\n\\pi(u_i|\\mathbf{y}) = \\int\\pi(\\mathbf{u}|\\theta)\\ d\\mathbf{u}_{-i}\n\\]\n\nThis is a hard integral as \\(\\mathbf{u}\\) can be very high dimentional!!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#numerical-integration-1",
    "href": "slides/slides_3.html#numerical-integration-1",
    "title": "Lecture 2",
    "section": "Numerical Integration",
    "text": "Numerical Integration\nWe want to compute \\(\\pi(u_i|\\mathbf{y})\\)\n\n\n\nHard way üëé\n\\[\n\\pi(u_i|\\mathbf{y}) = \\int\\pi(\\mathbf{u}|\\mathbf{y})\\ d\\mathbf{u}_{-i}\n\\]\n\nThis is a hard integral as \\(\\mathbf{u}\\) can be very high dimentional!!\n\n\nEasier way üëç\n\\[\n\\pi(u_i|\\mathbf{y}) = \\int\\pi(u_i|\\theta, \\mathbf{y})\\pi(\\theta|\\mathbf{y)}\\ d\\theta\n\\]\n\n\\(\\theta\\) is a smaller dimensional vectore, easier to integrate\nBUT we need to approximate both \\(\\pi(\\theta|\\mathbf{y})\\) and \\(\\pi(u_i|\\theta, \\mathbf{y})\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#approximating-the-posterior-for-the-hyperparameters",
    "href": "slides/slides_3.html#approximating-the-posterior-for-the-hyperparameters",
    "title": "Lecture 2",
    "section": "Approximating the posterior for the hyperparameters",
    "text": "Approximating the posterior for the hyperparameters\nWe want to build an approximation to \\(\\pi(\\theta|\\mathbf{y})\\)\n\\[\n\\pi(\\mathbf{\\theta} \\mid \\mathbf{y}) =  \\frac{\n           \\pi(\\mathbf{u}, \\mathbf{\n           \\theta\n}\\mid \\mathbf{y})}\n      {\\pi(\\mathbf{u} \\mid \\mathbf{y},\n            \\mathbf{\\theta})} \\: \\Bigg|_{\\text{This is just the Bayes theorem! It is valid for  any value of } \\mathbf{u}}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#approximating-the-posterior-for-the-hyperparameters-1",
    "href": "slides/slides_3.html#approximating-the-posterior-for-the-hyperparameters-1",
    "title": "Lecture 2",
    "section": "Approximating the posterior for the hyperparameters",
    "text": "Approximating the posterior for the hyperparameters\nWe want to build an approximation to \\(\\pi(\\theta|\\mathbf{y})\\)\n\\[\\begin{aligned}\n\\pi(\\mathbf{\\theta} \\mid \\mathbf{y}) & =  \\frac{\n           \\pi(\\mathbf{u}, \\mathbf{\n           \\theta\n}\\mid \\mathbf{y})}\n      {\\pi(\\mathbf{u} \\mid \\mathbf{y},\n            \\mathbf{\\theta})} \\\\\n            & \\propto \\frac{\n            \\overbrace{\\pi( \\mathbf{y}\\mid \\mathbf{u},\\mathbf{\\theta})\\  \\pi(\\mathbf{\\theta})}^{\\text{Non-Gaussian, KNOWN}}  \\overbrace{\\pi( \\mathbf{u}\\mid \\mathbf{\\theta})}^{\\text{ Gaussian, KNOWN}}  }\n            {\\underbrace{\\pi(\\mathbf{u} \\mid \\mathbf{y},\n            \\mathbf{\\theta})}_{\\text{Non-Gaussian,UNKNOWN}}}\n\\end{aligned}\\]\n\n\nWe approximate \\(\\pi(\\mathbf{u} \\mid \\mathbf{y},\\mathbf{\\theta})\\) with a Gaussian distribution \\(\\pi_G(\\mathbf{u} \\mid \\mathbf{y},\\mathbf{\\theta})\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#the-gmrf-approximation",
    "href": "slides/slides_3.html#the-gmrf-approximation",
    "title": "Lecture 2",
    "section": "The GMRF Approximation",
    "text": "The GMRF Approximation\n\\[\n\\begin{aligned}\n\\pi(\\mathbf{u}\\mid {\\theta},\\mathbf{y}) & \\propto \\pi(\\mathbf{u}\\mid {\\theta}) \\pi(\\mathbf{y}|\\mathbf{u},\\theta) \\\\\n& \\propto \\exp\\left\\{-\\frac{1}{2}\\mathbf{u}^T\\mathbf{Q}\\mathbf{u} + \\sum \\log \\pi(y_i|\\eta_i,\\theta)\\right\\}\\\\\n\\end{aligned}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#the-gmrf-approximation-1",
    "href": "slides/slides_3.html#the-gmrf-approximation-1",
    "title": "Lecture 2",
    "section": "The GMRF Approximation",
    "text": "The GMRF Approximation\n\\[\n\\begin{aligned}\n\\pi(\\mathbf{u}\\mid {\\theta},\\mathbf{y}) & \\propto \\pi(\\mathbf{u}\\mid {\\theta}) \\pi(\\mathbf{y}|\\mathbf{u},\\theta) \\\\\n& \\propto \\exp\\left\\{-\\frac{1}{2}\\mathbf{u}^T\\mathbf{Q}\\mathbf{u} + \\sum \\log \\pi(y_i|\\eta_i,\\theta)\\right\\}\\\\\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n& \\approx \\exp\\left\\{-\\frac{1}{2}\\mathbf{u}^T\\mathbf{Q}\\mathbf{u} + \\sum (b_i\\eta_i - \\frac{1}{2}c_i\\eta_i^2)\\right\\}\n\\end{aligned}\n\\]\nRecall \\[\n\\begin{aligned}\n        f(x) \\approx f(x_0) + f'(x_0)(x-x_0)+ \\frac{1}{2} f''(x_0)(x-x_0)^2\n        = a+ bx - \\frac{1}{2}cx^2\n\\end{aligned}\n\\] with\n\\[\n\\begin{aligned}\nb &=f'(x_0) - f''(x_0)x_0\\\\\nc &= -f''(x_0)\n\\end{aligned}\n\\].",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#the-gmrf-approximation-2",
    "href": "slides/slides_3.html#the-gmrf-approximation-2",
    "title": "Lecture 2",
    "section": "The GMRF Approximation",
    "text": "The GMRF Approximation\n\\[\n\\begin{aligned}\n\\pi(\\mathbf{u}\\mid {\\theta},\\mathbf{y}) & \\propto \\pi(\\mathbf{u}\\mid {\\theta}) \\pi(\\mathbf{y}|\\mathbf{u},\\theta) \\\\\n& \\propto \\exp\\left\\{-\\frac{1}{2}\\mathbf{u}^T\\mathbf{Q}\\mathbf{u} + \\sum \\log \\pi(y_i|\\eta_i,\\theta)\\right\\}\\\\\n& = \\exp \\left\\{ -\\frac{1}{2} \\mathbf{u}^T\\tilde{\\mathbf{Q}}  \\mathbf{u} + \\tilde{\\mathbf{b}}\\mathbf{u}\\right\\}\n\\end{aligned}\n\\] Which means that we approximate \\(\\pi(\\mathbf{u}\\mid {\\theta},\\mathbf{y})\\) with\n\\[\n\\mathcal{N}(\\tilde{\\mathbf{Q}}^{-1}\\tilde{\\mathbf{b}},\\tilde{\\mathbf{Q}}^{-1})\\  \\text{ with   }\\ \\tilde{\\mathbf{Q}} = \\mathbf{Q} +\\begin{bmatrix}\n\\text{diag}(\\mathbf{c}) & 0\\\\\n0& 0\n\\end{bmatrix}\\qquad \\tilde{\\mathbf{b}} = [\\mathbf{b}\\ 0]^T\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#the-gmfr-approximation---one-dimensional-example",
    "href": "slides/slides_3.html#the-gmfr-approximation---one-dimensional-example",
    "title": "Lecture 2",
    "section": "The GMFR approximation - One dimensional example",
    "text": "The GMFR approximation - One dimensional example\n\n\nAssume \\[\n\\begin{aligned}\n  y|\\lambda \\sim\\text{Poisson}(\\lambda)  & \\text{ Likelihood}\\\\\n  \\eta = \\log(\\lambda)  & \\text{ Predictor}\\\\\n  \\mathbf{u}  = \\eta & \\text{ Latent field }\\\\\n  x\\sim\\mathcal{N}(0,1) & \\text{ Latent Model}\n\\end{aligned}\n\\] we have that\n\\[\n\\begin{aligned}\n  \\pi(x|y) & \\propto\\pi(y|x)\\pi(x)\\\\\n  & \\propto\\exp\\{ -\\frac{1}{2}x^2+\n  \\underbrace{xy-\\exp(x)}_{\\text{non-gaussian part}}\n  \\}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#the-gmfr-approximation",
    "href": "slides/slides_3.html#the-gmfr-approximation",
    "title": "Lecture 2",
    "section": "The GMFR approximation",
    "text": "The GMFR approximation\n\nüòÑ Easy to compute\nüòÑ Exact for Gaussian likelihood\nüòÑ Good when you have little or a lot of data\nüòÑ inherits the same sparseness of \\(\\mathbf{u}\\)\nüòü No skewness",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#the-laplace-approximation",
    "href": "slides/slides_3.html#the-laplace-approximation",
    "title": "Lecture 2",
    "section": "The Laplace Approximation",
    "text": "The Laplace Approximation\n\n\n\\[\n\\begin{aligned}\n\\pi(\\mathbf{\\theta} \\mid  \\mathbf{y})  & \\propto \\frac{\n            \\pi( \\mathbf{y}\\mid \\mathbf{u},\\mathbf{\\theta})\\  \\pi(\\mathbf{\\theta})  \\pi( \\mathbf{u}\\mid \\mathbf{\\theta})  }\n            {\\pi(\\mathbf{u} \\mid \\mathbf{y},\n            \\mathbf{\\theta})}\\\\\n            & \\approx \\frac{\n            \\pi( \\mathbf{y}\\mid \\mathbf{u},\\mathbf{\\theta})\\  \\pi(\\mathbf{\\theta})  \\pi( \\mathbf{u}\\mid \\mathbf{\\theta})  }\n            {\\pi_G(\\mathbf{u} \\mid \\mathbf{y},\n            \\mathbf{\\theta})}  \\: \\Bigg|_{\\mathbf{u} = \\mathbf{u}^\\star(\\mathbf{\\theta})}\n\\end{aligned}\n\\]\nIf \\(\\theta\\) is low dimension we can actually compute this!\n\n\n\n\n\n\n\n\n\n\n\nThis is the Laplace approximation to \\(\\pi(\\theta|\\mathbf{y})\\) !",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#the-inla-scheme",
    "href": "slides/slides_3.html#the-inla-scheme",
    "title": "Lecture 2",
    "section": "The INLA scheme",
    "text": "The INLA scheme\n\nApproximate \\(\\pi(\\theta|\\mathbf{y})\\) ‚úÖ\nApproximate \\(\\pi(u_i|\\theta,\\mathbf{y})\\)\nUse numerical integration to compute \\(\\pi(u_i|\\mathbf{y})\\) ‚úÖ",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#approximating-the-posterior-for-the-latent-field.",
    "href": "slides/slides_3.html#approximating-the-posterior-for-the-latent-field.",
    "title": "Lecture 2",
    "section": "Approximating the posterior for the latent field.",
    "text": "Approximating the posterior for the latent field.\n\nNow we have an aproximation for \\(\\pi(\\theta|\\mathbf{y})\\)\nWe need an approximation for \\(\\pi(u_i|\\theta,\\mathbf{y})\\)\nThis is more difficult‚Ä¶ \\(\\mathbf{u}\\) is large!! Anything we do we have to do it many times!\n\n\nCan we use the marginal from \\(\\pi_G(\\mathbf{u}|\\theta, \\mathbf{y})\\) ?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#unfortunately-it-is-not-accurate-enough",
    "href": "slides/slides_3.html#unfortunately-it-is-not-accurate-enough",
    "title": "Lecture 2",
    "section": "‚Ä¶unfortunately it is not accurate enough!",
    "text": "‚Ä¶unfortunately it is not accurate enough!\n\n\n\n\n\n\n\n\nFrom Rue and Martino 2007\n\n\nNotes:\n\nResults are quite accurate in general\nThere can be errors in the location and/or errors due to the lack of skewness‚Ä¶",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#inla-and-variational-bayes",
    "href": "slides/slides_3.html#inla-and-variational-bayes",
    "title": "Lecture 2",
    "section": "INLA and Variational Bayes",
    "text": "INLA and Variational Bayes\n\nVariational Bayes techniques are used to improve the mean of the GMRF approximation \\(\\pi_G(\\mathbf{u}|\\theta, \\mathbf{y})\\)\nFrom this improved GMRF approximation we can compute the marginals \\(\\pi_G^*(u_i|\\theta,\\mathbf{y})\\) that can then be used to numerically compute: \\[\n\\tilde{\\pi}(u_i|\\mathbf{y}) = \\sum_k\\pi_G^*(u_i|\\theta, \\mathbf{y})\\tilde{\\pi}(\\theta|\\mathbf{y)}\\ w_k\n\\]\n\n\nNOTE:\n\nWe know the precision matrix \\(\\mathbf{Q}\\) of the GRMF approximation \\(\\pi_G^*(\\mathbf{u}|\\theta, \\mathbf{y})\\)\nEfficient algoriths to compute the marginal variance are used.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#inla-posterior-sampling",
    "href": "slides/slides_3.html#inla-posterior-sampling",
    "title": "Lecture 2",
    "section": "INLA posterior sampling",
    "text": "INLA posterior sampling\nAlthough INLA mainly computes posterior marginals\n\n\\(\\pi(\\theta|\\mathbf{y})\\)\n\\(\\pi(u_i|\\mathbf{y})\\)\n\nINLA allowes to sample from the approximate posterior \\(\\pi(\\mathbf{u}|\\mathbf{y})\\)\nThis is important is one is interested in the posterior of functions of one or more parameters.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#take-home-messages",
    "href": "slides/slides_3.html#take-home-messages",
    "title": "Lecture 2",
    "section": "Take home messages!",
    "text": "Take home messages!\n\nThe INLA methodology heavily relies on sparse matrices !\nGMRF are the key tool in INLA-friendly models\nINLA uses numerical approximation and numerical integration to provide efficient and precise approximations to posterior marginals\n\nBut ultimately‚Ä¶.\n\nYou do not need to deeply understand all of this if you want to use inlabru to solve your (statistical) problems üòÑ",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#good-news",
    "href": "slides/slides_3.html#good-news",
    "title": "Lecture 2",
    "section": "Good news!",
    "text": "Good news!\nRecall‚Ä¶ we have computationally efficient and flexible methodology‚Ä¶\n\nwe can fit more (realistically) complex models‚Ä¶\ne.g.¬†spatial and spatio-temporal models (to geo-referenced data, point pattern data, aerial data, spatio-temporal data‚Ä¶)\n\n\nUsing INLA + SPDE approach we can:\n\nfit complex models quickly\nflexibly fit joint models of more than one response variable\nmodel on complex domains, (e.g.¬†the sphere = the earth)\nobservation areas with barriers (islands, archipelagos‚Ä¶)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "slides/slides_3.html#how-to-use-the-inla-approach",
    "href": "slides/slides_3.html#how-to-use-the-inla-approach",
    "title": "Lecture 2",
    "section": "How to use the INLA approach",
    "text": "How to use the INLA approach\nThe INLA methodology is implemented in two R packages: R-INLA and inlabru.\nWhy do we (strongly) recommend inlabru:\n\n\nwrapper around R-INLA + extra functionality.\nmuch easier to fit spatial models\nfull support for sf and terra libraries\nmakes fitting of tricky models (e.g.¬†multi-likelihood models, spatial point processes) less fiddly than R-INLA\ntakes observation process into account (e.g.¬†for distance sampling, plot sampling); more on this later\nallows for non-linear predictors defined by general R expressions (this is done by linearizing and running the INLA approximation several times‚Ä¶)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 2"
    ]
  },
  {
    "objectID": "day5_practical_9.html",
    "href": "day5_practical_9.html",
    "title": "Practical 8 - Zero Inflated",
    "section": "",
    "text": "Aim of this practical:\nIn this practical we are going to look zero-inflated models.\nDownload Practical 9 R script"
  },
  {
    "objectID": "day5_practical_9.html#sec-prep",
    "href": "day5_practical_9.html#sec-prep",
    "title": "Practical 8 - Zero Inflated",
    "section": "Data Preparation",
    "text": "Data Preparation\nThe following example use the gorillas dataset available in the inlabru library.\nThe data give the locations of Gorilla‚Äôs nests in an area:\n\ngorillas_sf &lt;- inlabru::gorillas_sf\nnests &lt;- gorillas_sf$nests\nboundary &lt;- gorillas_sf$boundary\n\nggplot() + geom_sf(data = nests) +\n  geom_sf(data = boundary, alpha = 0)\n\n\n\n\nLocation of gorilla nests\n\n\n\n\nThe dataset also contains covariates in the form or raster data. We consider two of them here:\n\ngcov = gorillas_sf_gcov()\nelev_cov &lt;- gcov$elevation\ndist_cov &lt;-  gcov$waterdist\n\n\n\n\n\n\nCovariates\n\n\n\n\nNote: the covariates have been expanded to cover all the nodes in the mesh.\n\nTo obtain the count data, we rasterize the species counts to match the spatial resolution of the covariates available. Then we aggregate the pixels to a rougher resolution (5x5 pixels in the original covariate raster dimensions). Finally, we mask regions outside the study area.\nIn addition we compute the area of each grid cell.\n\n# Rasterize data\ncounts_rstr &lt;-\n  terra::rasterize(vect(nests), gcov, fun = sum, background = 0) %&gt;%\n  terra::aggregate(fact = 5, fun = sum) %&gt;%\n  mask(vect(sf::st_geometry(boundary)))\nplot(counts_rstr)\n\n\n\n\nCounts of gorilla nests\n\n\n\n# compute cell area\ncounts_rstr &lt;- counts_rstr %&gt;%\n  cellSize(unit = \"km\") %&gt;%\n  c(counts_rstr)\n\nTo create our dataset of counts, we extract also the coordinate of center point of each raster pixel. In addition we create a column with presences and one with the pixel area\n\ncounts_df &lt;- crds(counts_rstr, df = TRUE, na.rm = TRUE) %&gt;%\n  bind_cols(values(counts_rstr, mat = TRUE, na.rm = TRUE)) %&gt;%\n  rename(count = sum) %&gt;%\n  mutate(present = (count &gt; 0) * 1L) %&gt;%\n  st_as_sf(coords = c(\"x\", \"y\"), crs = st_crs(nests))\n\nWe then aggregate the covariates to the same resolution as the nest counts and scale them.\n\nelev_cov1 &lt;- elev_cov %&gt;% \n  terra::aggregate(fact = 5, fun = mean) %&gt;% scale()\ndist_cov1 &lt;- dist_cov %&gt;% \n  terra::aggregate(fact = 5, fun = mean) %&gt;% scale()\n\n\n\n\n\n\n\n\n\nFigure¬†1: Covariates\n\n\n\n\n\n\nMesh building\nWe now define the mesh and the spde object.\n\n\nmesh &lt;- fm_mesh_2d(\n  loc = st_as_sfc(counts_df),\n  max.edge = c(0.5, 1),\n  crs = st_crs(counts_df)\n)\n\nmatern &lt;- inla.spde2.pcmatern(mesh,\n  prior.sigma = c(1, 0.01),\n  prior.range = c(5, 0.01)\n)\n\n\n\n\n\n\nMesh over the count locations\n\n\n\n\nIn our dataset, the number of zeros is quite substantial, and our model may struggle to account for them adequately. To address this, we should select a model capable of handling an ‚Äúinflated‚Äù number of zeros, exceeding what a standard Poisson model would imply. For this purpose, we opt for a ‚Äúzero-inflated Poisson model,‚Äù commonly abbreviated as ZIP."
  },
  {
    "objectID": "day5_practical_9.html#sec-zip",
    "href": "day5_practical_9.html#sec-zip",
    "title": "Practical 8 - Zero Inflated",
    "section": "Zero-Inflated model (Type1)",
    "text": "Zero-Inflated model (Type1)\nWe fit now a Zero-Inflated model to our data.\nThe Type 1 Zero-inflated Poisson model is defined as follows:\n\\[\n\\text{Prob}(y\\vert\\dots)=\\pi\\times 1_{y=0}+(1-\\pi)\\times \\text{Poisson}(y)\n\\]\nHere, \\(\\pi=\\text{logit}^{-1}(\\theta)\\)\nThe expected value and variance for the counts are calculated as:\n\\[\n\\begin{gathered}\nE(count)=(1-\\pi)\\lambda \\\\\nVar(count)= (1-\\pi)(\\lambda+\\pi \\lambda^2)\n\\end{gathered}\n\\tag{1}\\]\nThis model has two parameters:\n\nThe probability of excess zero \\(\\pi\\) - This is a hyperparameter and therefore it is constant\nThe mean of the Poisson distribution \\(\\lambda\\). This is linked to the linear predictor as: \\[\n\\eta = E\\log(\\lambda) = \\log(E) + \\beta_0 + \\beta_1\\text{Elevation} + \\beta_2\\text{Distance } + u\n\\] where \\(\\log(E)\\) is an offset (the area of the pixel) that accounts for the size of the cell.\n\n\n\n\n\n\n\nWarning Task\n\n\n\nFit a zero-inflated model to the data (zeroinflatedpoisson1) by completing the following code:\n\ncmp = ~ Intercept(1) + elevation(...) + distance(...) + space(...)\n\nlik = bru_obs(...,\n    E = area)\n\nfit_zip &lt;- bru(cmp, lik)\n\n\n\nTake hint\n\nThe E = area is an offset that adjusts for the size of each cell.\n\n\n\n\nClick here to see the solution\n\n\nCode\n\ncmp = ~ Intercept(1) + elevation(elev_cov1, model = \"linear\") + distance(dist_cov1, model = \"linear\") + space(geometry, model = matern)\n\n\n\nlik = bru_obs(formula = count ~ .,\n    family = \"zeroinflatedpoisson1\", \n    data = counts_df,\n    E = area)\n\nfit_zip &lt;- bru(cmp, lik)\n\n\n\n\n\nOnce the model is fitted we can look at the results\n\n\n\n\n\n\nWarning Task\n\n\n\nCheck what the estimated excess zero probaility is.\nUse the predict() function to look at the estimated \\(\\lambda(s)\\) and mean count in Equation¬†1\n\n\nTake hint\n\nTo get the right name for the hyperparameters to use in the predict() function, you can use the function bru_names().\n\n\n\n\nClick here to see the solution\n\n\nCode\n# to check the estimated excess zero probability:\n# fit_zip$summary.hyperpar\n\npred_zip &lt;- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- (1-pi) * lambda\n    variance &lt;- (1-pi) * (lambda + pi * lambda^2)\n    list(\n      lambda = lambda,\n      expect = expect,\n      variance = variance\n    )\n  },\n  n.samples = 2500\n)\n\n\n\n\n\n\n\n\n\n\nEstimated \\(\\lambda\\) (left) and expected counts (right) with zero inflated model"
  },
  {
    "objectID": "day5_practical_9.html#sec-zap",
    "href": "day5_practical_9.html#sec-zap",
    "title": "Practical 8 - Zero Inflated",
    "section": "Hurdle model (Type0)",
    "text": "Hurdle model (Type0)\nWe now fit a hurdle model to the same data.\nIn the zeroinflatedpoisson0 model is defined by the following observation probability model\n\\[\n\\text{Prob}(y\\vert\\dots)=\\pi\\times 1_{y=0}+(1-\\pi)\\times \\text{Poisson}(y\\vert y&gt;0)\n\\]\nwhere \\(\\pi\\) is the probability of zero.\nThe expectation and variance of the counts are as follows:\n\\[\n\\begin{aligned}\nE(\\text{count})&=\\frac{1}{1-\\exp(-\\lambda)}\\pi\\lambda \\\\\nVar(\\text{count})&=  E(\\text{count}) \\left(1-\\exp(-\\lambda) E(\\text{count})\\right)\n\\end{aligned}\n\\tag{2}\\]\n\n\n\n\n\n\nWarning Task\n\n\n\nFit a hurdle model to the data using the zeroinflatedpoisson0 likelihood\n\n\nTake hint\n\nYou do not need to redefine the components as the linear predictor is not changing.\n\n\n\n\nClick here to see the solution\n\n\nCode\nlik = bru_obs(formula = count ~ .,\n    family = \"zeroinflatedpoisson0\", \n    data = counts_df,\n    E = area)\n\nfit_zap &lt;- bru(cmp, lik)\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nAs before, check what the estimated probability of zero is and use predict() to obtain a map of the estimated mean counts in Equation¬†2 over the domain.\n\n\nTake hint\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n\npred_zap &lt;- predict( fit_zap, counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      lambda = lambda,\n      expect = expect\n    )\n  },\n  n.samples = 2500\n)\n\n\n\n\n\n\n\n\n\n\nEstimated \\(\\lambda\\) (left) and expected counts (right) with hurdle model"
  },
  {
    "objectID": "day5_practical_9.html#sec-two-lik",
    "href": "day5_practical_9.html#sec-two-lik",
    "title": "Practical 8 - Zero Inflated",
    "section": "Hurdle model using two likelihoods",
    "text": "Hurdle model using two likelihoods\nHere the model is the same as in Section¬†1.3, but this time we also want to model \\(\\pi\\) using covariates and random effects. Therefore we define a second linear predictor \\[\n\\eta^2 =\\beta_0^2 + \\beta_1^2\\text{Elevation} +  \\beta_2^2\\text{Distance} + u^2\n\\] Note here we have defined the two linear predictor to use the same covariates, but this is not necessary, they can be totally independent.\nTo fit this model we have to define two likelihoods: - One will account for the presence-absence process and has a Binomial model - One will account for the counts and has a truncated Poisson model\n\n\n\n\n\n\nWarning Task\n\n\n\nComplete the following code to fit a hurdle model based on two likelihoods:\n\n# define components\ncmp &lt;- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    space_count(geometry, model = matern) +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space_presence(geometry, model = matern) \n\n# positive count model\npos_count_obs &lt;- bru_obs(formula = ...,\n      family = ...,\n      data = counts_df[counts_df$present &gt; 0, ],\n      E = area)\n  \n# presence model\npresence_obs &lt;- bru_obs(formula ...,\n  family = ...,\n  data = counts_df,\n)\n\n# fit the model\nfit_zap2 &lt;- bru(...)\n\n\n\nTake hint\n\nAdd hint details here‚Ä¶\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp &lt;- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    space_count(geometry, model = matern) +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space_presence(geometry, model = matern) \n\n\npos_count_obs &lt;- bru_obs(formula = count ~ Intercept_count + elev_count + \n                                   dist_count + space_count,\n      family = \"nzpoisson\",\n      data = counts_df[counts_df$present &gt; 0, ],\n      E = area)\n  \n\npresence_obs &lt;- bru_obs(formula = present ~ Intercept_presence + elev_presence + dist_presence +\n                          space_presence,\n  family = \"binomial\",\n  data = counts_df,\n)\n\nfit_zap2 &lt;- bru(\n  cmp,\n  presence_obs,\n  pos_count_obs\n)"
  },
  {
    "objectID": "day5_practical_9.html#sec-two-lik-share",
    "href": "day5_practical_9.html#sec-two-lik-share",
    "title": "Practical 8 - Zero Inflated",
    "section": "Hurdle model using two likelihoods and a shared component",
    "text": "Hurdle model using two likelihoods and a shared component\nNote that in the model above, there is no direct link between the parameters of the two observation parts, and we could estimate them separately. However, the two likelihoods could share some of the components; for example the space_count component could be used for both predictors. This would be possible using the copy argument.\nWe would then need to define one component as space(geometry, model = matern) and then a copy of it as space_copy(geometry, copy = \"space\", fixed = FALSE).\nThe results from the model in ?@sec-sec-two-lik show that the estimated covariance parameters for the two fields are very different, so it is probably not sensible to share the same component between the two parts. We do it anyway to show an example:\n\ncmp &lt;- ~\n  Intercept_count(1) +\n    elev_count(elev_cov1, model = \"linear\") +\n    dist_count(dist_cov1, model = \"linear\") +\n    Intercept_presence(1) +\n    elev_presence(elev_cov1, model = \"linear\") +\n    dist_presence(dist_cov1, model = \"linear\") +\n    space(geometry, model = matern) +\n  space_copy(geometry, copy = \"space\", fixed = FALSE)\n\n\npos_count_obs &lt;- bru_obs(formula = count ~ Intercept_count + elev_count + dist_count + space,\n      family = \"nzpoisson\",\n      data = counts_df[counts_df$present &gt; 0, ],\n      E = area)\n\npresence_obs &lt;- bru_obs(formula = present ~ Intercept_presence + elev_presence + dist_presence + space_copy,\n  family = \"binomial\",\n  data = counts_df)\n\nfit_zap3 &lt;- bru(\n  cmp,\n  presence_obs,\n  pos_count_obs)"
  },
  {
    "objectID": "day5_practical_9.html#comparing-models",
    "href": "day5_practical_9.html#comparing-models",
    "title": "Practical 8 - Zero Inflated",
    "section": "Comparing models",
    "text": "Comparing models\nWe have fitted four different models. Now we want to compare them and see how they fit the data.\n\nComparing model predictions\nWe first want to compare the estimated surfaces of expected counts. To do this we want to produce the estimated expected counts, similar to what we did in Section¬†1.2 and Section¬†1.3 for all four models and plot them together:\n\npred_zip &lt;- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- (1-pi) * lambda\n    variance &lt;- (1-pi) * (lambda + pi * lambda^2)\n    list(\n      expect = expect\n    )\n  },n.samples = 2500)\n\npred_zap &lt;- predict( fit_zap, counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\ninv.logit = function(x) (exp(x)/(1+exp(x)))\n\npred_zap2 &lt;- predict( fit_zap2, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_presence)\n    lambda &lt;- area * exp( dist_count + elev_count + space_count + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\npred_zap3 &lt;- predict( fit_zap3, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_copy)\n    lambda &lt;- area * exp( dist_count + elev_count + space + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    list(\n      expect = expect)\n  },n.samples = 2500)\n\n\n\n\np =   data.frame(x = st_coordinates(counts_df)[,1],\n             y = st_coordinates(counts_df)[,2],\n    zip = pred_zip$expect$mean,\n         hurdle = pred_zap$expect$mean,\n         hurdle2 = pred_zap2$expect$mean,\n         hurdle3 = pred_zap3$expect$mean)  %&gt;%\n  pivot_longer(-c(x,y)) %&gt;%\n  ggplot() + geom_tile(aes(x,y, fill = value)) + facet_wrap(.~name) +\n    theme_map + scale_fill_scico(direction = -1)\n\n\n\n\n\n\n\nWarning Task\n\n\n\nCreate plots of the estimated variance of the counts.\n\n\nTake hint\n\nThe fomulas for the variances are in Equation¬†1 and Equation¬†2.\n\n\n\n\nClick here to see the solution\n\n\nCode\npred_zip &lt;- predict(\n  fit_zip, \n  counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_1\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    variance &lt;- (1-pi) * (lambda + pi * lambda^2)\n    list( variance = variance)\n  },n.samples = 2500)\n\npred_zap &lt;- predict( fit_zap, counts_df,\n  ~ {\n    pi &lt;- zero_probability_parameter_for_zero_inflated_poisson_0\n    lambda &lt;- area * exp( distance + elevation + space + Intercept)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\ninv.logit = function(x) (exp(x)/(1+exp(x)))\n\npred_zap2 &lt;- predict( fit_zap2, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_presence)\n    lambda &lt;- area * exp( dist_count + elev_count + space_count + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\npred_zap3 &lt;- predict( fit_zap3, counts_df,\n  ~ {\n    pi &lt;- inv.logit(Intercept_presence + elev_presence + dist_presence + space_copy)\n    lambda &lt;- area * exp( dist_count + elev_count + space + Intercept_count)\n    expect &lt;- ((1-exp(-lambda))^(-1) * pi * lambda)\n    variance = expect *(1-exp(-lambda) * expect)\n    list(variance = variance)\n  },\n  n.samples = 2500)\n\n\n\n\n  data.frame(x = st_coordinates(counts_df)[,1],\n             y = st_coordinates(counts_df)[,2],\n    zip = pred_zip$variance$mean,\n         hurdle = pred_zap$variance$mean,\n         hurdle2 = pred_zap2$variance$mean,\n         hurdle3 = pred_zap3$variance$mean)  %&gt;%\n  pivot_longer(-c(x,y)) %&gt;%\n  ggplot() + geom_tile(aes(x,y, fill = value)) + facet_wrap(.~name) +\n    theme_map + scale_fill_scico(direction = -1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing scores\nWe can compare model using the scores that the bru() function computes since we have set, at the beginning. the options to\n\nbru_options_set(control.compute = list(dic = TRUE,\n                                       waic = TRUE,\n                                       mlik = TRUE,\n                                       cpo = TRUE))\n\nLets use these scores to compare the models.\n\n\n\n\n\n\nWarning Task\n\n\n\nExtract the DIC, WAIC and MLIK values for the four models and compare them\n\n\n\nClick here to see the solution\n\n\nCode\ndata.frame( Model = c(\"ZIP\", \"HURDLE\", \"HURDLE_2\",\"HURDLE_3\" ),\n  DIC = c(fit_zip$dic$dic, fit_zap$dic$dic, WAIC = fit_zap2$dic$dic, fit_zap3$dic$dic),\n            WAIC = c(fit_zip$waic$waic, fit_zap$waic$waic, fit_zap2$waic$waic, fit_zap3$waic$waic),\n            MLIK = c(fit_zip$mlik[1], fit_zap$mlik[1], fit_zap2$mlik[1], fit_zap3$mlik[1]))\n#&gt;      Model      DIC     WAIC      MLIK\n#&gt; 1      ZIP 1214.133 1223.711 -686.9499\n#&gt; 2   HURDLE 1886.065 1909.721 -994.3807\n#&gt; 3 HURDLE_2 1268.306 1285.507 -734.3704\n#&gt; 4 HURDLE_3 1884.262 3912.861 -858.1406\n\n\n\n\n\nFrom the table above we can see that the model that best balances complexity and fit is the zero inflated one (ZIP)."
  },
  {
    "objectID": "day4_practical_7.html",
    "href": "day4_practical_7.html",
    "title": "Practical 7",
    "section": "",
    "text": "Aim of this practical:\nIn this first practical we are going to look multiple likelihood models\nDownload Practical 7 R script\nLibraries to load:\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(inlabru) \nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\n\n# load some libraries to generate nice map plots\nlibrary(scico)\nlibrary(ggplot2)\nlibrary(patchwork)"
  },
  {
    "objectID": "day4_practical_7.html#multiple-likelihood-models",
    "href": "day4_practical_7.html#multiple-likelihood-models",
    "title": "Practical 7",
    "section": "Multiple likelihood models",
    "text": "Multiple likelihood models\n\nSimple simulated example\nIn this example we are going to fit a model where one covariate acts in the same way for two different responses\n\nGaussian observations\nPoisson observations\n\nWe first define the unobserved latent field and then simulate both Gaussian and Poisson observations.\n\nN = 200\nx =  runif(N)\ndf = data.frame(idx = 1:N,\n                x = x)\n\n# simulate data\ndf = df %&gt;% \n  mutate(y_gaus = rnorm(N, mean = 1 + 1.5 * x), sd = 0.5) %&gt;%\n  mutate(y_pois = rpois(N, lambda  = exp( -1 + 1.5 * x))) \n\n# plot the data\ndf %&gt;% ggplot() + \n  geom_point(aes(x, y_gaus, color = \"Gaussian\")) +\n  geom_point(aes(x, y_pois, color = \"Poisson\")) \n\n\n\n\n\n\n\n\nThe model we aim to fit is\n\\[\n\\begin{aligned}\n\\eta_t & = \\beta_0 + \\beta_1x_t,\\ t = 1,\\dots,T\\\\\ny^{\\text{Gaus}} &\\sim\\mathcal{N}(\\mu_t,\\sigma^2_y)\\\\\n\\eta^{\\text{Gaus}}_t  & = \\mu_t\\\\\ny^{\\text{Pois}}_t & \\sim\\text{Poisson}(\\lambda_t)\\\\\n\\eta^{\\text{Pois}}_t  & =\\log(\\lambda_t)\n\\end{aligned}\n\\]\nWe first define the components\n\ncmp = ~ -1 + \n  Intercept_gaus(1) + \n  Intercept_pois(1) +\n  covariate(x, model = \"linear\") \n\nand the two likelihoods:\n\nlik_gaus = bru_obs(formula = y_gaus ~ Intercept_gaus + covariate,\n                    data = df)\n\nlik_pois = bru_obs(formula = y_pois ~ Intercept_pois + covariate,\n                    data = df,\n                   family = \"poisson\")\n\n\n\n\n\n\n\nWarning Task\n\n\n\nFit three models using the components and likelihoods above:\n\nA model that only considers the Gaussian data\nA model that only considers the Poisson data\nA model that considers both data sets\n\n\n\n\nClick here to see the solution\n\nfit_gaus = bru(cmp, lik_gaus)\n\nfit_pois = bru(cmp, lik_pois)\n\nfit_join = bru(cmp, lik_gaus, lik_pois)\n\n\nggplot() + \n  geom_line(data = fit_gaus$marginals.fixed$covariate, aes(x,y, color = \"Gaussian\"))+ \n  geom_line(data = fit_pois$marginals.fixed$covariate, aes(x,y, color = \"Poisson\"))+ \n  geom_line(data = fit_join$marginals.fixed$covariate, aes(x,y, color = \"Joint\"))+\n  geom_vline(xintercept = 1.5, linetype = \"dashed\")"
  },
  {
    "objectID": "day4_practical_7.html#coregionalization-model",
    "href": "day4_practical_7.html#coregionalization-model",
    "title": "Practical 7",
    "section": "Coregionalization model",
    "text": "Coregionalization model\nIn this practical we present a way to fit the Bayesian coregionalization model similar to the one presented in Chapter 8 in Blangiardo and Cameletti (2015).\nThese models are often used when measurement stations record several variables; for example, a station measuring pollution may register values of CO2 and NO2. Instead of modelling these as several univariate datasets, the models we present in this section deal with the joint dependency structure. Dependencies among the different outcomes are modelled through shared components at the predictor level.\nUsually, in coregionalization models, the different responses are assumed to be observed at the same locations. With the INLA-SPDE approach, we do not require the different outcome variables to be measured at the same locations. Hence, in the code example below we show how to model responses observed at different locations.\n\nThe model\nWe consider the following model \\[\n\\begin{aligned}\ny_1(s) & = \\beta_1 + \\omega_1(s) + e_1(s)\\\\\ny_2(s) & = \\beta_2 + \\lambda\\ \\omega_1(s) + \\omega_2(s) +e_1(s),\n\\end{aligned}\n\\] where the\n\n\\(\\beta_k\\) are intercepts, \\(k=1,2\\).\n\\(\\omega_1(s)\\) is a Gaussian spatial effect that is common for both observations\n\\(\\lambda\\) is a weight for the spatial effect \\(\\omega_1(s)\\).\n\\(\\omega_2(s)\\) is a Gaussian spatial effect specific to the second observations\n\\(e_k(s)\\) are uncorrelated error terms, \\(k=1,2\\).\n\n\n\nData simulation\nWe start by simulating the data. We first define a function to sample from a Matern RF with given range and sd.\n\nrMatern &lt;- function(n, coords, sigma=1, range, \n                    kappa = sqrt(8*nu)/range, \n                    variance = sigma^2, \n                    nu=1) {\n  m &lt;- as.matrix(dist(coords))\n  m &lt;- exp((1-nu)*log(2) + nu*log(kappa*m)-\n             lgamma(nu))*besselK(m*kappa, nu)\n  diag(m) &lt;- 1\n  return(drop(crossprod(chol(variance*m),\n                        matrix(rnorm(nrow(coords)*n), ncol=n))))\n}\n\nWe then define the true values of all parameters\n\n# Intercept on reparametrized model\nbeta &lt;- c(-5, 3) \n# Random field marginal variances for omega1 and omega2:\nm.var &lt;- c(0.5, 0.4) \n# GRF range parameters for omega1 and omega2:\nrange &lt;- c(4, 6)\n# Copy parameters: reparameterization of coregionalization \n# parameters\nlambda &lt;- c(0.7) \n# Standard deviations of error terms\ne.sd &lt;- c(0.3, 0.2)\n\nand simulate our data. We assume that we observe data in a window \\((0:10)\\times(0:5)\\). We assume that in some locations we observe both \\(y_1\\) and \\(y_2\\) while in others we only observe one of them\n\n# define the area of interest\npoly_geom = st_polygon(list(cbind(c(0,10,10,0,0), c(0,0,5,5,0)) ))\n# Wrap it in an sfc (simple feature collection)\npoly_sfc &lt;- st_sfc(poly_geom)\n# Now create the sf object\nborder &lt;- st_sf(id = 1, geometry = poly_sfc)\n\n\n\n# how many observation we have\nn1 &lt;- 200\nn2 &lt;- 150\nn_common = 50\n\n# simulate observation locations\n\nloc_common = st_sf(geometry = st_sample(border, n_common))\nloc_only1 = st_sf(geometry = st_sample(border, n1-n_common))\nloc_only2 = st_sf(geometry = st_sample(border, n2-n_common))\n\n\n\n# simulate the two gaussian field at the locations\nz1 &lt;- rMatern(1, st_coordinates(rbind( loc_common,loc_only1, loc_only2)), range = range[1],\n                  sigma = sqrt(m.var[1]))\n\nz2 &lt;- rMatern(1, st_coordinates(rbind(loc_common, loc_only2)), range = range[2],\n                  sigma = sqrt(m.var[2]))\n\n\n## Create data.frame\nloc1 = rbind( loc_common, loc_only1)\nloc2 = rbind( loc_common, loc_only2)\n\ndf1 =  loc1 %&gt;% mutate(z1 = z1[1:n1])\ndf2 =  loc2 %&gt;% mutate(z1 = z1[-c(1:(n1-n_common))], z2 =z2)\n\n\n## create the linear predictors\n\ndf1  = df1 %&gt;%\n  mutate(eta1 = beta[1] + z1)\n\ndf2  = df2 %&gt;%\n  mutate(eta2 = beta[2] + lambda * z1 + z2)\n\n\n# simulate data by addint the obervation noise\n\ndf1  = df1 %&gt;%\n  mutate(y = rnorm(n1, mean = eta1, sd = e.sd[1]))\n\ndf2  = df2 %&gt;%\n  mutate(y = rnorm(n2, mean = eta2, sd = e.sd[1]))\n\nWe can visualize the observations\n\np1 = ggplot(data = df1) + geom_sf(aes(color = z1)) \np2 = ggplot(data = df2) + geom_sf(aes(color = z2)) \np1+p2+plot_layout(ncol = 1)\n\n\n\n\n\n\n\n\n\n\nMesh definition\nWe need to define a mesh. We use the location observations and the area of interest as a starting point.\n\nmesh &lt;-  fm_mesh_2d(loc = rbind(loc1, loc2), \n                   boundary = border,\n                     max.edge = c(0.5, 1.5), \n                     offset = c(0.1, 2.5), \n                     cutoff = 0.1)\n\nHere is the mesh, together with the observations \\(y_1\\) (red) and \\(y_2\\) (black).\n\n\n\n\n\n\n\n\n\n\n\nSPDE definition\n\n\n\n\n\n\nWarning Task\n\n\n\nDefine one spde object that contains information about priors for the range and the standard deviation.\nUse the same priors for both \\({ z}_1\\) and \\({ z}_2\\) so we create only one spde object.\n\\[\n\\begin{aligned}\n\\text{Prob(range}&lt;0.5)& = 0.01\\\\\n\\text{Prob(sd}&gt;1) &= 0.01\\\\\n\\end{aligned}\n\\]\n\n\n\nClick here to see the solution\n\nspde &lt;- inla.spde2.pcmatern(\n  mesh = mesh,\n  prior.range = c(0.5, 0.01), # P(range &lt; 0.5) = 0.01\n  prior.sigma = c(1, 0.01)) # P(sigma &gt; 1) = 0.01\n\n\n\n\n\n\nRun the model\nWe now need to define the components of the model:\n\ncmp = ~ -1 +  Intercept1(1) + Intercept2(1) +\n  omega1(geometry, model = spde) +\n  omega1_copy(geometry, copy = \"omega1\", fixed = FALSE) +\n  omega2(geometry, model = spde)\n\n\n\n\n\n\n\nWarning Task\n\n\n\nRun the joint model. To do this you need to\n\nDefine the two likelihoods using the bru_obs function\nFit the model using the bru() function\n\n\\[\n\\begin{aligned}\n\\text{Prob(range}&lt;0.5)& = 0.01\\\\\n\\text{Prob(sd}&gt;1) &= 0.01\\\\\n\\end{aligned}\n\\]\n\n\n\nClick here to see the solution\n\nlik1 = bru_obs(formula = y ~ Intercept1 + omega1,\n            family  = \"gaussian\",\n            data = df1)\n\nlik2 = bru_obs(formula = y ~ Intercept2 + omega1_copy + omega2,\n            family = \"gaussian\",\n            data = df2)\n\n\nres = bru(cmp, lik1, lik2)\n\n\n\n\n\n\nModel Results\n\n\n\n\n\n\nWarning Task\n\n\n\nCheck the model results and see that the model manages to recover the true value of the parameters.\n\n\n\nClick here to see the solution\n\n# fixed effects\n\n#fixed effects\nfixed = data.frame(true = beta, res$summary.fixed[,c(1,3,5)])\n#hyperparameters\nhyper = data.frame(true = c(1/e.sd^2, range[1], sqrt(m.var[1]),\n                          range[2], sqrt(m.var[2]),\n                    lambda),\n           res$summary.hyperpar[,c(1,3,5)])\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nCompute predictions from the model at the observation points and compare them with the observed values.\n\n\n\nClick here to see the solution\n\npred1 = predict(res, df1, ~Intercept1 + omega1)\npred2 = predict(res, df2, ~Intercept2 + omega1_copy + omega2)\n\np1 = ggplot() + geom_point(data = pred1 , aes(y, mean)) +\n  geom_errorbar(data = pred1 ,aes(y, ymin = q0.025, ymax = q0.975)) +\n  geom_abline(intercept = 0, slope = 1)\n\np2 = ggplot() +   geom_point(data = pred2 , aes(y, mean)) +\n  geom_errorbar(data = pred2 ,aes(y, ymin = q0.025, ymax = q0.975)) +\n  geom_abline(intercept = 0, slope = 1)\np1+p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nCompute predictions from the model over the area of interest. Plot the posterior mean and the posterior sd.\n\n\n\nClick here to see the solution\n\npxl = fm_pixels(mesh, mask = border)\npred1 = predict(res, pxl, ~Intercept1 + omega1)\npred2 = predict(res, pxl, ~Intercept2 + omega1_copy + omega2 )\n\n\np1 = ggplot() + gg(pred1, aes(color = mean)) + \n  ggtitle(\"Posterior mean for eta_1\") +  xlab(\"\") + ylab(\"\")\np2 = ggplot() + gg(pred2, aes(color = mean)) + \n  ggtitle(\"Posterior mean for eta_2\")+  xlab(\"\") + ylab(\"\")\n\np3 = ggplot() + gg(pred1, aes(color = sd)) + \n  ggtitle(\"Posterior sd for eta_1\")+  xlab(\"\") + ylab(\"\")\np4 = ggplot() + gg(pred2, aes(color = sd)) + \n  ggtitle(\"Posterior sd for eta_2\")+ xlab(\"\") + ylab(\"\")\n# p1 + p2 + p3 + p3\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nUse the function generate() to create 4 simulations from \\(\\widetilde{\\pi}(\\omega_1(s)|{ y}_1,{ y}_2)\\) and \\(\\widetilde{\\pi}(\\omega_2(s)|{ y}_1,{ y}_2)\\)\n\n\n\nClick here to see the solution\n\nsamples = generate(res, pxl,\n                   ~ data.frame(omega1 = omega1,\n                                omega2 = omega2),\n                   n.samples = 5)\n\n\nomega1 = sapply(samples, function(x) x$omega1)\np1 = cbind(pxl,omega1) %&gt;%\n  pivot_longer(-geometry) %&gt;% ggplot() +\n  geom_sf(aes(color =value)) + facet_wrap(.~name) + ggtitle(\"Omega 1\")\n\nomega2 = sapply(samples, function(x) x$omega2)\np2 = cbind(pxl,omega2) %&gt;%\n  pivot_longer(-geometry) %&gt;% ggplot() +\n  geom_sf(aes(color =value)) + facet_wrap(.~name) + ggtitle(\"Omega 2\")"
  },
  {
    "objectID": "day3_practical_5.html",
    "href": "day3_practical_5.html",
    "title": "Practical 5",
    "section": "",
    "text": "Aim of this practical:\nIn this first practical we are going to fit spatial modes for Areal, Geostatistical and Point-Process Data.\nDownload Practical 5 R script"
  },
  {
    "objectID": "day3_practical_5.html#areal-data",
    "href": "day3_practical_5.html#areal-data",
    "title": "Practical 5",
    "section": "Areal Data",
    "text": "Areal Data"
  },
  {
    "objectID": "day3_practical_5.html#the-data",
    "href": "day3_practical_5.html#the-data",
    "title": "Practical 5",
    "section": "The data",
    "text": "The data\nWe consider data on respiratory hospitalizations for Greater Glasgow and Clyde in 2007. The data are available from the CARBayesdata R Package:\n\nlibrary(CARBayesdata)\n\ndata(pollutionhealthdata)\ndata(GGHB.IZ)\n\nThe pollutionhealthdata contains the spatiotemporal data on respiratory hospitalizations, air pollution concentrations and socio-economic deprivation covariates for the 271 Intermediate Zones (IZ) that make up the Greater Glasgow and Clyde health board in Scotland. Data are provided by the Scottish Government and the available variables are:\n\nIZ: unique identifier for each IZ.\nyear: the year when the measurements were taken\nobserved: observed numbers of hospitalizations due to respiratory disease.\nexpected: expected numbers of hospitalizations due to respiratory disease computed using indirect standardisation from Scotland-wide respiratory hospitalization rates.\npm10: Average particulate matter (less than 10 microns) concentrations.\njsa: The percentage of working age people who are in receipt of Job Seekers Allowance\nprice: Average property price (divided by 100,000).\n\nThe GGHB.IZ data is a Simple Features (sf) object containing the spatial polygon information for the set of 271 Intermediate Zones (IZ), that make up of the Greater Glasgow and Clyde health board in Scotland ( Figure¬†1 ).\n\n\n\n\n\n\n\n\nFigure¬†1: Greater Glasgow and Clyde health board represented by 271 Intermediate Zones\n\n\n\n\nWe first merge the two dataset and select only one year of data, compute the SME and plot the observed\n\nresp_cases &lt;- merge(GGHB.IZ %&gt;%\n                      mutate(space = 1:dim(GGHB.IZ)[1]),\n                             pollutionhealthdata, by = \"IZ\") %&gt;%\n  filter(year == 2007) %&gt;%\n    mutate(SMR = observed/expected)\n\nggplot() + geom_sf(data = resp_cases, aes(fill = SMR)) + scale_fill_scico(direction = -1)\n\n\n\n\n\n\n\n\nThen we compute the adjacency matrix using the functions poly2nb() and nb2mat() in the spdep library. We then convert the adjacency matrix into the precision matrix \\(\\mathbf{Q}\\) of the CAR model. Remember this matrix has, on the diagonal the number of e\n\nlibrary(spdep)\n\nW.nb &lt;- poly2nb(GGHB.IZ,queen = TRUE)\nR &lt;- nb2mat(W.nb, style = \"B\", zero.policy = TRUE)\n\ndiag = apply(R,1,sum)\nQ = -R\ndiag(Q) = diag"
  },
  {
    "objectID": "day3_practical_5.html#the-model",
    "href": "day3_practical_5.html#the-model",
    "title": "Practical 5",
    "section": "The model",
    "text": "The model\nWe fit a first model to the data where we consider a Poisson model for the observed cases.\nStage 1 Model for the response \\[\ny_i|\\eta_i\\sim\\text{Poisson}(E_i\\lambda_i)\n\\] where \\(E_i\\) are the expected cases for area \\(i\\).\nStage 2 Latent field model \\[\n\\eta_i = \\text{log}(\\lambda_i) = \\beta_0 + \\omega_i + z_i\n\\] where\n\n\\(\\beta_0\\) is a common intercept\n\\(\\mathbf{\\omega} = (\\omega_1, \\dots, \\omega_k)\\) is a Besag (or ICAR) model with precision matrix \\(\\tau_1\\mathbf{Q}\\)\n\\(\\mathbf{z} = (z_1, \\dots, z_k)\\) is an unstructured random effect with precision \\(\\tau_2\\)\n\nStage 3 Hyperparameters\nThe hyperparameters of the model are \\(\\tau_1\\) and \\(\\tau_2\\)\nNOTE In this case the linear predictor \\(\\eta\\) consists of three components!!\n\n\n\n\n\n\nWarning Task\n\n\n\nFit the above model in using inlabru by completing the following code:\n\ncmp = ~ Intercept(1) + space(...) + iid(...)\n\nformula = ...\n\n\nlik = bru_obs(formula = formula, \n              family = ...,\n              E = ...,\n              data = ...)\n\nfit = bru(cmp, lik)\n\n\n\nAnswer\n\n\ncmp = ~ Intercept(1) + space(space, model = \"besag\", graph = Q) + iid(space, model = \"iid\")\n\nformula = observed ~ Intercept + space + iid\n\nlik = bru_obs(formula = formula, \n              family = \"poisson\",\n              E = expected,\n              data = resp_cases)\n\nfit = bru(cmp, lik)\n\n\n\n\nAfter fitting the model we want to extract results.\n\n\n\n\n\n\nTip Question\n\n\n\n\nWhat is the estimated value for \\(\\beta_0\\)? \nLook at the estimated values of the hyperparameters using fit$summary.hyperpar , which of the two spatial components (structured or unstructured) explains more of the variability in the counts? structuredunstructured\n\n\n\nWe now look at the predictions over space.\n\n\n\n\n\n\nWarning Task\n\n\n\nComplete the code below to produce prediction of the linear predictor \\(\\eta_i\\) and of the risk \\(\\lambda_i\\) and of the expected cases \\(E_i\\exp(\\lambda_i)\\) over the whole space of interest. Then plot the mean and sd of the resulting surfaces.\n\npred = predict(fit, resp_cases, ~data.frame(log_risk = ...,\n                                             risk = exp(...),\n                                             cases = ...\n                                             ),\n               n.samples = 1000)\n\n\n\nShow Answer\n\n\n# produce predictions\npred = predict(fit, resp_cases, ~data.frame(log_risk = Intercept + space,\n                                             risk = exp(Intercept + space),\n                                             cases = expected * exp(Intercept + space)\n                                             ),\n               n.samples = 1000)\n# plot the predictions\n\np1 = ggplot() + geom_sf(data = pred$log_risk, aes(fill = mean)) + scale_fill_scico(direction = -1) + ggtitle(\"mean log risk\")\np2 = ggplot() + geom_sf(data = pred$log_risk, aes(fill = sd)) + scale_fill_scico(direction = -1) + ggtitle(\"sd log risk\")\np1 + p2\n\n\n\n\n\n\n\np1 = ggplot() + geom_sf(data = pred$risk, aes(fill = mean)) + scale_fill_scico(direction = -1) + ggtitle(\"mean  risk\")\np2 = ggplot() + geom_sf(data = pred$risk, aes(fill = sd)) + scale_fill_scico(direction = -1) + ggtitle(\"sd  risk\")\np1 + p2\n\n\n\n\n\n\n\np1 = ggplot() + geom_sf(data = pred$cases, aes(fill = mean)) + scale_fill_scico(direction = -1)+ ggtitle(\"mean  expected counts\")\np2 = ggplot() + geom_sf(data = pred$cases, aes(fill = sd)) + scale_fill_scico(direction = -1)+ ggtitle(\"sd  expected counts\")\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\nFinally, we want to compare our observations \\(y_i\\) with the predicted means of the Poisson distribution \\(E_i\\exp(\\lambda_i)\\)\n\npred$cases %&gt;% ggplot() + geom_point(aes(observed, mean)) + \n  geom_errorbar(aes(observed, ymin = q0.025, ymax = q0.975)) +\n  geom_abline(intercept = 0, slope = 1)\n\n\n\n\n\n\n\n\nNote: Here we are predicting the mean of counts, not the counts!!! Predicting counts is the theme of the next task!"
  },
  {
    "objectID": "day3_practical_5.html#getting-prediction-densities",
    "href": "day3_practical_5.html#getting-prediction-densities",
    "title": "Practical 5",
    "section": "Getting prediction densities",
    "text": "Getting prediction densities\nPosterior predictive distributions, i.e.¬†\\(\\pi(y_i^{\\text{new}}|\\mathbf{y})\\), are of interest in many applied problems. The bru() function does not return predictive densities. In the previous step we have computed predictions for the expected counts \\(\\pi(E_i\\lambda_i|\\mathbf{y})\\). The predictive distribution is then: \\[\n\\pi(y_i^{\\text{new}}|\\mathbf{y}) = \\int \\pi(y_i|E_i\\lambda_i)\\pi(E_i\\lambda_i|\\mathbf{y})\\ dE_i\\lambda_i\n\\] where, in our case, \\(\\pi(y_i|E_i\\lambda_i)\\) is Poisson with mean \\(E_i\\lambda_i\\). We can achieve this using the following algorithm:\n\nSimulate \\(n\\) replicates of \\(g^k = E_i\\lambda_i\\) for \\(k = 1,\\dots,n\\) using the function generate(), which takes the same input as predict()\nFor each of the \\(k\\) replicates simulate a new value \\(y_i^{new}\\) using the function rpois()\nSummarise the \\(n\\) samples of \\(y_i^{new}\\) using, for example the mean and the 0.025 and 0.975 quantiles.\n\nHere is the code:\n\n# simulate 1000 realizations of E_i\\lambda_i\nexpected_counts = generate(fit, resp_cases, \n                           ~ expected * exp(Intercept + space),\n                           n.samples = 1000)\n\n\n# simulate poisson data\naa = rpois(271*1000, lambda = as.vector(expected_counts))\nsim_counts = matrix(aa, 271, 1000)\n\n# summarise the samples with posterior means and quantiles\npred_counts = data.frame(observed = resp_cases$observed,\n                         m = apply(sim_counts,1,mean),\n                         q1 = apply(sim_counts,1,quantile, 0.025),\n                         q2 = apply(sim_counts,1,quantile, 0.975),\n                         vv = apply(sim_counts,1,var)\n                         )\n\n\n\n\n\n\n\nWarning Task\n\n\n\nPlot the observations against the predicted new counts and the predicted expected counts. Include the uncertainty and compare the two.\n\n\n\nClick here to see the solution\n\n\nCode\nggplot() + \n  geom_point(data = pred_counts, aes(observed, m, color = \"Pred_obs\")) + \n  geom_errorbar(data = pred_counts, aes(observed, ymin = q1, ymax = q2, color = \"Pred_obs\")) +\n  geom_point(data = pred$cases, aes(observed, mean, color = \"Pred_means\")) + \n  geom_errorbar(data = pred$cases, aes(observed, ymin = q0.025, ymax = q0.975, color = \"Pred_means\")) +\n  \n  geom_abline(intercept = 0, slope =1)"
  },
  {
    "objectID": "day3_practical_5.html#adding-a-spatial-covariate",
    "href": "day3_practical_5.html#adding-a-spatial-covariate",
    "title": "Practical 5",
    "section": "Adding a spatial covariate",
    "text": "Adding a spatial covariate\nFinally, we want to add a covariate to the model. We are going to check if the PM10 levels influence the numbers of hospitalizations.\n\n\n\n\n\n\nWarning Task\n\n\n\nAdd the effect of PM10 in the previous model. First as a linear effect and then as a smooth effect (RW2). Check the results.\n\n\nTake hint\n\nTo use the RW2 model you first need to group the values of the PM10 using the inla.group() function.\n\n\n\n\nClick here to see the solution\n\n\nCode\n## Linear Effect\ncmp = ~ Intercept(1) + space(space, model = \"besag\", graph = Q) + iid(space, model = \"iid\") +\n  cov(pm10, model = \"linear\")\n\nformula = observed ~ Intercept + space + iid + cov\n\nlik = bru_obs(formula = formula, \n              family = \"poisson\",\n              E = expected,\n              data = resp_cases)\n\nfit_lin = bru(cmp, lik)\n# effect of the covaraite\n# fit_lin$summary.fixed\n\n## Linear Effect\nresp_cases$pm10_group=  inla.group(resp_cases$pm10)\ncmp = ~ Intercept(1) + space(space, model = \"besag\", graph = Q) + iid(space, model = \"iid\") +\n  cov(pm10_group, model = \"rw2\")\n\nformula = observed ~ Intercept + space + iid + cov\n\nlik = bru_obs(formula = formula, \n              family = \"poisson\",\n              E = expected,\n              data = resp_cases)\n\nfit_smooth = bru(cmp, lik)\n\n#check the smooth effect of the covariate\n#fit_smooth$summary.random$cov %&gt;% ggplot() + geom_line(aes(ID,mean)) +\n#  geom_ribbon(aes(ID, ymin =`0.025quant`, ymax = `0.975quant`), alpha = 0.5)"
  },
  {
    "objectID": "day3_practical_5.html#geostatistics",
    "href": "day3_practical_5.html#geostatistics",
    "title": "Practical 5",
    "section": "Geostatistics",
    "text": "Geostatistics"
  },
  {
    "objectID": "day3_practical_5.html#the-data-1",
    "href": "day3_practical_5.html#the-data-1",
    "title": "Practical 5",
    "section": "The data",
    "text": "The data\nWe will explore data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound. The pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)). The qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound.\nThe dataset contains presence/absence data from 2003 to 2017. In this practical we only consider year 2003.\nWe first load the dataset and select the year of interest\n\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod %&gt;% filter(year==2003)\nqcs_grid = sdmTMB::qcs_grid\n\nThen, we create an sf object and assign the rough coordinate reference to it:\n\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\npcod_sf = st_transform(pcod_sf,\ncrs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\n\nWe convert the covariate into a raster and assign the same coordinate reference:\n\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\ncrs(depth_r) &lt;- crs(pcod_sf)\n\nFinally, we can plot our dataset. Note that to plot the raster we need to also upload the tidyterra library.\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_scico(name = \"Depth\",\n                   palette = \"nuuk\",\n                   na.value = \"transparent\" ) + xlab(\"\") + ylab(\"\")"
  },
  {
    "objectID": "day3_practical_5.html#the-model-1",
    "href": "day3_practical_5.html#the-model-1",
    "title": "Practical 5",
    "section": "The model",
    "text": "The model\nWe first fit a simple model where we consider the observation as Bernoulli and where the linear predictor contains only one intercept and the GR field defined through the SPDE approach. The model is defined as:\nStage 1 Model for the response\n\\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\] Stage 2 Latent field model\n\\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s)\n\\]\nwith\n\\[\n\\omega(s)\\sim \\text{  GF with range } \\rho\\  \\text{ and maginal variance }\\ \\sigma^2\n\\]\nStage 3 Hyperparameters\nThe hyperparameters of the model are \\(\\rho\\) and \\(\\sigma\\)\nNOTE In this case the linear predictor \\(\\eta\\) consists of two components!!\n\nThe workflow\nWhen fitting a geostatistical model we need to go through the following steps:\n\nBuild the mesh\nDefine the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\nDefine the components of the linear predictor. This includes the spatial GF and all potential covariates\nDefine the observation model using the bru_obs() function\nRun the model using the bru() function\n\n\n\n1. Building the mesh\nThe first task, when dealing with geostatistical models in inlabru is to build the mesh that covers the area of interest. For this purpose we use the function fm_messh_2d.\nOne way to build the mesh is to start from the locations where we have observations, these are contained in the dataset pcod_sf.\n\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\nAs you can see from the plot above, some of the locations are very close to each other, this causes some very small triangles. This can be avoided using the option cutoff = which collapses the locations that are closer than a cutoff (those points are collapsed in the mesh construction but, of course, not when it come to estimaation.)\n\nmesh = fm_mesh_2d(loc = pcod_sf,           # Build the mesh\n                  cutoff = 2,\n                  max.edge = c(10,20),     # The largest allowed triangle edge length.\n                  offset = c(5,50))       # The automatic extension distance\nggplot() + gg(mesh) + geom_sf(data= pcod_sf, aes(color = factor(present)), size = 0.1) + xlab(\"\") + ylab(\"\")\n\n\n\n\n\n\n\n\nKey parameters in mesh construction include: max.edge for maximum triangle edge lengths, offset for inner and outer extensions (to prevent edge effects), and cutoff to avoid overly small triangles in clustered areas.\n\n\n\n\n\n\nNote\n\n\n\nGeneral guidelines for creating the mesh\n\nCreate triangulation meshes with fm_mesh_2d()\nMove undesired boundary effects away from the domain of interest by extending to a smooth external boundary\nUse a coarser resolution in the extension to reduce computational cost (max.edge=c(inner, outer))\nUse a fine resolution (subject to available computational resources) for the domain of interest (inner correlation range) and filter out small input point clusters (0 &lt; cutoff &lt; inner)\nCoastlines and similar can be added to the domain specification in fm_mesh_2d() through the boundary argument.\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nLook at the documentation for the fm_mesh_2d function typing\n\n?fm_mesh_2d\n\nplay around with the different options and create different meshes.\nThe rule of thumb is that your mesh should be:\n\nfine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\nthe triangles should be regular, avoid long and thin triangles.\nThe mesh should contain a buffer around your area of interest (this is what is defined in the offset option) in order to avoid boundary artefact in the estimated variance.\n\n\n\n\n\n2. Define the SPDE representation of the spatial GF\nTo define the SPDE representation of the spatial GF we use the function inla.spde2.pcmatern. This takes as input the mesh we have defined and the PC-priors definition for \\(\\rho\\) and \\(\\sigma\\) (the range and the marginal standard deviation of the field).\nPC priors Gaussian Random field are defined in (Fuglstad et al.¬†2018). From a practical perspective for the range \\(\\rho\\) you need to define two paramters \\(\\rho_0\\) and \\(p_{\\rho}\\) such that you believe it is reasonable that\n\\[\nP(\\rho&lt;\\rho_0)=p_{\\rho}\n\\]\nwhile for the margianal variance \\(\\sigma\\) you need to define two parameters \\(\\sigma_0\\) and \\(p_{\\sigma}\\) such that you believe it is reasonable that\n\\[\nP(\\sigma&lt;\\sigma_0)=p_{\\sigma}\n\\]\nYou can use the following function to compute and plot the prior distributions for the range and sd of the Matern field.\n\ndens_prior_range = function(rho_0, p_alpha)\n{\n  # compute the density of the PC prior for the\n  # range rho of the Matern field\n  # rho_0 and p_alpha are defined such that\n  # P(rho&lt;rho_0) = p_alpha\n  rho = seq(0, rho_0*10, length.out =100)\n  alpha1_tilde = -log(p_alpha) * rho_0\n  dens_rho =  alpha1_tilde / rho^2 * exp(-alpha1_tilde / rho)\n  return(data.frame(x = rho, y = dens_rho))\n}\n\ndens_prior_sd = function(sigma_0, p_sigma)\n{\n  # compute the density of the PC prior for the\n  # sd sigma of the Matern field\n  # sigma_0 and p_sigma are defined such that\n  # P(sigma&gt;sigma_0) = p_sigma\n  sigma = seq(0, sigma_0*10, length.out =100)\n  alpha2_tilde = -log(p_sigma)/sigma_0\n  dens_sigma = alpha2_tilde* exp(-alpha2_tilde * sigma) \n  return(data.frame(x = sigma, y = dens_sigma))\n}\n\nHere are some alternatives for defining priors for our model\n\nspde_model1 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(.1, 0.5),\n                                  prior.range = c(30, 0.5))\nspde_model2 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(10, 0.5),\n                                  prior.range = c(1000, 0.5))\nspde_model3 =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\nAnd here we plot the different priors for the range:\n\nggplot() + \n  geom_line(data = dens_prior_range(30,.5), aes(x,y, color = \"model1\")) +\n  geom_line(data = dens_prior_range(1000,.5), aes(x,y, color = \"model2\")) +\n  geom_line(data = dens_prior_range(100,.5), aes(x,y, color = \"model3\")) \n\n\n\n\n\n\n\n\nand for the sd:\n\nggplot() + \n  geom_line(data = dens_prior_sd(1,.5), aes(x,y, color = \"model1\")) +\n  geom_line(data = dens_prior_sd(10,.5), aes(x,y, color = \"model2\")) +\n  geom_line(data = dens_prior_sd(.1,.5), aes(x,y, color = \"model3\")) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip Question\n\n\n\nConsider the pcod_sf, the spatial extension and type of the data‚Ä¶ are some of the previous choices more reasonable than others? spde_model1spde_model2spde_model3\nNOTE Remember that a prior should be reasonable, but the model should not totally depend on it.\n\n\n\n\n3. Define the components of the linear predictor\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components:\n\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3)\n\nNOTE since the dataframe we use (pcod_sf) is an sf object the input in the space() component is the geometry of the dataset.\n\n\n4. Define the observation model\nOur data are Bernoulli distributed so we can define the observation model as:\n\nformula = present ~ Intercept  + space\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\n\n5. Run the model\nFinally we are ready to run the model\n\nfit1 = bru(cmp,lik)"
  },
  {
    "objectID": "day3_practical_5.html#extract-results",
    "href": "day3_practical_5.html#extract-results",
    "title": "Practical 5",
    "section": "Extract results",
    "text": "Extract results\n\nHyperparameters\n\n\n\n\n\n\nWarning Task\n\n\n\nPlot the posterior densities for the range \\(\\rho\\) and the standard deviation \\(\\sigma\\) alog with the prior for both parameters.\n\n\nTake hint\n\nPosterior marginals for the hyperparameters can be extracted from the fitted model as:\n\n\n\nClick here to see the solution\n\n\nCode\nfit1$marginals.hyperpar$'name of the parameter'\n\n\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Extract marginal for the range\n\nggplot() + \n  geom_line(data = fit1$marginals.hyperpar$`Range for space`,\n            aes(x,y, color = \"Posterior\")) +\n  geom_line(data = dens_prior_range(100,.5),\n            aes(x,y, color = \"Prior\"))\n\n\nggplot() + \n  geom_line(data = fit1$marginals.hyperpar$`Stdev for space`,\n            aes(x,y, color = \"Posterior\")) +\n  geom_line(data = dens_prior_sd(1,.5), aes(x,y, color = \"Prior\"))"
  },
  {
    "objectID": "day3_practical_5.html#spatial-prediction",
    "href": "day3_practical_5.html#spatial-prediction",
    "title": "Practical 5",
    "section": "Spatial prediction",
    "text": "Spatial prediction\nWe now want to extract the estimated posterior mean and sd of spatial GF. To do this we first need to define a grid of points where we want to predict. We do this using the function fm_pixel(), which creates a regular grid of points covering the mesh\n\npxl = fm_pixels(mesh)\n\nthen compute the prediction for both the spatial GF and the linear predictor (spatial GF + intercept)\n\npreds = predict(fit1, pxl, ~data.frame(spatial = space,\n                                      total = Intercept + space))\n\nFinally, we can plot the maps\n\nggplot() + geom_sf(data = preds$spatial,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\nggplot() + geom_sf(data = preds$spatial,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n\n\n\n\n\n\n\n\nNote The posterior sd is lowest at the observation points. Note how the posterior sd is inflated around the border, this is the ‚Äúborder effect‚Äù due to the SPDE representation.\nInstead of predicting over a grid covering the whole mesh, we can limit our predictions to the points where the covariate is defined. We can do this by defining a sf object using coordinates in the object depth_r.\n\npxl1 = data.frame(crds(depth_r), \n                  as.data.frame(depth_r$depth)) %&gt;% \n       filter(!is.na(depth)) %&gt;%\nst_as_sf(coords = c(\"x\",\"y\"))\n\n\n\n\n\n\n\nWarning Task\n\n\n\nProduce prediction over pxl1 using the same techniques as before. Plot your results.\n\n\nTake hint\n\nAdd hint details here‚Ä¶\n\n\n\n\nClick here to see the solution\n\n\nCode\npred_pxl1 = predict(fit1, pxl1, ~data.frame(spatial = space,\n                                      total = Intercept + space))\n\nggplot() + geom_sf(data = pred_pxl1$total,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\n\n\nCode\nggplot() + geom_sf(data = pred_pxl1$total,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of computing the posterior mean and standard deviations of the estimated surface, one can also simulate possible realizations of such a surface. This will give the user a better idea of the type of realized surfaces one can expect. We can do this using the function generate().\n\n# we simulate 4 samples from the \ngens = generate(fit1, pxl1, ~ (Intercept + space),\n                n.samples = 4)\n\npp = cbind(pxl1, gens)\n\npp %&gt;% select(-depth) %&gt;%\n  pivot_longer(-geometry) %&gt;%\n    ggplot() + \n      geom_sf(aes(color = value)) +\n      facet_wrap(.~name) +\n        scale_color_scico(direction = -1) +\n        ggtitle(\"Sample from the fitted model\")"
  },
  {
    "objectID": "day3_practical_5.html#an-alternative-model",
    "href": "day3_practical_5.html#an-alternative-model",
    "title": "Practical 5",
    "section": "An alternative model",
    "text": "An alternative model\nWe now want to check if the depth covatiate has an influence on the probability of presence. We do this in two different models\n\nModel 1 The depth enters the model in a linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) + \\beta_1\\ \\text{depth}(s)\n\\]\n\nModel 1 The depth enters the model in a non linear way. The linear predictor is then defined as:\n\n\\[\n  \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\omega(s) +  f(\\text{depth}(s))\n\\] where \\(f(.)\\) is a smooth function. We will use a RW2 model for this.\n\n\n\n\n\n\nWarning Task\n\n\n\nFit model 1. Define components, observation model and use the bru() function to estimate the parameters.\nNote Use the scaled version of the covariate stored in depth_r$depth_scaled.\nWhat is the liner effect of depth on the logit probability?\n\n\nTake hint\n\nAdd hint details here‚Ä¶\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3) +\n        covariate(depth_r$depth_scaled, model = \"linear\")\n\nformula = present ~ Intercept  + space + covariate\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\nfit2 = bru(cmp, lik)\n\n\n\n\n\nWe now want to fit Model 2 where we allow the effect of depth to be non-linear. To use the RW2 model we need to group the values of depth into distinct classe. To do this we use the function inla.group() which, by default, creates 20 groups. The we can fit the model as usual\n\n# create the grouped variable\ndepth_r$depth_group = inla.group(values(depth_r$depth_scaled))\n\n# run the model\ncmp = ~ Intercept(1) + space(geometry, model = spde_model3) +\n        covariate(depth_r$depth_group, model = \"rw2\")\n\nformula = present ~ Intercept  + space + covariate\n\nlik = bru_obs(formula = formula, \n              data = pcod_sf, \n              family = \"binomial\")\n\n\nfit3 = bru(cmp, lik)\n\n# plot the estimated effect of depth\n\nfit3$summary.random$covariate %&gt;% \n  ggplot() + geom_line(aes(ID,mean)) + \n                                  geom_ribbon(aes(ID, ymin = `0.025quant`, \n                                                      ymax = `0.975quant`), alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nCreate a map of predicted probability from Model 3. You can use a inverse logit function defined as\n\ninv_logit = function(x) (1+exp(-x))^(-1)\n\n\n\nTake hint\n\nThe predict() function can take as input also functions of elements of the components you want to consider\n\n\n\n\nClick here to see the solution\n\n\nCode\ninv_logit = function(x) (1+exp(-x))^(-1)\n\npred3  = predict(fit3, pxl1, ~inv_logit(Intercept + space + covariate) )\n\npred3 %&gt;% ggplot() + \n      geom_sf(aes(color = mean)) +\n        scale_color_scico(direction = -1) +\n        ggtitle(\"Sample from the fitted model\")"
  },
  {
    "objectID": "day3_practical_5.html#point-process",
    "href": "day3_practical_5.html#point-process",
    "title": "Practical 5",
    "section": "Point Process",
    "text": "Point Process"
  },
  {
    "objectID": "day3_practical_5.html#the-data-2",
    "href": "day3_practical_5.html#the-data-2",
    "title": "Practical 5",
    "section": "The data",
    "text": "The data\nIn this practical we consider the data clmfires in the spatstat library.\nThis dataset is a record of forest fires in the Castilla-La Mancha region of Spain between 1998 and 2007. This region covers approximately 400 by 400 kilometres. The coordinates are recorded in kilometres. For more info about the data you can type:\n\n?clmfires\n\nWe first read the data and transform them into an sf object. We also create a polygon that represents the border of the Castilla-La Mancha region. We select the data for year 2004 and only those fires caused by lightning.\n\ndata(\"clmfires\")\npp = st_as_sf(as.data.frame(clmfires) %&gt;%\n                mutate(x = x, \n                       y = y),\n              coords = c(\"x\",\"y\"),\n              crs = NA) %&gt;%\n  filter(cause == \"lightning\",\n         year(date) == 2004)\n\npoly = as.data.frame(clmfires$window$bdry[[1]]) %&gt;%\n  mutate(ID = 1)\n\nregion = poly %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\"), crs = NA) %&gt;% \n  dplyr::group_by(ID) %&gt;% \n  summarise(geometry = st_combine(geometry)) %&gt;%\n  st_cast(\"POLYGON\") \n  \nggplot() + geom_sf(data = region, alpha = 0) + geom_sf(data = pp)  \n\n\n\n\n\n\n\nFigure¬†2: Distribution of the observed forest fires caused by lightning in Castilla-La Mancha in 2004"
  },
  {
    "objectID": "day3_practical_5.html#HPP",
    "href": "day3_practical_5.html#HPP",
    "title": "Practical 5",
    "section": "Fit a homogeneous Poisson Process",
    "text": "Fit a homogeneous Poisson Process\nAs a first exercise we are going to fit a homogeneous Poisson process (HPP) to the data. This is a model that assume constant intensity over the whole space so our linear predictor is then:\n\\[\n\\eta(s) = \\log\\lambda(s) = \\beta_0 , \\ \\mathbf{s}\\in\\Omega\n\\]\nso the likelihood can be written as:\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp\\left( -\\int_{\\Omega}\\exp(\\beta_0)ds\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n\\]\nwhere \\(|\\Omega|\\) is the area of the domain of interest.\nWe need to approximate the integral using a numerical integration scheme as:\n\\[\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n\\]\nWhere \\(N_k\\) is the number of integration points \\(s_1,\\dots,s_{N_k}\\) and \\(w_1,\\dots,w_{N_k}\\) are the integration weights.\nIn this case, since the intensity is constant, the integration scheme is really simple: it is enough to consider one random point inside the domain with weight equal to the area of the domain.\n\n# define integration scheme\n\nips = st_sf(\ngeometry = st_sample(region, 1)) # some random location inside the domain\nips$weight = st_area(region) # integration weight is the area of the domain\n\ncmp = ~ 0 + beta_0(1)\n\nformula = geometry ~ beta_0\n\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit1 = bru(cmp, lik)\n\n\n\n\n\n\n\nWarning Task\n\n\n\n\nPlot the estimated posterior distribution of the intensity\nCompare the estimated expected number of fires on the whole domain with the observed ones.\n\n\n\nTake hint\n\nRemember that in the inlabru framework we model the log intensity \\(\\eta = log(\\lambda)\\)\n\n\n\n\nClick here to see the solution\n\n\nCode\n# 1) The estimated posterior distribution of the  intensity is\n\npost_int = inla.tmarginal(function(x) exp(x), fit1$marginals.fixed$beta_0)\npost_int %&gt;% ggplot() + geom_line(aes(x,y))\n\n\n\n\n\n\n\n\n\nCode\n# 2) To compute the expected number of points in the area we need to multiply the\n# estimated intensity by the area of the domain.\n# In the same plot we also show the number of observed fires as a vertical line.\n\npost_int = inla.tmarginal(function(x) st_area(region)* exp(x), fit1$marginals.fixed$beta_0)\npost_int %&gt;% ggplot() + geom_line(aes(x,y)) +\n  geom_vline(xintercept = dim(pp)[1])"
  },
  {
    "objectID": "day3_practical_5.html#NHPP",
    "href": "day3_practical_5.html#NHPP",
    "title": "Practical 5",
    "section": "Fit an Inhomogeneous Poisson Process",
    "text": "Fit an Inhomogeneous Poisson Process\nThe model above has the clear disadvantages that assumes a constant intensity and from Figure¬†2 we clearly see that this is not the case.\nThe library spatstat contains also some covariates that can help explain the fires distribution. Figure @fit-altitude shows the location of fires together with the (scaled) altitude.\n\n#|label: fig-altitude\n#|fig-cap: \"Distribution of the observed forest fires and scaled altitude\"\n#| \nelev_raster = rast(clmfires.extra[[2]]$elevation)\nelev_raster = scale(elev_raster)\nggplot() + \n  geom_spatraster(data = elev_raster) + \n  geom_sf(data = pp) +\n  scale_fill_scico()\n\n\n\n\n\n\n\n\nWe are now going to use the altitude as a covariate to explain the variability of the intensity \\(\\lambda(s)\\) over the domain of interest.\nOur model is \\[\n\\log\\lambda(s) = \\beta_0 + \\beta_1x(s)\n\\] where \\(x(s)\\) is the altitude at location \\(s\\).\nThe likelihood becomes:\n\\[\n\\begin{aligned}\np(\\mathbf{y} | \\lambda)  & \\propto \\exp \\left( -\\int_\\Omega \\lambda(\\mathbf{s}) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n& = \\exp \\left( -\\int_\\Omega \\exp(\\beta_0 + \\beta_1x(s)) \\mathrm{d}\\mathbf{s} \\right) \\prod_{i=1}^n \\lambda(\\mathbf{s}_i) \\\\\n\\end{aligned}\n\\]\nNow we need to choose an integration scheme to solve the integral.\nIn this case we will take a simple grid based approach where each quadrature location has an equal weight. Our grid consists of \\(N_k = 1000\\) points and the weights are all equal to \\(|\\Omega|/N_k\\).\n\n#|label: fig-int2\n#|fig-cap: \"Integration scheme.\"\n\nn.int = 1000\nips = st_sf(geometry = st_sample(region,\n            size = n.int,\n            type = \"regular\"))\n\nips$weight = st_area(region) / n.int\nggplot() + geom_sf(data = ips, aes(color = weight)) + geom_sf(data= region, alpha = 0)\n\n\n\n\n\n\n\n\nOBS: The implicit assumption here is that the intensity is constant inside each grid box, and so is the covariate!!\nWe can now fit the model:\n\ncmp = ~ Intercept(1) + elev(elev_raster, model = \"linear\")\nformula = geometry ~ Intercept + elev\nlik = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips)\nfit2 = bru(cmp, lik)\n\n\n\n\n\n\n\nWarning Task\n\n\n\nWhat is the effect of the altitude on the (log) intensity of the process?\n\n\nTake hint\n\nYou can look at the summary for the fixed effects\n\n\n\n\nClick here to see the solution\n\n\nCode\nfit2$summary.fixed\n\n\n                mean         sd 0.025quant   0.5quant 0.975quant       mode\nIntercept -6.6053551 0.10204628 -6.8053621 -6.6053551  -6.405348 -6.6053551\nelev       0.6500802 0.06955835  0.5137483  0.6500802   0.786412  0.6500802\n                   kld\nIntercept 0.000000e+00\nelev      4.851206e-16\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n‚ö†Ô∏è WARNING!!‚ö†Ô∏è When fitting a Point process, the integration scheme has to be fine enough to capture the spatial variability of the covariate!!\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nRerun the model with the altitude as covariate, but this time change the integration scheme as follows:\n\nn.int2 = 50\n\nips2 = st_sf(geometry = st_sample(region,\n            size = n.int2,\n            type = \"regular\"))\nips2$weight = st_area(region) / n.int2\n\nWhat happens to the effect of the covariate?\n\n\nTake hint\n\nRe-run the model changing the integration scheme in the ips input of the bru_obs() function.\n\n\n\n\nClick here to see the solution\n\n\nCode\nlik_bis = bru_obs(data = pp,\n              family = \"cp\",\n              formula = formula,\n              ips = ips2)\n\nfit2bis = bru(cmp, lik_bis)\n\n# you can the check the differences between the two models\nrbind(fit2$summary.fixed,\nfit2bis$summary.fixed)\n\n\n                 mean         sd 0.025quant   0.5quant 0.975quant       mode\nIntercept  -6.6053551 0.10204628 -6.8053621 -6.6053551 -6.4053480 -6.6053551\nelev        0.6500802 0.06955835  0.5137483  0.6500802  0.7864120  0.6500802\nIntercept1 -6.6210289 0.10337083 -6.8236320 -6.6210289 -6.4184258 -6.6210289\nelev1       0.6823490 0.07209954  0.5410365  0.6823490  0.8236615  0.6823490\n                    kld\nIntercept  0.000000e+00\nelev       4.851206e-16\nIntercept1 3.415957e-14\nelev1      0.000000e+00\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nNow we want to predict the log-intensity over the whole domain. Use the grid from the elevation raster to predict the intensity over the domain.\n\nest_grid = st_as_sf(data.frame(crds(elev_raster)), coords = c(\"x\",\"y\"))\nest_grid  = st_intersection(est_grid, region)\n\n\n\n\nClick here to see the solution\n\n\nCode\npreds2 = predict(fit2, est_grid, ~ data.frame(log_scale = Intercept + elev,\n\n                                              lin_scale = exp(Intercept + elev)))\n# then visualize it like\npreds2$log_scale %&gt;% \n  ggplot() +\n  geom_sf(aes(color = mean)) +\n  scale_color_scico()\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we want to use the fitted model to estimate the total number of fires over the whole region. To do this we first have to fine the expected number of fires as:\n\\[\nE(N_{\\Omega}) = \\int_{\\Omega}\\exp(\\lambda(s))\\ ds\n\\]\nThen simulate possible realizations of \\(N_{\\Omega}\\) to include also the likelihood variability in our estimate:\n\nN_fires = generate(fit2, ips,\n                      formula = ~ {\n                        lambda = sum(weight * exp(elev + Intercept))\n                        rpois(1, lambda)},\n                    n.samples = 2000)\n\nggplot(data = data.frame(N = as.vector(N_fires))) +\n  geom_histogram(aes(x = N),\n                 colour = \"blue\",\n                 alpha = 0.5,\n                 bins = 20) +\n  geom_vline(xintercept = nrow(pp),\n             colour = \"red\") +\n  theme_minimal() +\n  xlab(expression(Lambda))"
  },
  {
    "objectID": "day3_practical_5.html#LGCP",
    "href": "day3_practical_5.html#LGCP",
    "title": "Practical 5",
    "section": "Fit a Log-Gaussian Cox Process",
    "text": "Fit a Log-Gaussian Cox Process\nFinally we want to fit a LGCP with log intensity:\n\\[\n\\log(s) = \\beta_0 + \\beta_1x + u(s)\n\\]\nwhere \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) the effect of (standarized) altitude \\(x(s)\\) as before and \\(u(s)\\) is a Gaussian Random field defined through the SPDE approach.\n\nDefine the mesh\nThe first step, as any time we use the SPDE approach is to defie the mesh and the priors for the marginal variance and range:\n\nmesh = fm_mesh_2d(boundary = region,\n                  max.edge = c(5, 10),\n                  cutoff = 4, crs = NA)\n\nggplot() + gg(mesh) + geom_sf(data = pp)\n\n\n\n\n\n\n\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\nWe can then define the integration weight. Here we use the same points to define the SPDE approximation and to approximate the integral in the likelihood. We will see later that this does not have to be like this, BUT integration weight and SPDE weights have to be consistent with each other!\n\nips = fm_int(mesh, samplers = region)\n\nggplot() + geom_sf(data = ips, aes(color = weight)) +\n  gg(mesh) +\n   scale_color_scico()\n\n\n\n\n\n\n\n\n\n\nRun the model\n\n\n\n\n\n\nWarning Task\n\n\n\nSet up the components and the formula for the model above by completing the code below and run the model.\n\ncmp = ~ ...\n\nformula = geometry ~ ...\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = pp,\n              ips = ...)\n\nfit3 = bru(cmp, lik)\n\n\n\nTake hint\n\nThe model has three components: intercept, linear effect of altitude and the spatial GRF\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp = ~ Intercept(1) + space(geometry, model = spde_model) + elev(elev_raster, model = \"linear\")\n\nformula = geometry ~ Intercept + space + elev\n\nlik = bru_obs(\"cp\",\n              formula = formula,\n              data = pp,\n              ips = ips)\n\nfit3 = bru(cmp, lik)\n\n\n\n\n\nNote when running the model above you will get a warning:\n\n\nWarning in bru_log_warn(msg): Model input 'elev_raster' for 'elev' returned some NA values.\nAttempting to fill in spatially by nearest available value.\nTo avoid this basic covariate imputation, supply complete data.\n\n\nIt means that the bru() function cannot find the covariate values for some of the mesh nodes. This is a common situation. As the warning says, the bru() function automatically imputes the value of the covarite using the nearest nodes. This increases the running time of the bru() function, so one solution is to impute the values of the covariate over the whole mesh ‚Äòbefore‚Äô running the bru() function.\nHere, we notice that there is a single point for which elevation values are missing (see Figure¬†3 the red point that lies outside the raster extension ).\n\n\n\n\n\n\n\n\nFigure¬†3: Integration scheme for numerical approximation of the stochastic integral in La Mancha Region\n\n\n\n\n\nTo solve this, we can increase the raster extension so it covers all both data-points and quadrature locations as well. Then, we can use the bru_fill_missing() function to input the missing values with the nearest-available-value. We can achieve this using the following code:\n\n# Extend raster ext by 30 % of the original raster so it covers the whole mesh\nre &lt;- extend(elev_raster, ext(elev_raster)*1.3)\n# Convert to an sf spatial object\nre_df &lt;- re %&gt;% stars::st_as_stars() %&gt;%  st_as_sf(na.rm=F)\n# fill in missing values using the original raster \nre_df$lyr.1 &lt;- bru_fill_missing(elev_raster,re_df,re_df$lyr.1)\n# rasterize\nelev_rast_p &lt;- stars::st_rasterize(re_df) %&gt;% rast()\nggplot() + geom_spatraster(data = elev_rast_p) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe bru_fill_missing() function was added mainly to handle very local infilling on domain boundaries. For properly missing data, one should consider doing a proper model of the spatial field instead."
  },
  {
    "objectID": "day3_practical_5.html#results",
    "href": "day3_practical_5.html#results",
    "title": "Practical 5",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nWarning Task\n\n\n\nPlot the estimated mean and standard deviation of the spatial GF and the log-intensity over the domain of interest\n\n\nTake hint\n\nUse the fm_pixels() and predict() functions.\n\n\n\n\nClick here to see the solution\n\n\nCode\npxl = fm_pixels(mesh, mask= region, dims = c(200,200))\npreds = predict(fit3, pxl, ~data.frame(spde = space,\n                                       log_int = Intercept + space + elev))\n\n#and plot as \nlibrary(scico)\nlibrary(patchwork)\n\nggplot(data= preds$spde) + \n  geom_sf(aes(color = mean)) + \n  scale_color_scico() +\n ggtitle(\"spde mean\") +\nggplot(data=preds$spde ) +\n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"spde sd\") +\n\nggplot(data=preds$log_int) + \n  geom_sf(aes(color = mean)) + \n  scale_color_scico() +\n ggtitle(\"log-int mean\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(data=preds$log_int) + \n  geom_sf(aes(color = sd)) +\n  scale_color_scico() +\n ggtitle(\"log-int sd\") +\n  plot_layout(ncol=2)\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of just looking at the posterior mean and standard deviation, it can be usefull to look at simulated fields from the posterior distribution. This is because the mean field is, by definition, smoother than any realization of the field. So looking at simulation can give us a better idea of how the field might look like. We can do this using the generate() function:\n\nsim_fields = generate(fit3, pxl, ~data.frame(spde = space,\n                                       log_int = Intercept + space + elev),\n                     n.samples = 4)\n\ncbind(pxl,sapply(sim_fields, function(x) x$spde)) %&gt;%\n  pivot_longer(-geometry) %&gt;%\n  ggplot() + geom_sf(aes(color = value)) + \n  facet_wrap(.~name) + scale_color_scico() +\n  ggtitle(\"simulated spatial fields\")\n\n\n\n\n\n\n\ncbind(pxl,sapply(sim_fields, function(x) x$log_int)) %&gt;%\n  pivot_longer(-geometry) %&gt;%\n  ggplot() + geom_sf(aes(color = value)) + \n  facet_wrap(.~name) + scale_color_scico() + \n  ggtitle(\"simulated log intensity\")"
  },
  {
    "objectID": "day2_practical_3.html",
    "href": "day2_practical_3.html",
    "title": "Practical 3",
    "section": "",
    "text": "Aim of this practical:\nwe are going to learn:\nDownload Practical 3 R script"
  },
  {
    "objectID": "day2_practical_3.html#ar1-models-in-inlabru",
    "href": "day2_practical_3.html#ar1-models-in-inlabru",
    "title": "Practical 3",
    "section": "AR(1) models in inlabru",
    "text": "AR(1) models in inlabru\nIn this exercise we will:\n\nSimulate a time series with autocorrelated errors.\nFit an AR(1) process with inlabru\nVisualize model predictions.\nForecasting for future observations\n\nStart by loading useful libraries:\n\nlibrary(tidyverse)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nTime series analysis is particularly valuable for modelling data with temporal dependence or autocorrelation, where observations taken at nearby time points tend to be more similar than those further apart.\nA time series process is a stochastic process \\(\\{X_t~|~t \\in T\\}\\), which is a collection of random variables that are ordered in time where \\(T\\) is the index set that determines the set of discrete and equally spaced time points at which the process is defined and observations are made.\nAutoregressive processes allow us to account for the time dependence by regressing \\(X_t\\) on past values \\(X_{t-1},\\ldots,X_{t-p}\\) with associated coefficients \\(\\phi_k\\) for each lag \\(k = 1,\\ldots,p\\). Thus an autoregressive process of order \\(p\\), denoted AR(\\(p\\)) , is given by:\n\\[\nX_t = \\phi_1 X_{t-1} + \\ldots + \\phi_p X_{t-p} + \\varepsilon_t; ~~ \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2_e)\n\\]\nConsider now an univariate time series \\(y_t\\) which evolves over time according to some autoregressive stochastic process. For example, a time series where the system follows an AR(1) process can be defined as:\n\\[\n\\begin{aligned}\ny_t &\\sim \\mathcal{N}(\\mu_t,\\tau_y^{-1})\\\\\n\\eta_t &= g^{-1}(\\mu_t) = \\alpha + u_t \\\\\nu_t &= \\phi u_{t-1} + \\delta_t ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t &gt; 1 \\\\\nu_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n\\]\nThe response \\(y_t\\) is assumed to be normal distributed with mean \\(\\alpha + u_t\\) and precision error \\(\\tau_y\\) ( here \\(g(\\cdot)\\) is just the identity link function that maps the linear predictor to the mean of the process). Then, the process \\(u_t\\) follows an AR(1) process where \\(u_1\\) is drawn from a stationary normal distribution such that \\(\\kappa\\) denotes the marginal precision for state \\(u_t\\)\nThe covariance matrix is then given by:\n\\[\n\\Sigma = \\frac{\\tau^{-1}_u}{1-\\phi^2}\n\\begin{bmatrix}\n1 & \\phi & \\phi^2 & \\ldots & \\phi^{n-1}\\\\\n\\phi & 1 & \\phi & \\ldots & \\phi^{n-2} \\\\\n\\phi^2 & \\phi & 1 & \\ldots & \\phi^{n-3} \\\\\n\\phi^{n-1} & \\phi^{n-2} & \\phi^{n-3} & \\ldots & 1\n\\end{bmatrix}\n\\]\nNotice that conditionally on \\(u_t\\), the observed data \\(y_y\\) are independent from\\(y_{t-1},y_{t-2}.y_{t-3},\\ldots\\), also the conditional distribution of \\(u_t\\) is a markov chain such that \\(\\pi(u_t|u_{t-1},u_{t-2},u_{t-3}) = \\pi(u_t|u_{t-1})\\). Thus, each time point is only conditionally dependent on the two closest time points:\n\\[\nu_t|\\mathbf{u}_{-t} \\sim \\mathcal{N}\\left(\\frac{\\phi}{1-\\phi^2}(u_{t-1}+u_{t+1}),\\frac{\\tau_u^{-1}}{1-\\phi^2}\\right)\n\\]\n\nSimulate example data\nFirst, we simulate data from the model:\n\\[\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t;~ \\varepsilon_t \\sim \\mathcal{N}(0,\\tau_y^{-1})\\\\\nu_t &= \\phi y_{t-1} + \\delta_t; ~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1})\n\\end{aligned}\n\\]\n\nset.seed(123)\n\nphi = 0.8\ntau_u = 10\nmarg.prec = tau_u * (1-phi^2) # ar1 in INLA is parametrized as marginal variance\nu_t =  as.vector(arima.sim(list(order = c(1,0,0), ar = phi), \n                          n = 100,\n                          sd=sqrt(1/tau_u)))\na = 1\ntau_e = 5\nepsilon_t = rnorm(100, sd = sqrt(1/tau_e))\ny = a + u_t + epsilon_t\n\n\nts_dat &lt;- data.frame(y =y , x= 1:100)\n\n\n\n\n\n\n\n\n\n\n\n\nFitting an AR(1) model with inlabru\nModel components\nFirst, we define the model components, notice that the latent field is defined by two components: the intercept \\(\\alpha\\) and the autoregressive random effects \\(u_t\\):\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n\nThe we can define the formula for the linear predictor and specify the observational model\n\n# Model formula\nformula = y ~ alpha + ut\n# Observational model\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts_dat)\n\nLastly, we fit the model using the bru function and compare the model estimates with the true mdeol parameters we simulate our data from.\n\n# fit the model\nfit.ar1 = bru(cmp, lik)\n\n# compare against the true values\n\ndata.frame(\n  true = c(a,tau_e,marg.prec,phi),\n  rbind(fit.ar1$summary.fixed[,c(1,3,5)],\n        fit.ar1$summary.hyperpar[,c(1,3,5)])\n        ) %&gt;% round(2)\n\n                                        true mean X0.025quant X0.975quant\nalpha                                    1.0 1.00        0.69        1.28\nPrecision for the Gaussian observations  5.0 4.91        3.11        7.29\nPrecision for ut                         3.6 7.96        2.91       18.10\nRho for ut                               0.8 0.80        0.54        0.94\n\n\nModel predictions\nHere, we will predict the mean of our time series along with 95% credible intervals. Note that this interval are for the mean and not for new observations, we will cover forecasting new observations next.\n\npred_ar1 = predict(fit.ar1, ts_dat, ~ alpha + ut)\n\nggplot(pred_ar1,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=y,x=x))\n\n\n\n\n\n\n\n\n\n\nForecasting\nA common goal in time series modelling is forecasting into the future. Forecasting can be treated as a missing data problem where future values of the response variable are missing. Let \\(y_m\\) be the missing response, then, by fitting a statistical model to the observed data \\(\\mathbf{y}_{obs}\\), we condition on its parameters to obtain the posterior predictive distribution:\n\\[\n\\pi(y_{m} \\mid \\mathbf{y}_{obs}) = \\int \\pi(y_{m}, \\theta \\mid \\mathbf{y}_{obs})  d\\theta = \\int \\pi(y_{m} \\mid \\mathbf{y}_{obs}, \\theta) \\pi(\\theta \\mid \\mathbf{y}_{obs})  d\\theta\n\\]\nThis distribution, which integrates over all parameter uncertainty, provides the complete probabilistic forecast for the missing values. INLA will automatically compute the predictive distributions for all missing values in the response. To do so, we can augment our data set by including the new time points at which the prediction will be made and setting the response value to¬†NA¬†for these new time points:\n\nts.forecast &lt;- rbind(ts_dat, \n  data.frame(y = rep(NA, 50), x = 101:150))\n\nNext, we fit the¬†ar1¬†model to the new dataset so that the predictive distributions are computed:\n\ncmp =  ~ -1 + alpha(1) + ut(x,model = \"ar1\")\n\npred_lik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = ts.forecast)\n\nfit.forecast = bru(cmp, pred_lik)\n\nLastly, we can draw samples from the posterior predictive distribution using the predict function and visualize our forecast as follows:\n\npred_forecast = predict(fit.forecast, ts.forecast, ~ alpha + ut)\n\np1= ggplot(pred_forecast,aes(y=mean,x=x))+\n  geom_line()+\n    geom_ribbon(aes(x = x, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(data=ts_dat, aes(y=y,x=x))"
  },
  {
    "objectID": "day2_practical_3.html#modelling-great-lakes-water-level",
    "href": "day2_practical_3.html#modelling-great-lakes-water-level",
    "title": "Practical 3",
    "section": "Modelling Great Lakes water level",
    "text": "Modelling Great Lakes water level\nIn this exercise we will:\n\nFit an AR(1) process with inlabru to model lakes water levels\nChange the default priors for the observational error\nSet penalized complexity priors for the correlation and precision parameters of the latent effects.\nFit a RW(1) model\nFit a an AR(1) model with group-level correlation\n\nStart by loading useful libraries:\n\nlibrary(tidyverse) \nlibrary(INLA) \nlibrary(ggplot2)\nlibrary(patchwork) \nlibrary(inlabru)\nlibrary(DAAG)\n\nIn this exercise we will look at greatLakes dataset from the DAAG package. The data set contains the water level heights for the lakes Erie, Michigan/Huron, Ontario and St Clair from 1918 to 2009.\nLets begin by loading and formatting the data into a tidy format.\n\ndata(\"greatLakes\")\n\ngreatLakes.df = data.frame(as.matrix(greatLakes),\n                           year = time(greatLakes)) %&gt;%\n  pivot_longer(cols = c(\"Erie\",\"michHuron\",\"Ontario\",\"StClair\"),\n               names_to = \"Lakes\",\n               values_to = \"height\" ) \n\n\n\n\n\n\n\n\n\n\n\nFitting an AR(1) model in inlabru\nWe will focus on the Erin lake for now. Lets begin by fitting an AR(1) model of the form:\n\\[\n\\begin{aligned}\n\\text{height}_t &= \\alpha + u_t +\\varepsilon_t~; ~~ \\varepsilon_t\\sim \\mathcal{N}(0,\\tau_e^{-1}) \\\\\nu_t &= \\phi u_{t-1} + \\delta_t~~~ ; ~~ \\delta_t \\sim \\mathcal{N}(0,\\tau_u^{-1}); ~~ t &gt; 1 \\\\\nx_1 &= \\mathcal{N}(0,\\kappa^{-1})\\\\\n\\kappa &= \\tau_u (1-\\phi^2)\n\\end{aligned}\n\\]\nWhere \\(\\alpha\\) is the intercept, \\(\\phi\\) is the correlation term, \\(\\varepsilon\\) is the observational Gaussian error with mean zero and precision \\(\\tau_e\\) and \\(\\kappa\\) is the marginal precision for the state \\(u_t\\) for \\(t= 1,\\ldots,92\\).\nFirst we make a subset of the dataset and create a time index \\(T\\):\n\ngreatLakes.df$t.idx &lt;- greatLakes.df$year-1917\n\nErie.df = greatLakes.df %&gt;% filter(Lakes == \"Erie\")\n\n\n\n\n\n\n\nWarning Task\n\n\n\nFit an AR(1) model to the Erie lake data using inlabru, then plot the model fitted values showing 95% credible intervals.\n\n\nTake hint\n\nRemember this is done by (1) defining the model components, (2) the formula and (3) the observational model. Then you can use the predict function to compute the predicted values for the mean along with 95% credible intervals.\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx,model = \"ar1\")\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height   ~.,\n            family = \"gaussian\",\n            data = Erie.df )\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n\n# Model predictions \n\npred_ar1.Erie = predict(fit.Erie_ar1, Erie.df, ~ alpha + ut)\n\n# plot model fitted values\nggplot(pred_ar1.Erie,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip Question\n\n\n\nAre there any issues with the fitted model, and if so, how do you think we should address them?\n\n\nAnswer\n\nIt is clear that the model overfits the data, leading to poor predictive performance. Thus, we need to introduce some prior information on the what we expect the variation of the process to be.\n\n\n\nPriors\nLet review INLA‚Äô prior parametrization for autoregressive models:\nLet \\(\\pmb{\\theta} = \\{\\theta_y,\\theta_u,\\theta_\\phi\\}\\) be INLA‚Äôs internal representation of the hyperparameters such that:\n\\(\\theta_y = \\log(\\tau^2_y)\\)\n\\(\\theta_u = \\log(\\kappa) = \\log\\left(\\tau_u[1-\\phi^2]\\right)\\)\n\\(\\theta_\\phi = \\log \\left(\\frac{1+\\phi}{1-\\phi}\\right)\\)\nThe default priors for \\(\\{\\theta_y,\\theta_u\\}\\) are \\(\\text{log-gamma} (1, 5\\times 10^{-5} )\\) priors with default initial values set to 4 in each case. Then, Gaussian priors \\(\\alpha \\sim \\mathcal{N}(0,\\tau_y = 0.001)\\) and \\(\\theta_\\phi \\sim \\mathcal{N}(0, \\tau_y= 0.15)\\) are used for the intercept and correlation parameter respectively.\n\n\n\n\n\n\nNote\n\n\n\nSpecifically for AR(1) correlation parameter \\(\\phi\\), INLA uses the following logit transformation on \\(\\theta_\\phi\\):\n\\[ \\phi = \\frac{2\\exp(\\theta_\\phi)}{1+ \\exp(\\theta_\\phi)} -1. \\]\n\n\nSetting priors and PC-priors\nLets now set a Gamma prior with parameters 1 and 1, so that the precision of the Gaussian osbervational error is centered at 1 with a variance of 1. Additionally we will set Penalized Complexity (PC) priors according to the following probability statements:\n\n\\(P(\\sigma &gt; 1) = 0.01\\)\n\\(P(\\phi &gt; 0.5) = 0.3\\)\n\nNotice that the PC prior for the precision \\(\\tau_u\\) is defined on the standard deviation \\(\\sigma_u = \\tau_u^{-1/2}\\)\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(t.idx, model = \"ar1\",  hyper = pc_prior)\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = Erie.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.Erie_ar1 = bru(cmp, lik)\n\n\n\n\n\n\n\nTip Question\n\n\n\nWhat is the posterior mean for the correlation parameter \\(\\rho\\)? \n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nPlot the fitted values of the model, has the overfitting problem being alleviated?\n\n\n\nClick here to see the solution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFitting a RW(1) model\nNow we fit a random walk of order 1 to the Erie lake data:\n\\[\n\\begin{aligned}\ny_t &= \\alpha + u_t + \\varepsilon_t, ~ t = 1,\\ldots,92 \\\\\n\\varepsilon_t & \\sim \\mathcal{N}(0,\\tau_e) \\\\\nu_t - u_{t-1} &\\sim \\mathcal{N}(0,\\tau_u),~ t = 2,\\ldots,92 \\\\\n\\end{aligned}\n\\]\nFirs we define model priors:\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 1))) # prior values\n\nNow we define model components:\n\ncmp_rw =  ~ -1 + alpha(1) + \n  ut(t.idx ,\n     constr=FALSE,\n     model = \"rw1\",\n     hyper=pc_prior,\n     scale.model = TRUE)\n\nNotice that we have set scale.model = TRUE to scale the latent effects. This is particularly important when Intrinsic Gaussian Markov random fields (IGMRFs) are used as priors (e.g., random walk models or some spatial models) for the latent effects. By defining scale.model = TRUE, the rw1-model is scaled to have a generalized variance equal to one. By scaling scaling the models we ensure that a fixed hyperprior for the precision parameter has a similar interpretation for different types of IGMRFs, making precision estimates comparable between different models. Scaling also allows estimates to be less sensitive to re-scaling covariates in the linear predictor and makes the precision invariant to changes in the shape and size of the latent effect (see S√∏rbye (2014) for further details) .\nWe can now fit the model with the updated components and plot the predicted values\n\n# Model formula\nformula = height ~ alpha + ut\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = Erie.df,\n            control.family = list(hyper = prec.tau_e))\n# fit the model\nfit.Erie_rw1 = bru(cmp_rw, lik)\n# Model predictions\npred_rw1.Erie = predict(fit.Erie_rw1, Erie.df, ~ alpha + ut)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip Question\n\n\n\nTake a look at the model summaries using the summary function, do you see anything odd?\n\n\nAnswer\n\nThe intercept has zero mean and a very large variance. This is because we have not imposed a sum-to-zero constraint on the model random effects (constr=FALSE). Without this constraint, intrinsic models are non-identifiable. The intercept and the random effects are confounded, For example, you could add a constant value to every random effect and subtract it from the intercept without changing the model‚Äôs predictions. Take for instance for any constant \\(c\\), the following models are identical:\n\\(y_t = \\alpha + u_{t} + \\varepsilon_t\\) \\(y_t = (\\alpha - c) + (u_{t} + c) + \\varepsilon_t\\)\nThus, you need to set constr=FALSE so that \\(\\sum_t u_t=0\\) to ensure identifiability of \\(\\alpha\\)\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nFit an RW(1) model to the Erie data but now set constr=TRUE to impose a sum-to-zero constraint on the random effect. Then compare your results with the unconstrained model.\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp_rw =  ~ -1 + alpha(1) + \n  ut(t.idx ,\n     constr=TRUE,\n     model = \"rw1\",\n     hyper=pc_prior,\n     scale.model = TRUE)\n\nfit.Erie_rw1_constr = bru(cmp_rw, lik)\n\nfit.Erie_rw1_constr$summary.fixed\n\n\n          mean         sd 0.025quant 0.5quant 0.975quant     mode          kld\nalpha 174.1381 0.02374158   174.0914 174.1381   174.1848 174.1381 1.457136e-08\n\n\n\n\n\n\n\nGroup-level effects\nNow we will model the height water levels for all four lakes by grouping the random effects. This will allow a within-lakes correlation to be included. In the next example, we allow for correlated effects using an ar1 model for the years and iid random effects on the lakes. First we create a lakes id and set the priors for our model:\n\ngreatLakes.df$lake_id &lt;- as.numeric(as.factor(greatLakes.df$Lakes))\n\npc_prior &lt;- list(theta = list(prior = \"pc.prec\", param = c(1, 0.01)),\n                 rho = list(prior = \"pc.cor0\", param = c(0.5, 0.3))) \n\nprec.tau_e &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(1, 10))) # prior values\n\nNow we define the model components. The lakes IDs that define the group are passed with parameter group argument and the iid model and other parameters are passed through the control.group parameter.\n\n# Model components\ncmp =  ~ -1 + alpha(1) + ut(year,model = \"ar1\",\n                            hyper = pc_prior,\n                            group =lake_id,\n                            control.group = \n                            list(model = \"iid\", \n                                 scale.model = TRUE))\n\nWe fit the model in a similar fashion as we did before:\n\n# Model formula\nformula = height ~ alpha + ut\n\n\n# Observational model\nlik =  bru_obs(formula = height  ~.,\n            family = \"gaussian\",\n            data = greatLakes.df,\n            control.family = list(hyper = prec.tau_e))\n\n# fit the model\nfit.all_lakes_ar1 = bru(cmp, lik)\n\n# Model predictions\npred_ar1.all = predict(fit.all_lakes_ar1, greatLakes.df, ~ alpha + ut)\n\nLastly we can visualize group-level model predictions as follows:\n\nggplot(pred_ar1.all,aes(y=mean,x=year))+\n  geom_line()+\n    geom_ribbon(aes(x = year, y = mean, ymin = q0.025, ymax = q0.975),\n                alpha = 0.5) +\n  geom_point(aes(y=height,x=year)) + facet_wrap(~Lakes,scales = \"free\")"
  },
  {
    "objectID": "day2_practical_3.html#non-gaussian-data",
    "href": "day2_practical_3.html#non-gaussian-data",
    "title": "Practical 3",
    "section": "Non-Gaussian data",
    "text": "Non-Gaussian data\nIn the next example we will use the Toyo data set to illustrate how temporal models can be fit to non-Gaussian data.\nThe Tokyo data set available in INLA contains the recorded days of rain above 1 mm in Tokyo for 2 years, 1983:84. The data set contains the following variables:\n\ny : number of days with rain\nn : total number of days\ntime : day of the year\n\n\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(ggplot2)\nlibrary(tidyr)\n\n\ndata(\"Tokyo\")\n\nA possible observational model for these data is\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim\\text{Bin}(n_t, p_t) \\\\\n\\eta_t &= \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\end{aligned}\n\\]\n\\[\nn_t = \\left\\{\n\\begin{array}{lr}\n1, & \\text{for}\\; 29\\; \\text{February}\\\\\n2, & \\text{other days}\n\\end{array}\\right.\n\\]\n\\[\ny_t =\n\\begin{cases}\n\\{0,1\\}, & \\text{for}\\; 29\\; \\text{February}\\\\\n\\{0,1,2\\}, & \\text{other days}\n\\end{cases}\n\\]\nThen, the latent field is given by\n\\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\n\nWhere the probability of rain depends on on the day of the year \\(t\\)\n\\(\\beta_0\\) is an intercept\n\\(f(\\text{time}_t)\\) is a temporal model, e.g., a RW2 model (this is just a smoother).\n\nThe smoothness is controlled by a hyperparameter \\(\\tau_f\\) . Thus, we assign a prior to \\(\\tau_f\\) to finalize the model.\nWe can fit the model as follows:\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)\n\nNotice that we have set cyclic = TRUE as this is a cyclic effect. Finally, we can produce model predictions in a similar fashion as we did before:\n\npTokyo = predict(fit, Tokyo, ~ plogis(beta0 + time_effect))\n\nggplot(data=pTokyo , aes(x= time, y= y) ) +\n  geom_point() + \n  ylab(\"\") + xlab(\"\") +\n  # Custom the Y scales:\n  scale_y_continuous(\n    # Features of the first axis\n    name = \"\",\n    # Add a second axis and specify its features\n    sec.axis = sec_axis( transform=~./2, name=\"Probability\")\n  )  + geom_line(aes(y=mean*2,x=time)) +\n  geom_ribbon(aes( ymin = q0.025*2, \n                             ymax = q0.975*2), alpha = 0.5)"
  },
  {
    "objectID": "day1_practical.html",
    "href": "day1_practical.html",
    "title": "Practical 1",
    "section": "",
    "text": "Aim of this practical:\nIn this first practical we are going to look at some simple models\nwe are going to learn:\nDownload Practical 1 R script"
  },
  {
    "objectID": "day1_practical.html#sec-linmodel",
    "href": "day1_practical.html#sec-linmodel",
    "title": "Practical 1",
    "section": "Linear Model",
    "text": "Linear Model\nIn this practical we will:\n\nSimulate Gaussian data\nLearn how to fit a linear model with inlabru\nGenerate predictions from the model\n\nStart by loading useful libraries:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n# load some libraries to generate nice plots\nlibrary(scico)\n\nAs our first example we consider a simple linear regression model with Gaussian observations \\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor (\\(\\eta_i\\)) through an identity function: \\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated. We assign \\(\\beta_0\\) and \\(\\beta_1\\) a vague Gaussian prior.\nTo finalize the Bayesian model we assign a \\(\\text{Gamma}(a,b)\\) prior to the precision parameter \\(\\tau = 1/\\sigma^2\\) and two independent Gaussian priors with mean \\(0\\) and precision \\(\\tau_{\\beta}\\) to the regression parameters \\(\\beta_0\\) and \\(\\beta_1\\) (we will use the default prior settings in INLA for now).\n\n\n\n\n\n\nTip Question\n\n\n\nWhat is the dimension of the hyperparameter vector and latent Gaussian field?\n\n\nAnswer\n\nThe hyperparameter vector has dimension 1, \\(\\pmb{\\theta} = (\\tau)\\) while the latent Gaussian field \\(\\pmb{u} = (\\beta_0, \\beta_1)\\) has dimension 2, \\(0\\) mean, and sparse precision matrix:\n\\[\n\\pmb{Q} = \\begin{bmatrix}\n\\tau_{\\beta_0} & 0\\\\\n0 & \\tau_{\\beta_1}\n\\end{bmatrix}\n\\] Note that, since \\(\\beta_0\\) and \\(\\beta_1\\) are fixed effects, the precision parameters \\(\\tau_{\\beta_0}\\) and \\(\\tau_{\\beta_1}\\) are fixed.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe can write the linear predictor vector \\(\\pmb{\\eta} = (\\eta_1,\\dots,\\eta_N)\\) as\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 = \\begin{bmatrix}\n1 \\\\\n1\\\\\n\\vdots\\\\\n1\n\\end{bmatrix} \\beta_0 + \\begin{bmatrix}\nx_1 \\\\\nx_2\\\\\n\\vdots\\\\\nx_N\n\\end{bmatrix} \\beta_1\n\\]\nOur linear predictor consists then of two components: an intercept and a slope.\n\n\n\nSimulate example data\nFirst, we simulate data from the model\n\\[\ny_i\\sim\\mathcal{N}(\\eta_i,0.1^2), \\ i = 1,\\dots,100\n\\]\nwith\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i\n\\]\nwhere \\(\\beta_0 = 2\\), \\(\\beta_1 = 0.5\\) and the values of the covariate \\(x\\) are generated from an Uniform(0,1) distribution. The simulated response and covariate data are then saved in a data.frame object.\n\n\nSimulate Data from a LM\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting a linear regression model with inlabru\n\nDefining model components\nThe model has two parameters to be estimated \\(\\beta_1\\) and \\(\\beta_2\\). We need to define the two corresponding model components.\n\n\n\n\n\n\nWarning Task\n\n\n\nDefine an object called cmp that includes and (i) intercept beta_0 and (ii) a covariate x linear effect beta_1.\n\n\nTake hint\n\nThe cmp object is here used to define model components. We can give them any useful names we like, in this case, beta_0 and beta_1. You can remove the automatic intercept construction by adding a -1 in the components\n\n\n\n\nClick here to see the solution\n\n\nCode\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that we have excluded the default Intercept term in the model by typing -1 in the model components. However, inlabru has automatic intercept that can be called by typing Intercept() , which is one of inlabru special names and it is used to define a global intercept, e.g.\n\ncmp =  ~  Intercept(1) + beta_1(x, model = \"linear\")\n\n\n\nObservation model construction\nThe next step is to construct the observation model by defining the model likelihood. The most important inputs here are the formula, the family and the data.\n\n\n\n\n\n\nWarning Task\n\n\n\nDefine a linear predictor eta using the component labels you have defined on the previous task.\n\n\nTake hint\n\nThe eta object defines how the components should be combined in order to define the model predictor.\n\n\n\n\nClick here to see the solution\n\n\nCode\neta = y ~ beta_0 + beta_1\n\n\n\n\n\nThe likelihood for the observational model is defined using the bru_obs() function.\n\n\n\n\n\n\nWarning Task\n\n\n\nDefine the observational model likelihood in an object called lik using the bru_obs() function.\n\n\nTake hint\n\nThe bru_obs is expecting three arguments:\n\nThe linear predictor eta we defined in the previous task\nThe data likelihood (this can be specified by setting family = \"gaussian\")\nThe data set df\n\n\n\n\n\nClick here to see the solution\n\n\nCode\nlik =  bru_obs(formula = eta,\n            family = \"gaussian\",\n            data = df)\n\n\n\n\n\nFit the model\nWe fit the model using the bru() functions which takes as input the components and the observation model:\n\nfit.lm = bru(cmp, lik)\n\nExtract results\nThe summary() function will give access to some basic information about model fit and estimates\n\nsummary(fit.lm)\n## inlabru version: 2.13.0.9011 \n## INLA version: 25.09.19 \n## Components: \n## Latent components:\n## beta_0: main = linear(1)\n## beta_1: main = linear(x)\n## Observation models: \n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ beta_0 + beta_1\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1], latent[] \n## Time used:\n##     Pre = 1.1, Running = 0.427, Post = 0.0181, Total = 1.55 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.009 0.009      1.992    2.009      2.027 2.009   0\n## beta_1 0.507 0.010      0.488    0.507      0.527 0.507   0\n## \n## Model hyperparameters:\n##                                           mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 125.90 17.80      93.48   125.07\n##                                         0.975quant   mode\n## Precision for the Gaussian observations     163.15 123.39\n## \n## Marginal log-Likelihood:  77.52 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\nWe can see that both the intercept and slope and the error precision are correctly estimated.\n\n\nGenerate model predictions\n\nNow we can take the fitted bru object and use the predict function to produce predictions for \\(\\mu\\) given a new set of values for the model covariates or the original values used for the model fit\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)\n\nThe predict function generate samples from the fitted model. In this case we set the number of samples to 1000.\n\nPlotR Code\n\n\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\n\nCode\npred %&gt;% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n  geom_line(aes(x, q0.025), linetype = \"dashed\")+\n  geom_line(aes(x, q0.975), linetype = \"dashed\")+\n  xlab(\"Covariate\") + ylab(\"Observations\")\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nGenerate predictions for a new observation with \\(x_0 = 0.45\\)\n\n\nTake hint\n\nYou can create a new data frame containing the new observation \\(x_0\\) and then use the predict function.\n\n\n\n\nClick here to see the solution\n\n\nCode\nnew_data = data.frame(x = 0.45)\npred = predict(fit.lm, new_data, ~ beta_0 + beta_1,\n               n.samples = 1000)"
  },
  {
    "objectID": "day1_practical.html#linear-mixed-model",
    "href": "day1_practical.html#linear-mixed-model",
    "title": "Practical 1",
    "section": "Linear Mixed Model",
    "text": "Linear Mixed Model\nIn this practical we will:\n\nUnderstand the basic structure of a Linear Mixed Model (LLM)\nSimulate data from a LMM\nLearn how to fit a LMM with inlabru and predict from the model.\n\nConsider the a simple linear regression model except with the addition that the data that comes in groups. Suppose that we want to include a random effect for each group \\(j\\) (equivalent to adding a group random intercept). The model is then: \\[\ny_{ij}  = \\beta_0 + \\beta_1 x_i + u_j + \\epsilon_{ij} ~~~  \\text{for}~i = 1,\\ldots,N~ \\text{and}~ j = 1,\\ldots,m.\n\\]\nHere the random group effect is given by the variable \\(u_j \\sim \\mathcal{N}(0, \\tau^{-1}_u)\\) with \\(\\tau_u = 1/\\sigma^2_u\\) describing the variability between groups (i.e., how much the group means differ from the overall mean). Then, \\(\\epsilon_j \\sim \\mathcal{N}(0, \\tau^{-1}_\\epsilon)\\) denotes the residuals of the model and \\(\\tau_\\epsilon = 1/\\sigma^2_\\epsilon\\) captures how much individual observations deviate from their group mean (i.e., variability within group).\nThe model design matrix for the random effect has one row for each observation (this is equivalent to a random intercept model). The row of the design matrix associated with the \\(ij\\)-th observation consists of zeros except for the element associated with \\(u_j\\), which has a one.\n\\[\n\\pmb{\\eta} = \\pmb{A}\\pmb{u} = \\pmb{A}_1\\pmb{u}_1 + \\pmb{A}_2\\pmb{u}_2 + \\pmb{A}_3\\pmb{u}_3\n\\]\n\n\n\n\n\n\nNoteSupplementary material: LMM as a LGM\n\n\n\nIn matrix form, the linear mixed model for the j-th group can be written as:\n\\[ \\overbrace{\\mathbf{y}_j}^{ N \\times 1} = \\overbrace{X_j}^{ N \\times 2} \\underbrace{\\beta}_{1\\times 1} + \\overbrace{Z_j}^{n_j \\times 1} \\underbrace{u_j}_{1\\times1} + \\overbrace{\\epsilon_j}^{n_j \\times 1}, \\]\nIn a latent Gaussian model (LGM) formulation the mixed model predictor for the i-th observation can be written as :\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_i + \\sum_k^K f_k(u_j)\n\\]\nwhere \\(f_k(u_j) = u_j\\) since there‚Äôs only one random effect per group (i.e., a random intercept for group \\(j\\)). The fixed effects \\((\\beta_0,\\beta_1)\\) are assigned Gaussian priors (e.g., \\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)). The random effects \\(\\mathbf{u} = (u_1,\\ldots,u_m)^T\\) follow a Gaussian density \\(\\mathcal{N}(0,\\mathbf{Q}_u^{-1})\\) where \\(\\mathbf{Q}_u = \\tau_u\\mathbf{I}_m\\) is the precision matrix for the random intercepts. Then, the components for the LGM are the following:\n\nLatent field given by\n\\[\n\\begin{bmatrix} \\beta \\\\\\mathbf{u}\n\\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0},\\begin{bmatrix}\\tau_\\beta^{-1}\\mathbf{I}_2&\\mathbf{0}\\\\\\mathbf{0} &\\tau_u^{-1}\\mathbf{I}_m\\end{bmatrix}\\right)\n\\]\nLikelihood:\n\\[\ny_i \\sim \\mathcal{N}(\\eta_i,\\tau_{\\epsilon}^{-1})\n\\]\nHyperparameters:\n\n\\(\\tau_u\\sim\\mathrm{Gamma}(a,b)\\)\n\\(\\tau_\\epsilon \\sim \\mathrm{Gamma}(c,d)\\)\n\n\n\n\n\nSimulate example data\n\nset.seed(12)\nbeta = c(1.5,1)\nsd_error = 1\ntau_group = 1\n\nn = 100\nn.groups = 5\nx = rnorm(n)\nv = rnorm(n.groups, sd = tau_group^{-1/2})\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error) +\n  rep(v, each = 20)\n\ndf = data.frame(y = y, x = x, j = rep(1:5, each = 20))  \n\nNote that inlabru expects an integer indexing variable to label the groups.\n\n\nCode\nggplot(df) +\n  geom_point(aes(x = x, colour = factor(j), y = y)) +\n  theme_classic() +\n  scale_colour_discrete(\"Group\")\n\n\n\n\n\nData for the linear mixed model example with 5 groups\n\n\n\n\n\n\nFitting a LMM in inlabru\n\nDefining model components and observational model\nIn order to specify this model we must use the group argument to tell inlabru which variable indexes the groups. The model = \"iid\" tells INLA that the groups are independent from one another.\n\n# Define model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u(j, model = \"iid\")\n\nThe group variable is indexed by column j in the dataset. We have chosen to name this component v() to connect with the mathematical notation that we used above.\n\n# Construct likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n\nFitting the model\nThe model can be fitted exactly as in the previous examples by using the bru function with the components and likelihood objects.\n\nfit = bru(cmp, lik)\nsummary(fit)\n## inlabru version: 2.13.0.9011 \n## INLA version: 25.09.19 \n## Components: \n## Latent components:\n## beta_0: main = linear(1)\n## beta_1: main = linear(x)\n## u: main = iid(j)\n## Observation models: \n##   Family: 'gaussian'\n##     Tag: &lt;No tag&gt;\n##     Data class: 'data.frame'\n##     Response class: 'numeric'\n##     Predictor: y ~ .\n##     Additive/Linear: TRUE/TRUE\n##     Used components: effects[beta_0, beta_1, u], latent[] \n## Time used:\n##     Pre = 0.885, Running = 0.183, Post = 0.034, Total = 1.1 \n## Fixed effects:\n##         mean    sd 0.025quant 0.5quant 0.975quant  mode kld\n## beta_0 2.108 0.438      1.229    2.108      2.986 2.108   0\n## beta_1 1.172 0.120      0.936    1.172      1.407 1.172   0\n## \n## Random effects:\n##   Name     Model\n##     u IID model\n## \n## Model hyperparameters:\n##                                          mean    sd 0.025quant 0.5quant\n## Precision for the Gaussian observations 0.995 0.144      0.738    0.986\n## Precision for u                         1.613 1.060      0.369    1.356\n##                                         0.975quant  mode\n## Precision for the Gaussian observations       1.30 0.971\n## Precision for u                               4.35 0.918\n## \n## Marginal log-Likelihood:  -179.93 \n##  is computed \n## Posterior summaries for the linear predictor and the fitted values are computed\n## (Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\nModel predictions\nTo compute model predictions we can create a data.frame containing a range of values of covariate where we want the response to be predicted for each group. Then we simply call the predict function while specifying the model components.\n\n\nLMM fitted values\n# New data\nxpred = seq(range(x)[1], range(x)[2], length.out = 100)\nj = 1:n.groups\npred_data = expand.grid(x = xpred, j = j)\npred = predict(fit, pred_data, formula = ~ beta_0 + beta_1 + u) \n\n\npred %&gt;%\n  ggplot(aes(x=x,y=mean,color=factor(j)))+\n  geom_line()+\n  geom_ribbon(aes(x,ymin = q0.025, ymax= q0.975,fill=factor(j)), alpha = 0.5) + \n  geom_point(data=df,aes(x=x,y=y,colour=factor(j)))+\n  facet_wrap(~j)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip Question\n\n\n\nSuppose that we are also interested in including random slopes into our model. Assuming intercept and slopes are independent, can your write down the linear predictor and the components of this model as a LGM?\n\n\nGive me a hint\n\nIn general, the mixed model predictor can decomposed as:\n\\[ \\pmb{\\eta} = X\\beta + Z\\mathbf{u} \\]\nWhere \\(X\\) is a \\(n \\times p\\) design matrix and \\(\\beta\\) the corresponding p-dimensional vector of fixed effects. Then \\(Z\\) is a \\(n\\times q_J\\) design matrix for the \\(q_J\\) random effects and \\(J\\) groups; \\(\\mathbf{v}\\) is then a \\(q_J \\times 1\\) vector of \\(q\\) random effects for the \\(J\\) groups. In a latent Gaussian model (LGM) formulation this can be written as:\n\\[ \\eta_i = \\beta_0 + \\sum\\beta_j x_{ij} + \\sum_k f(k) (u_{ij}) \\]\n\n\n\nSee Solution\n\n\nThe linear predictor is given by\n\\[\n\\eta_i = \\beta_0 + \\beta_1x_i + u_{0j} + u_{1j}x_i\n\\]\nLatent field defined by:\n\n\\(\\beta \\sim \\mathcal{N}(0,\\tau_\\beta^{-1})\\)\n\\(\\mathbf{u}_j = \\begin{bmatrix}u_{0j} \\\\ u_{1j}\\end{bmatrix}, \\mathbf{u}_j \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{Q}_u^{-1})\\) where the precision matrix is a block-diagonal matrix with entries \\(\\mathbf{Q}_u= \\begin{bmatrix}\\tau_{u_0} & {0} \\\\{0} & \\tau_{u_1}\\end{bmatrix}\\)\n\nThe hyperparameters are then:\n\n\\(\\tau_{u_0},\\tau_{u_1} \\text{and}~\\tau_\\epsilon\\)\n\n\nTo fit this model in inlabru we can simply modify the model components as follows:\n\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\") +\n  u0(j, model = \"iid\") + u1(j,x, model = \"iid\")"
  },
  {
    "objectID": "day1_practical.html#sec-genlinmodel",
    "href": "day1_practical.html#sec-genlinmodel",
    "title": "Practical 1",
    "section": "Generalized Linear Model",
    "text": "Generalized Linear Model\nIn this practical we will:\n\nSimulate non-Gaussian data\nLearn how to fit a generalised linear model with inlabru\nGenerate predictions from the model\n\nA generalised linear model allows for the data likelihood to be non-Gaussian. In this example we have a discrete response variable which we model using a Poisson distribution. Thus, we assume that our data \\[\ny_i \\sim \\text{Poisson}(\\lambda_i)\n\\] with rate parameter \\(\\lambda_i\\) which, using a log link, has associated predictor \\[\n\\eta_i = \\log \\lambda_i = \\beta_0 + \\beta_1 x_i\n\\] with parameters \\(\\beta_0\\) and \\(\\beta_1\\), and covariate \\(x\\).\n\nSimulate example data\nThis code generates 100 samples of covariate x and data y.\n\nset.seed(123)\nn = 100\nbeta = c(1,1)\nx = rnorm(n)\nlambda = exp(beta[1] + beta[2] * x)\ny = rpois(n, lambda  = lambda)\ndf = data.frame(y = y, x = x)  \n\n\n\nFitting a GLM in inlabru\n\nDefine model components\nThe predictor here only contains only 2 components (Intercept and Slope).\n\n\n\n\n\n\nWarningTask\n\n\n\nDefine an object called cmp that includes and (i) intercept beta_0 and (ii) a covariate x linear effect beta_1.\n\n\n\nClick here to see the solution\n\n\nCode\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n\n\n\n\n\nDefine linear predictor\n\n\n\n\n\n\nWarningTask\n\n\n\nDefine a linear predictor eta using the component labels you have defined on the previous task.\n\n\n\nClick here to see the solution\n\n\nCode\neta = y ~ beta_0 + beta_1\n\n\n\n\n\nBuild observational model\nWhen building the observation model likelihood we must now specify the Poisson likelihood using the family argument (the default link function for this family is the \\(\\log\\) link).\n\nlik =  bru_obs(formula = eta,\n            family = \"poisson\",\n            data = df)\n\nFit the model\nOnce the likelihood object is constructed, fitting the model is exactly the same process, we just need to specify the model components and the observational model, and pass this on to the bru function:\n\nfit_glm = bru(cmp, lik)\n\nAnd model summaries can be viewed using\n\nsummary(fit_glm)\n\ninlabru version: 2.13.0.9011 \nINLA version: 25.09.19 \nComponents: \nLatent components:\nbeta_0: main = linear(1)\nbeta_1: main = linear(x)\nObservation models: \n  Family: 'poisson'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: y ~ beta_0 + beta_1\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta_0, beta_1], latent[] \nTime used:\n    Pre = 0.827, Running = 0.183, Post = 0.00559, Total = 1.02 \nFixed effects:\n        mean    sd 0.025quant 0.5quant 0.975quant  mode kld\nbeta_0 0.915 0.071      0.775    0.915      1.054 0.915   0\nbeta_1 1.048 0.056      0.938    1.048      1.157 1.048   0\n\nMarginal log-Likelihood:  -204.02 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\n\nGenerate model predictions\n\nTo generate new predictions we must provide a data frame that contains the covariate values for \\(x\\) at which we want to predict.\nThis code block generates predictions for the data we used to fit the model (contained in df$x) as well as 10 new covariate values sampled from a uniform distribution runif(10).\n\n# Define new data, set to NA the values for prediction\n\nnew_data = data.frame(x = c(df$x, runif(10)),\n                      y = c(df$y, rep(NA,10)))\n\n# Define predictor formula\npred_fml &lt;- ~ exp(beta_0 + beta_1)\n\n# Generate predictions\npred_glm &lt;- predict(fit_glm, new_data, pred_fml)\n\nSince we used a log link (which is the default for family = \"poisson\"), we want to predict the exponential of the predictor. We specify this using a general R expression using the formula syntax.\n\n\n\n\n\n\nNote\n\n\n\nNote that the predict function will call the component names (i.e.¬†the ‚Äúlabels‚Äù) that were decided when defining the model.\n\n\nSince the component definition is looking for a covariate named \\(x\\), all we need to provide is a data frame that contains one, and the software does the rest.\n\nPlotR Code\n\n\n\n\n\n\n\nData and 95% credible intervals\n\n\n\n\n\n\n\npred_glm %&gt;% ggplot() + \n  geom_point(aes(x,y), alpha = 0.3) +\n  geom_line(aes(x,mean)) +\n    geom_ribbon(aes(x = x, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations (counts)\")\n\n\n\n\n\n\n\n\n\n\nWarningTask\n\n\n\nSuppose a binary response such that\n\\[\n    \\begin{aligned}\ny_i &\\sim \\mathrm{Bernoulli}(\\psi_i)\\\\\n\\eta_i &= \\mathrm{logit}(\\psi_i) = \\alpha_0 +\\alpha_1 \\times w_i\n\\end{aligned}\n\\] Using the following simulated data, use inlabru to fit the logistic regression above. Then, plot the predictions for the data used to fit the model along with 10 new covariate values\n\nset.seed(123)\nn = 100\nalpha = c(0.5,1.5)\nw = rnorm(n)\npsi = plogis(alpha[1] + alpha[2] * w)\ny = rbinom(n = n, size = 1, prob =  psi) # set size = 1 to draw binary observations\ndf_logis = data.frame(y = y, w = w)  \n\nHere we use the logit link function \\(\\mathrm{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right)\\) (plogis() function in R) to link the linear predictor to the probabilities \\(\\psi\\).\n\n\nTake hint\n\nYou can set family = \"binomial\" for binary responses and the plogis() function for computing the predicted values.\n\n\n\n\n\n\nNote\n\n\n\nThe Bernoulli distribution is equivalent to a \\(\\mathrm{Binomial}(1, \\psi)\\) pmf. If you have proportional data (e.g.¬†no. successes/no. trials) you can specify the number of events as your response and then the number of trials via the Ntrials = n argument of the bru_obs function (where n is the known vector of trials in your data set).\n\n\n\n\n\n\nClick here to see the solution\n\n\nCode\n# Model components\ncmp_logis =  ~ -1 + alpha_0(1) + alpha_1(w, model = \"linear\")\n# Model likelihood\nlik_logis =  bru_obs(formula = y ~.,\n            family = \"binomial\",\n            data = df_logis)\n# fit the model\nfit_logis &lt;- bru(cmp_logis,lik_logis)\n\n# Define data for prediction\nnew_data = data.frame(w = c(df_logis$w, runif(10)),\n                      y = c(df_logis$y, rep(NA,10)))\n# Define predictor formula\npred_fml &lt;- ~ plogis(alpha_0 + alpha_1)\n\n# Generate predictions\npred_logis &lt;- predict(fit_logis, new_data, pred_fml)\n\n# Plot predictions\npred_logis %&gt;% ggplot() + \n  geom_point(aes(w,y), alpha = 0.3) +\n  geom_line(aes(w,mean)) +\n    geom_ribbon(aes(x = w, ymax = q0.975, ymin = q0.025),fill = \"tomato\", alpha = 0.3)+\n  xlab(\"Covariate\") + ylab(\"Observations\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "inlabru Workshop",
    "section": "",
    "text": "Welcome to the course!\n\nWelcome to the inlabru workshop!\nThe aim of this workshop is to introduce you to a range of statistical modelling approaches, in particular the temporal, spatial and spatio-temporal modelling as implemented in the inlabru package.\nWorkshop materials are available in the github repository inlabru-workshop\n\n\n\nLearning Objectives for the workshop\nAt the end of the workshop, participants will have an understanding of:\n\nthe motivation for and the challenges of analysing and modelling spatial data\nstatistical models used to analyse spatial and spatio-temporal data\nthe implementation of these models in the inlabru package\nhow to independently analyse spatial data with inlabru\n\n\n\nIntended audience\nThe workshop aims to cater for participants with a range of different backgrounds, who is interested in analysing data with modern spatial and spatio-temporal statistical modelling approaches.\n\n\nPrerequisites\nParticipants should be familiar with the R environment, and general statistical approaches for modelling such as regression, analysis of (co)variance, and generalized linear models.‚Äã\nNo knowledge of R-INLA or inlabru is required.\n\n\nSchedule\n\nDay 1Day 2Day 3Day 4Day 5\n\n\n\n\n\nTime\nTopic\n\n\n\n\n10:00 - 10:30\nICES information session\n\n\n10:30 - 11:30\nSession 1: Introduction to inlabru\n\n\n11:30 - 13:00\nPractical Session 1\n\n\n13:00 - 14:30\nLunch break üçΩÔ∏è\n\n\n14:30 - 15:30\nSession 2: Latent Gaussian Models and INLA\n\n\n15:30 - 15:45\nCoffee Break ‚òï\n\n\n15:45 - 16:45\nPractical Session 2\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 3: Temporal modelling and smoothing part 1\n\n\n10:00 - 10:30\nSnack ü•ô\n\n\n10:30 - 11:30\nTemporal modelling and smoothing part 2\n\n\n11:30 - 13:00\nPractical Session 3\n\n\n13:00 - 14:30\nLunch break üçΩÔ∏è\n\n\n14:30 - 15:30\nSession 5: Introduction to Spatial Statistics\n\n\n15:35 - 15:45\nCoffee Break ‚òï\n\n\n15:45 - 16:45\nPractical Session 4\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 6: Areal Processes\n\n\n10:00 - 10:30\nSnack ü•ô\n\n\n10:30 - 11:30\nSession 7: Geostatistics\n\n\n11:30 - 13:00\nPractical Session 5\n\n\n13:00 - 14:30\nLunch break üçΩÔ∏è\n\n\n14:30 - 15:30\nSession 8: Spatial Point processes\n\n\n15:35 - 15:45\nCoffee Break ‚òï\n\n\n15:45 - 16:45\nPractical Session 5 continued\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 9: Spatiotemporal models\n\n\n10:00 - 10:30\nSnack ü•ô\n\n\n10:30 - 11:30\nSession 10: Model comparison and evaluation\n\n\n11:30 - 13:00\nPractical Session 6\n\n\n13:00 - 14:30\nLunch break üçΩÔ∏è\n\n\n14:30 - 15:30\nSession 11: Multi-likelihood/joint likelihood models\n\n\n15:35 - 15:45\nCoffee Break ‚òï\n\n\n15:45 - 16:45\nPractical Session 7\n\n\n16:45 - 17:00\nwrap-up and outlook\n\n\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 10:00\nSession 12: Zero inflated models\n\n\n10:00 - 10:30\nSnack ü•ô\n\n\n10:30 - 11:30\nSession 13: Complex observational processes: Distance Sampling\n\n\n11:30 - 13:00\nPractical Session 8: Zero Inflated models - Distance sampling\n\n\n13:00 - 13:15\nCoffee Break ‚òï\n\n\n13:15 - 14:00\nClosing session\n\n\n\n\n\n\n\n\nIn preparation for the workshop\nParticipants are required to follow the below steps ahead of the first day of the workshop:\n\nCheck your R version, it should be at least 4.3.*\nInstall R-INLA\nInstall inlabru (available from CRAN)\n\n\n# Enable universe(s) by inlabru-org\noptions(repos = c(\n  inlabruorg = \"https://inlabru-org.r-universe.dev\",\n  INLA = \"https://inla.r-inla-download.org/R/testing\",\n  CRAN = \"https://cloud.r-project.org\"\n))\n\n# Install some packages\ninstall.packages(\"inlabru\")\n\n\nMake sure you have the latest R-INLA, inlabru and R versions installed.\nInstall the following libraries:\n\n\n\ninstall.packages(c(\n  \"CARBayesdata\",\n  \"DAAG\",\n  \"dplyr\",\n  \"FSAdata\",\n  \"ggplot2\",\n  \"gt\",\n  \"lubridate\",\n  \"magrittr\",\n  \"mapview\",\n  \"patchwork\",\n  \"scico\",\n  \"sdmTMB\",\n  \"sf\",\n  \"spatstat\",\n  \"spdep\",\n  \"terra\",\n  \"tidyr\",\n  \"tidyterra\",\n  \"tidyverse\"\n))",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "day1_practical_2.html",
    "href": "day1_practical_2.html",
    "title": "Practical 2",
    "section": "",
    "text": "Aim of this practical:\nwe are going to learn:\nDownload Practical 2 R script"
  },
  {
    "objectID": "day1_practical_2.html#setting-priors-and-model-checking-for-linear-models",
    "href": "day1_practical_2.html#setting-priors-and-model-checking-for-linear-models",
    "title": "Practical 2",
    "section": "Setting priors and model checking for Linear Models",
    "text": "Setting priors and model checking for Linear Models\nIn this exercise we will:\n\nLearn how to set priors for linear effects \\(\\beta_0\\) and \\(\\beta_1\\)\nLearn how to set the priors for the hyperparameter \\(\\tau = 1/\\sigma^2\\).\nVisualize marginal posterior distributions\n\nStart by loading useful libraries:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nRecall a simple linear regression model with Gaussian observations \\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor through an identity function: \\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated. In INLA, we assume that the model is a latent Gaussian model, i.e., we have to assign \\(\\beta_0\\) and \\(\\beta_1\\) a Gaussian prior. For the precision hyperparameter \\(\\tau = 1/\\sigma^2\\) a typical prior choice is a \\(\\text{Gamma}(a,b)\\) prior.\nIn R-INLA, the default choice of priors for each \\(\\beta\\) is\n\\[\n\\beta \\sim \\mathcal{N}(0,10^3).\n\\]\nand the prior for the variance parameter in terms of the log precision is\n\\[ \\log(\\tau) \\sim \\mathrm{logGamma}(1,5 \\times 10^{-5}) \\]\n\n\n\n\n\n\nNote\n\n\n\nIf your model uses the default intercept construction (i.e., Intercept(1) in the linear predictor) inlabru will assign a default \\(\\mathcal{N} (0,0)\\) prior to it.\n\n\nLets see how can we change the default priors using some simulated data\n\nSimulate example data\nWe simulate data from a simple linear regression model\n\n\nCode\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting the linear regression model with inlabru\nNow we fit a simple linear regression model in inlabru by defining (1) the model components, (2) the linear predictor and (3) the likelihood.\n\n# Model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n# Linear predictor\nformula = y ~ Intercept + beta_1\n# Observational model likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n# Fit the Model\nfit.lm = bru(cmp, lik)\n\n\n\nChange the prior distributions\nUntil now, we have used the default priors for both the precision \\(\\tau\\) and the fixed effects \\(\\beta_0\\) and \\(\\beta_1\\). Let‚Äôs see how to customize these.\nTo check which priors are used in a fitted model one can use the function inla.prior.used()\n\ninla.priors.used(fit.lm)\n\nsection=[family]\n    tag=[INLA.Data1] component=[gaussian]\n        theta1:\n            parameter=[log precision]\n            prior=[loggamma]\n            param=[1e+00, 5e-05]\nsection=[linear]\n    tag=[beta_0] component=[beta_0]\n        beta:\n            parameter=[beta_0]\n            prior=[normal]\n            param=[0.000, 0.001]\n    tag=[beta_1] component=[beta_1]\n        beta:\n            parameter=[beta_1]\n            prior=[normal]\n            param=[0.000, 0.001]\n\n\nFrom the output we see that the precision for the observation \\(\\tau\\sim\\text{Gamma}(1e+00,5e-05)\\) while \\(\\beta_0\\) and \\(\\beta_1\\) have precision 0.001, that is variance \\(1/0.001\\).\nChange the precision for the linear effects\nThe precision for linear effects is set in the component definition. For example, if we want to increase the precision to 0.01 for \\(\\beta_0\\) we define the relative components as:\n\ncmp1 =  ~-1 +  beta_0(1, prec.linear = 0.01) + beta_1(x, model = \"linear\")\n\n\n\n\n\n\n\nWarning Task\n\n\n\nRun the model again using 0.1 as default precision for both the intercept and the slope parameter.\n\n\n\nClick here to see the solution\n\ncmp2 =  ~ -1 + \n          beta_0(1, prec.linear = 0.1) + \n          beta_1(x, model = \"linear\", prec.linear = 0.1)\n\nlm.fit2 = bru(cmp2, lik) \n\n\nNote that we can use the same observation model as before since both the formula and the dataset are unchanged.\n\n\nChange the prior for the precision of the observation error \\(\\tau\\)\nPriors on the hyperparameters of the observation model must be passed by defining argument hyper within control.family in the call to the bru_obs() function.\n\n# First we define the logGamma (0.01,0.01) prior \n\nprec.tau &lt;- list(prec = list(prior = \"loggamma\",   # prior name\n                             param = c(0.01, 0.01))) # prior values\n\nlik2 =  bru_obs(formula = y ~.,\n                family = \"gaussian\",\n                data = df,\n                control.family = list(hyper = prec.tau))\n\nfit.lm2 = bru(cmp2, lik2) \n\nThe names of the priors available in¬†R-INLA¬†can be seen with names(inla.models()$prior)\n\n\nVisualizing the posterior marginals\nPosterior marginal distributions of the Ô¨Åxed effects parameters and the hyperparameters can be visualized using the plot() function by calling the name of the component. For example, if want to visualize the posterior density of the intercept \\(\\beta_0\\) we can type:\n\n\nCode\nplot(fit.lm, \"beta_0\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nPlot the posterior marginals for \\(\\beta_1\\) and for the precision of the observation error \\(\\pi(\\tau|y)\\)\n\n\nTake hint\n\nSee the summary() output to check the names for the different model components.\n\n\n\n\nClick here to see the solution\n\n\nCode\nplot(fit.lm, \"beta_1\") +\nplot(fit.lm, \"Precision for the Gaussian observations\")"
  },
  {
    "objectID": "day1_practical_2.html#sec-llm_fish",
    "href": "day1_practical_2.html#sec-llm_fish",
    "title": "Practical 2",
    "section": "Linear Mixed Model for fish weight-length relationship",
    "text": "Linear Mixed Model for fish weight-length relationship\nIn this exercise we will:\n\nPlot random effects of a LMM\nCompute posterior densities and summaries for the variance components\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nIn this exercise, we will use a subset of the Pygmy Whitefish (Prosopium coulterii) dataset from the FSAdata R package, containing biological data collected in 2001 from Dina Lake, British Columbia.\n Download data set \nThe data set contains the following information:\n\nnet_noUnique net identification number\nwt Fish weight (g)\ntl Total fish length (cm)\nsex Sex code (F=Female, M = Male)\n\nWe can visualize the distribution of the response (weight) across the nets split by sex as follows:\n\nPygmyWFBC &lt;- read.csv(\"datasets/PygmyWFBC.csv\")\n\nggplot(PygmyWFBC, aes(x = factor(net_no), y = wt,fill = sex)) + \n  geom_boxplot() + \n  labs(y=\"Weight (g)\",x = \"Net no.\")\n\n\n\n\n\n\n\n\nSuppose we are interested in modelling the weight-length relationship for captured fish. The exploratory plot suggest some important variability in this relationship, potentially attributable to differences among sampling nets deployed across various sites in the Dina Lake.\nTo account for this between-net variability, we model net as a random effect using the following linear mixed model:\n\\[\n\\begin{aligned}\ny_{ij} &\\sim\\mathcal{N}(\\mu_{ij}, \\sigma_e^2), \\qquad i = 1,\\dots,a \\qquad j = 1,\\ldots,n \\\\\n\\eta_{ij} &= \\mu_{ij} = \\beta_0 + \\beta_1 \\times \\text{length}_j + \\beta_2 \\times \\mathbb{I}(\\mathrm{Sex}_{ij}=\\mathrm{M}) +  u_i \\\\\nu_i &\\sim \\mathcal{N}(0,\\sigma^2_u)\n\\end{aligned}\n\\]\nwhere:\n\n\\(y_{ij}\\) is the weight of the \\(j\\)-th fish from net \\(i\\)\n\\(\\text{length}_{ij}\\) is the corresponding fish length\n\\(\\mathbb{I}(\\text{Sex}_{ij} = \\text{M})\\) is an indicator/dummy such that for the ith net \\[\n\\mathbb{I}(\\mathrm{Sex}_{ij}) \\begin{cases}1 & \\text{if the } j \\text{th fish is Male} \\\\0 & \\text{otherwise} \\end{cases}\n\\]\n\\(u_i\\) represents the random intercept for net \\(i\\)\n\\(\\sigma_u^2\\) and \\(\\sigma_\\epsilon^2\\) are the between-net and residual variances, respectively\n\nTo run this model ininlabru we first need to create our sex dummy variable :\n\nPygmyWFBC$sex_M &lt;- ifelse(PygmyWFBC$sex==\"F\",0,1)\n\ninlabru will treat 0 as the reference category (i.e., the intercept \\(\\beta_0\\) will represent the baseline weight for females ). Now we can define the model component, the likelihood and fit the model.\n\ncmp =  ~ -1 + sex_M + \n  beta_0(1)  + \n  beta_1(tl, model = \"linear\") +  \n  net_eff(net_no, model = \"iid\")\n\nlik =  bru_obs(formula = wt ~ .,\n            family = \"gaussian\",\n            data = PygmyWFBC)\n\nfit = bru(cmp, lik)\n\nsummary(fit)\n\ninlabru version: 2.13.0.9011 \nINLA version: 25.09.19 \nComponents: \nLatent components:\nsex_M: main = linear(sex_M)\nbeta_0: main = linear(1)\nbeta_1: main = linear(tl)\nnet_eff: main = iid(net_no)\nObservation models: \n  Family: 'gaussian'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'numeric'\n    Predictor: wt ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[sex_M, beta_0, beta_1, net_eff], latent[] \nTime used:\n    Pre = 0.897, Running = 0.192, Post = 0.0332, Total = 1.12 \nFixed effects:\n          mean    sd 0.025quant 0.5quant 0.975quant    mode kld\nsex_M   -1.106 0.218     -1.534   -1.106     -0.678  -1.106   0\nbeta_0 -15.816 0.870    -17.516  -15.819    -14.099 -15.819   0\nbeta_1   2.555 0.072      2.414    2.555      2.696   2.555   0\n\nRandom effects:\n  Name    Model\n    net_eff IID model\n\nModel hyperparameters:\n                                         mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 0.475 0.044      0.393    0.473\nPrecision for net_eff                   2.146 1.313      0.565    1.839\n                                        0.975quant mode\nPrecision for the Gaussian observations      0.567 0.47\nPrecision for net_eff                        5.525 1.32\n\nMarginal log-Likelihood:  -467.54 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nFor interpretability, we could have centered the predictors, but our primary focus here is on estimating the variance components of the mixed model.\nWe can plot the posterior density of the nets random intercept as follows:\n\nplot(fit,\"net_eff\")\n\n\n\n\n\n\n\n\nFor theoretical and computational purposes, INLA works with the precision which is the inverse of the variance. To obtain the posterior summaries on the SDs scale we can sample from the posterior distribution for the precision while back-transforming the samples and then computing the summary statistics. Transforming the samples is necessary because some quantities such as the mean and mode are not invariant to monotone transformation; alternatively we can use some of the in-built inlabru functions to achieve this (see supplementary note).\nWe use the predict function to draw samples from the approximated joint posterior for the hyperparameters, then invert them to get variances and lastly compute the mean, std. dev., quantiles, etc.\n\n\n\n\n\n\nNote\n\n\n\nTo get the right name for the hyperparameters to use in the predict() function, you can use the function bru_names().\n\n\n\nsampvars &lt;-  predict(fit,PygmyWFBC, ~ {\n   tau_e &lt;- Precision_for_the_Gaussian_observations\n   tau_u &lt;- Precision_for_net_eff\n   list(sigma_u = 1/tau_u,\n        sigma_e = 1/tau_e)\n   },\n   n.samples = 1000\n  )\n\nnames(sampvars) = c(\"Error variance\",\"Between-net Variance\")\n\nsampvars\n\n$`Error variance`\n       mean        sd    q0.025      q0.5   q0.975    median sd.mc_std_err\n1 0.6589192 0.5098526 0.1805335 0.5183674 1.713751 0.5183674    0.03822821\n  mean.mc_std_err\n1      0.01854072\n\n$`Between-net Variance`\n      mean        sd   q0.025     q0.5   q0.975   median sd.mc_std_err\n1 2.116332 0.1935687 1.760922 2.109438 2.543536 2.109438   0.005476093\n  mean.mc_std_err\n1      0.00646752\n\nattr(,\"class\")\n[1] \"bru_prediction\" \"list\"          \n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nAnother useful quantity we can compute is the intraclass correlation coefÔ¨Åcient (ICC) which help us determine how much the response varies within groups compared to between groups. The intraclass correlation coefÔ¨Åcient is defined as:\n\\[\n\\text{ICC} = \\frac{\\sigma^2_u}{\\sigma^2_u + \\sigma^2_e}\n\\]\nCompute the mean, median, and quantiles for the ICC by drawing posterior samples for \\(\\sigma^2_e\\) and \\(\\sigma^2_u\\) using the predict function.\n\n\n\nClick here to see the solution\n\n\nCode\nICC &lt;-  predict(fit,PygmyWFBC, ~ {\n   tau_e &lt;- Precision_for_the_Gaussian_observations\n   tau_u &lt;- Precision_for_net_eff\n   sigma_u = 1/tau_u\n   sigma_e = 1/tau_e\n   list(ICC = sigma_u/ (sigma_u+sigma_e))\n   },\n   n.samples = 1000\n  )\n\n\n\n\n\n\n\n\n\n\n\nNoteSupplementary Material\n\n\n\nThe marginal densities for the hyper parameters can be also found by callinginlabru_model$marginals.hyperpar. We can then apply a transformation using the inla.tmarginal function to transform the precision posterior distributions.\n\nvar_e &lt;- fit$marginals.hyperpar$`Precision for the Gaussian observations` %&gt;%\n  inla.tmarginal(function(x) 1/x,.) \n\nvar_u &lt;- fit$marginals.hyperpar$`Precision for net_eff` %&gt;%\n  inla.tmarginal(function(x) 1/x,.) \n\nThe marginal densities for the hyper parameters can be found with inlabru_model$marginals.hyperpar, then we can apply a transformation using the inla.tmarginal function to transform the precision posterior distributions. Then, we can compute posterior summaries using inla.zmarginal function as follows:\n\npost_var_summaries &lt;- cbind( inla.zmarginal(var_e,silent = T),\n                             inla.zmarginal(var_u,silent = T))\ncolnames(post_var_summaries) &lt;- c(\"sigma_e\",\"sigma_u\")\npost_var_summaries\n\n           sigma_e  sigma_u  \nmean       2.12447  0.6497205\nsd         0.198099 0.4142985\nquant0.025 1.764527 0.1814794\nquant0.25  1.985373 0.3688918\nquant0.5   2.113733 0.5414238\nquant0.75  2.252014 0.8063269\nquant0.975 2.542007 1.747894"
  },
  {
    "objectID": "day2_practical_4.html",
    "href": "day2_practical_4.html",
    "title": "Practical 4",
    "section": "",
    "text": "Aim of this practical:\nDownload Practical 3 R script\nIn this practical we will:"
  },
  {
    "objectID": "day2_practical_4.html#sec-areal_data",
    "href": "day2_practical_4.html#sec-areal_data",
    "title": "Practical 4",
    "section": "Areal (lattice) data",
    "text": "Areal (lattice) data\nAreal data our measurements are summarised across a set of discrete, non-overlapping spatial units such as postcode areas, health board or pixels on a satellite image. In consequence, the spatial domain is a countable collection of (regular or irregular) areal units at which variables are observed. Many public health studies use data aggregated over groups rather than data on individuals - often this is for privacy reasons, but it may also be for convenience.\nIn the next example we are going to explore data on respiratory hospitalisations for Greater Glasgow and Clyde between 2007 and 2011. The data are available from the CARBayesdata R Package:\n\nlibrary(CARBayesdata)\n\ndata(pollutionhealthdata)\ndata(GGHB.IZ)\n\nThe pollutionhealthdata contains the spatiotemporal data on respiratory hospitalisations, air pollution concentrations and socio-economic deprivation covariates for the 271 Intermediate Zones (IZ) that make up the Greater Glasgow and Clyde health board in Scotland. Data are provided by the Scottish Government and the available variables are:\n\nIZ: unique identifier for each IZ.\nyear: the year were the measruments were taken\nobserved: observed numbers of hospitalisations due to respiratory disease.\nexpected: expected numbers of hospitalisations due to respiratory disease computed using indirect standardisation from Scotland-wide respiratory hospitalisation rates.\npm10: Average particulate matter (less than 10 microns) concentrations.\njsa: The percentage of working age people who are in receipt of Job Seekers Allowance\nprice: Average property price (divided by 100,000).\n\nThe GGHB.IZ data is a Simple Features (sf) object containing the spatial polygon information for the set of 271 Intermediate Zones (IZ), that make up of the Greater Glasgow and Clyde health board in Scotland ( Figure¬†1 ).\n\n\n\n\n\n\n\n\nFigure¬†1: Greater Glasgow and Clyde health board represented by 271 Intermediate Zones\n\n\n\n\nLet‚Äôs start by loading useful libraries:\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(scico)\n\nThe sf package allows us to work with vector data which is used to represent points, lines, and polygons. It can also be used to read vector data stored as a shapefiles.\nFirst, lets combine both data sets based on the Intermediate Zones (IZ) variable using the merge function from base R:\n\nresp_cases &lt;- merge(GGHB.IZ, pollutionhealthdata, by = \"IZ\")\n\nIn epidemiology, disease risk is usually estimated using Standardized Mortality Ratios (SMR). The SMR for a given spatial areal unit \\(i\\) is defined as the ratio between the observed ( \\(Y_i\\) ) and expected ( \\(E_i\\) ) number of cases:\n\\[\nSMR_i = \\dfrac{Y_i}{E_i}\n\\]\nA value \\(SMR &gt; 1\\) indicates that there are more observed cases than expected which corresponds to a high risk area. On the other hand, if \\(SMR&lt;1\\) then there are fewer observed cases than expected, suggesting a low risk area.\nWe can manipulate sf objects the same way we manipulate standard data frame objects via the dplyr package. Lets use the pipeline command %&gt;% and the mutate function to calculate the yearly SMR values for each IZ:\n\nlibrary(dplyr)\nresp_cases &lt;- resp_cases %&gt;% \n  mutate(SMR = observed/expected, .by = year )\n\nNow we use ggplot to visualize our data by adding a geom_sf layer and coloring it according to our variable of interest (i.e., SMR). We can further use facet_wrap to create a layer per year and chose an appropriate color palette using the scale_fill_scico from the scico package:\n\nggplot()+\n  geom_sf(data=resp_cases,aes(fill=SMR))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"roma\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nProduce a map that shows the spatial distribution of each of the following variables for the year 2011:\n\nAverage particulate matter pm10\nAverage property price price\nPercentage of working age people who are in receipt of Job Seekers Allowance jsa\n\n\n\nhint\n\nYou can use the filter function from dplyr to subset the data according to the year of interest.\n\n\n\n\nClick here to see the solution\n\n# Library for plotting multiple maps together\n\nlibrary(patchwork)\n\n# subset data set for 2011\n\nresp_cases_2011 &lt;- resp_cases %&gt;% filter(year ==2011)\n\n# pm10 plot\n\npm10_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=pm10))+\n  scale_fill_scico(palette = \"navia\")\n\n# property price\n\nprice_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=price))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"bilbao\")\n\n#  percentage jsa\n\njsa_plot &lt;- ggplot()+\n  geom_sf(data=resp_cases_2011,aes(fill=jsa))+\n  facet_wrap(~year)+scale_fill_scico(palette = \"lapaz\") \n\n# plot maps together\n\npm10_plot + price_plot + jsa_plot + plot_layout(ncol=3)\n\n\n\n\n\n\n\n\n\n\n\nIn this practical we will:\n\nExplore tools for geostatistical spatial data wrangling and visualization.\n\nFirst, lets load some useful libraries for data wrangling and visualization\n\n# For plotting\nlibrary(mapview)\nlibrary(ggplot2)\nlibrary(scico) # for colouring palettes\n\n# Data manipulation\nlibrary(dplyr)"
  },
  {
    "objectID": "day2_practical_4.html#georeferenced-data",
    "href": "day2_practical_4.html#georeferenced-data",
    "title": "Practical 4",
    "section": "Georeferenced data",
    "text": "Georeferenced data\nTobler‚Äôs first law of geography states that:\n‚ÄúEverything is related to everything else, but near things are more related than distant things‚Äù\nSpatial patterns are fundamental in environmental and ecological data. In many ecological and environmental settings, measurements from fixed sampling units, aiming to quantify spatial variation and interpolate values at unobserved sites.\nGeorefernced data are the most common form of spatial data found in environmental setting. In these data we regularly take measurements of a spatial ecological or environmental process at a set of fixed locations. This could be data from transects (e.g, where the height of trees is recorded), samples taken across a region (e.g., water depth in a lake) or from monitoring stations as part of a network (e.g., air pollution). In each of these cases, our goal is to estimate the value of our variable across the entire space.\nLet \\(D\\) be our two-dimensional region of interest. In principle, there is aninfinite number of locations within \\(D\\), each of which can be represented by mathematical coordinates (e.g., latitude and longitude). We then can identify any individual location as \\(s_i = (x_i, y_i)\\), where \\(x_i\\) and \\(y_i\\) are their coordinates.\nWe can treat our variable of interest as a random variable, \\(Z\\) which can be observed at any location as \\(Z(\\mathbf{s}_i)\\).\nOur geostatistical process can therefore be written as: \\[\\{Z(\\mathbf{s}); \\mathbf{s} \\in D\\}\\]\nIn practice, our data are observed in a finite number of locations, \\(m\\), and can be denoted as:\n\\[z = \\{z(\\mathbf{s}_1), \\ldots z(\\mathbf{s}_m) \\}\\]\nIn the next example, we will explore data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound. The pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)). The qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound.\n\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod \nqcs_grid = sdmTMB::qcs_grid\n\n\nGeoreferrenced data\nLet‚Äôs create an initial sf spatial object using the standard geographic coordinate system (EPSG:4326). This correctly defines the point locations based on latitude and longitude.\n\nlibrary(sf)\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\n\nNow we can transform to the standard UTM Zone 9N projection (EPSG:32609) which uses meters:\n\npcod_sf_proj &lt;- st_transform(pcod_sf, crs = 32609)\nst_crs(pcod_sf_proj)$units\n\n[1] \"m\"\n\n\nWe can change the spatial units to km to better reflect the scale of our ecological study and to make resulting distance/area values more intuitive to interpret:\n\npcod_sf_proj = st_transform(pcod_sf_proj,\n                            gsub(\"units=m\",\"units=km\",\n                                 st_crs(pcod_sf_proj)$proj4string)) \nst_crs(pcod_sf_proj)$units\n\n[1] \"km\"\n\n\nInstead of first setting an EPSG code and then transforming, we can define the target Coordinate Reference System (CRS) directly using a proj4string. This allows us to customize non-standard parameters in a single step, in this case, explicitly setting the projection units to kilometers (+units=km).\n\npcod_sf = st_transform(pcod_sf,\n                       crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\nst_crs(pcod_sf)$units\n\n[1] \"km\"\n\n\nSpatial sf objects can be manipulated the same way we manipulate standard data frame objects via the dplyr package. For example, you can select a specific year using the filter function from dplyr. Let‚Äôs map the present/absence of the Pacific Cod in 2017 using the mapview function:\n\npcod_sf %&gt;% \n  filter(year== 2017) %&gt;%\n  mutate(present = as.factor(present)) %&gt;%\nmapview(zcol = \"present\",\n        layer.name = \"Occupancy status of Pacific Cod in 2017\")\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nUse ggplot and the sf library to map the biomass density of the pacific cod across years.\n\n\nhint\n\nYou can plot ansf object by adding a geom_sf layer to a ggplot object. You can also use the facet_wrap argument to plot an arrange of plots according to a grouping variable.\n\n\n\n\nClick here to see the solution\n\nggplot()+ \n  geom_sf(data=pcod_sf,aes(color=density))+ \n  facet_wrap(~year)+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nRaster Data\nEnvironmental data are typically stored in raster format, which represents spatially continuous phenomena by dividing a region into a grid of equally-sized cells, each storing a value for the variable of interest. In R, the terra package is a modern and powerful tool for efficiently working with raster data. The function rast(), can be used both to read raster files from standard formats (e.g., .tif or .tiff) and to create a new raster object from a data frame. For instance, the following code creates a raster from the qcs_grid grid data for Queen Charlotte Sound.\n\nlibrary(terra)\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\ndepth_r\n\nclass       : SpatRaster \nsize        : 102, 121, 3  (nrow, ncol, nlyr)\nresolution  : 2, 2  (x, y)\nextent      : 341, 583, 5635, 5839  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nnames       :    depth, depth_scaled, depth_scaled2 \nmin values  :  12.0120,    -6.000040,  4.892624e-08 \nmax values  : 805.7514,     3.453937,  3.600048e+01 \n\n\nThe raster object contains three layers corresponding to the (i) depth values, (ii) the scaled depth values and (iii) the squared depth values.\nNotice that there are no CRS associated with the raster. Thus, we can assign appropriate CRS using the crs function. Additionally, we also want the raster CRS to match the CRS in the survey data (recall that we have previously reprojected our data to utm coordinates). We can assign an appropiate CRS that matches the CRS of the sf object as follows:\n\ncrs(depth_r) &lt;- crs(pcod_sf)\n\nWe can use the tidyterra package to plot raster data using ggplot by adding a geom_spatraster function and then select an appropriate fill and color palettes:\n\nlibrary(tidyterra)\n\n\nAttaching package: 'tidyterra'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth)+\n  geom_sf(data=pcod_sf,aes(color=factor(present))) +\n  facet_wrap(~year)+\n    scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n  scale_fill_scico(name = \"Depth\",\n                   palette = \"nuuk\",\n                   na.value = \"transparent\" )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nMap the scaled depth and the presence/absence records of the Pacific cod for 2003 to 2005 only.\n\n\nhint\n\nThe different layers of a raster can be accessed using the $ symbol.\n\n\n\n\nClick here to see the solution\n\nggplot()+ \n  geom_spatraster(data=depth_r$depth_scaled)+\n  geom_sf(data=pcod_sf %&gt;% filter(year %in% 2003:2005),\n          aes(color=factor(present)))+ \n  facet_wrap(~year)+\n  scale_color_manual(name=\"Occupancy status for the Pacific Cod\",\n                     values = c(\"black\",\"orange\"),\n                     labels= c(\"Absence\",\"Presence\"))+\n    scale_fill_scico(name = \"Scaled Depth\",\n                     palette = \"davos\",\n                     na.value = \"transparent\" )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this practical we will:\n\nExplore tools for spatial point pattern data wrangling and visualization.\n\nFirst, lets load some useful libraries:\n\n# For plotting\nlibrary(ggplot2)\nlibrary(scico) # for colouring palettes\n\n# Data manipulation\nlibrary(dplyr)"
  },
  {
    "objectID": "day2_practical_4.html#spatial-point-pattern-data",
    "href": "day2_practical_4.html#spatial-point-pattern-data",
    "title": "Practical 4",
    "section": "Spatial Point pattern data",
    "text": "Spatial Point pattern data\nSpatial point patterns repersent the locations of events, objects or individuals in space, and the coordinates of such occurrences are our data.\nPoint process models are probabilistic models that describe the likelihood of these patterns of points that represent the random locations of the events. A spatial point pattern is a set of locations that have been generated by some form of stochastic (random) mechanism. In other words, the point process is a random variable operating in continuous space, and we observe realisations of this variable as point patterns across space (and/or time).\nConsider a fixed geographical region \\(A\\). The set of locations at which events occur are denoted \\(\\mathbf{s} = s_1,\\ldots,s_n\\). We let \\(N(A)\\) be the random variable which represents the number of events in region \\(A\\).\nOur primary interest is in measuring where events occur, so the locations are our data. We typically assume that a spatial point pattern is generated by an unique point process over the whole study area. This means that the delimitation of the study area will affect the observed point pattens.\nThe observed distribution of points can be described based on the intensity of points within a delimited region. We can define the (first order) intensity of a point process as the expected number of events per unit area. This can also be thought of as a measure of the density of our points. In some cases, the intensity will be constant over space (homogeneous), while in other cases it can vary by location (inhomogeneous or heterogeneous).\nIn the next example, we will explore tools for visualizing spatial point patterns. Specifically, we will map the spatial distribution of the Ringlet butterfly in Scotland‚Äôs Cairngorms National Park (CNP).\n\nBNM citizen science program\nCitizen science initiatives have become an important source of information in ecological research, offering large volumes of species distribution data collected by volunters for multiple taxonomic groups across wide spatial and temporal scales\nButterflies for the New Millennium (BNM) is a large-scale monitoring scheme launched in the earlies 70‚Äôs to keep track of butterflies‚Äô populations in the UK. With over 12 million butterflies sighting and more than 10,000 volunteers, this recording scheme has proven to be a successful program that has been used to assess long-term changes in the distributions of UK butterfly species.\nHere we will focus on the distribution of the Ringlet butterfly species, which holds particular significance in environmental studies as one of the Habitat specialists species (UK Government, 2024). The data set consists of Ringlet butterfly presence-only records collected by volunteers in Scotland‚Äôs Cairngorms National Park (CNP).\nReading shapefiles into R\nFirst, we load the geographical region of interest which can be downloaded here (i.e., CNP boundaries). We can use thre st_read function from the sf library to load the .shp file by specifying the directory where you downloaded the files:\n\nlibrary(sf)\nshp_SGC &lt;-  st_read(\"datasets/SG_CairngormsNationalPark/SG_CairngormsNationalPark_2010.shp\",quiet =T)\n\nThen, we can use appropriate CRS for the UK (i.e., EPSG code: 27700) :\n\nshp_SGC &lt;- shp_SGC %&gt;% st_transform(crs = 27700)\nst_crs(shp_SGC)$units\n\n[1] \"m\"\n\n\nNotice that the spatial resolution is in meters. Let‚Äôs change the spatial units to km to make resulting distance/area values more intuitive to interpret:\n\nshp_SGC &lt;- st_transform(shp_SGC,gsub(\"units=m\",\"units=km\",st_crs(shp_SGC)$proj4string)) \nst_crs(shp_SGC)$units\n\n[1] \"km\"\n\n\nWe can then plot the CNP boundary as follows:\n\nggplot()+\n  geom_sf(data=shp_SGC)\n\n\n\n\n\n\n\n\nCreating sf spatial objects in R\nNow we will read the Ringlet butterfly records which can be downloaded below:\n Download data set \n\nringlett &lt;- read.csv(\"datasets/bnm_ringlett.csv\")\nhead(ringlett)\n\n         y         x\n1 57.58752 -2.712498\n2 54.97742 -3.274879\n3 54.89929 -3.771451\n4 55.40323 -5.737059\n5 54.91438 -3.959336\n6 55.87255 -4.167174\n\n\nThe data set contains the longitude latitude where an observation was made. We can convert this into a spatial sf object using the st_as_sf function by declaring the columns in our data that contain the spatial coordinates:\n\nringlett_sf &lt;- ringlett %&gt;% st_as_sf(coords = c(\"x\",\"y\"),crs = \"+proj=longlat +datum=WGS84\") \n\n\n\n\n\n\n\nWarning Task\n\n\n\nWe have set standard WGS84 coordinates for the Ringlet butterfly occurrence records. Set the CRS to match the CRS used in shapefile. Then, produce a map of the CNP region with the projected observations overlayed.\n\n\nTake hint\n\nYou can use the st_transform() function to change the coordinates of an sf object (type ?st_transform for more details)/\n\n\n\n\nClick here to see the solution\n\n\nCode\nringlett_sf &lt;- ringlett_sf %&gt;%\n  st_transform(st_crs(shp_SGC))\n\nggplot()+\n  geom_sf(data=shp_SGC)+\n  geom_sf(data=ringlett_sf)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can subset two sf objects with the same CRS in the same way as we subset a data frame in R. For example, if we want to subset the Ringlet butterfly occurrence records to those contained only within the CNP, we can type the following:\n\nringlett_CNP &lt;- ringlett_sf[shp_SGC,] # crop to mainland\n\nIf we plot the ringlett_CNP object along with the CNP boundary, we should then obtain a map of the occurrence records within the park:\n\nggplot()+\n  geom_sf(data=shp_SGC)+\n  geom_sf(data=ringlett_CNP)\n\n\n\n\n\n\n\n\nReading Raster Data\nWe can use the terra R package to read raster files. The Scotland_elev.tiff raster contains the output of a digital elevation model for Scotland:\n Download raster data \nOnce you download the raster you can read it using the rast function after specifying the path where the file has been stored. Then, we assign the same CRS as our data.\n\nlibrary(terra)\nelevation_r &lt;- rast(\"datasets/Scotland_elev.tiff\")\ncrs(elevation_r) = crs(shp_SGC)\nplot(elevation_r)\n\n\n\n\n\n\n\n\nWe can apply different R functions to our rasters. For example, we can scale the elevation values as follows:\n\nelevation_r &lt;- elevation_r %&gt;% scale()\n\nLastly, we can crop the raster to the boundaries of our region of interest. Let‚Äôs crop the elevation raster to the CNP area using the crop function:\n\nelev_CNP &lt;- terra::crop(elevation_r,shp_SGC,mask=T)\nplot(elev_CNP)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nUsing tidyterra and ggplot, produce a map of the elevation profile in the CNP and overlay the spatial point pattern of the Ringlet butterfly occurrence records. Use an appropriate colouring scheme for the elevation values. Do you see any pattern?\n\n\nTake hint\n\nYou can use the geom_spatraster() to add a raster layer to a ggplot object. Furthermore the scico library contains a nice range of coloring palettes you can choose, type scico_palette_show() to see the color palettes that are available.\n\n\n\n\nClick here to see the solution\n\n\nCode\nlibrary(scico)\n\nggplot()+ \n  tidyterra::geom_spatraster(data=elev_CNP)+\n  geom_sf(data=ringlett_CNP)+\n  scale_fill_scico(name = \"Elevation scaled\",\n                   palette = \"devon\",\n                   na.value = \"transparent\" )"
  },
  {
    "objectID": "day4_practical_6.html",
    "href": "day4_practical_6.html",
    "title": "Practical 6",
    "section": "",
    "text": "Aim of this practical:\nIn this practical we are going to look at space-time model and model comparison and validation techniques.\nDownload Practical 6 R script"
  },
  {
    "objectID": "day4_practical_6.html#space-time-models",
    "href": "day4_practical_6.html#space-time-models",
    "title": "Practical 6",
    "section": "Space time models",
    "text": "Space time models\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(inlabru) \nlibrary(sf)\nlibrary(terra)\n\n\n# load some libraries to generate nice map plots\nlibrary(scico)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(mapview)\nlibrary(tidyterra)\n\n\nThe data\nIn this practical, we will revisit the data on the Pacific Cod (Gadus macrocephalus) from a trawl survey in Queen Charlotte Sound. The pcod dataset is available from the sdmTMB package and contains the presence/absence records of the Pacific Cod during each surveys along with the biomass density of Pacific cod in the area swept (kg/Km\\(^2\\)). The qcs_grid data contain the depth values stored as \\(2\\times 2\\) km grid for Queen Charlotte Sound.\nThe dataset contains presence/absence data from 2003 to 2017. To make computations faster we only consider the first 3 years.\n\nlibrary(sdmTMB)\n\npcod_df = sdmTMB::pcod  %&gt;% filter(year&lt;=2005)\nqcs_grid = sdmTMB::qcs_grid\n\nThen, we create a sf object and assign the right coordinate reference to it:\n\npcod_sf =   st_as_sf(pcod_df, coords = c(\"lon\",\"lat\"), crs = 4326)\npcod_sf = st_transform(pcod_sf,\n          crs = \"+proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\" )\n\nWe convert the covariates into a raster and assign the same coordinate reference:\n\ndepth_r &lt;- rast(qcs_grid, type = \"xyz\")\ncrs(depth_r) &lt;- crs(pcod_sf)\n\n\n\n\n\n\n\n\n\n\n\n\nSpatio-temporal modeling\n\nModel fitting\nNow lets compare two different space-time models using LGOCV and some information criteria metrics. The general model structure is given by:\n\\[\n\\begin{aligned}\ny(s,t)|\\eta(s,t)&\\sim\\text{Binom}(1, p(s,t))\\\\\n\\eta(s,t) &= \\text{logit}(p(s,t)) \\\\\n\\end{aligned}\n\\] We also want to compare the models using WAIC, DIC and marginal likelihood:\n\n\n\n\n\n\nWarning Task\n\n\n\nSet the bru_options so that the quantities of interest are computed\n\n\n\nClick here to see the solution\n\nbru_options_set(control.compute = list(waic = TRUE,dic= TRUE,mlik = TRUE))\n\n\n\n\n\n\nModel 1\n\nModel 1 - time iid effect We consider a separable space-time model with a linear predictors given by:\n\n\\[\n\\eta(s,t) = \\beta_0 + f_1(\\text{depth}(s)) + f_2(t) + \\omega(s)\n\\]\n\n\\(f_1(\\text{depth}(s))\\) is a smooth covariate effect of depth (modeled using a RW2 model)\n\\(f_2(t)\\) is an IID effect of time\n\\(\\omega(s)\\) is Mat√©rn random field.\n\nThe first step is to define the mesh and the spde model\nConstruct the mesh and the SPDE model\n\nmesh = fm_mesh_2d(loc = pcod_sf,    \n                  cutoff = 1,\n                  max.edge = c(10,20),     \n                  offset = c(5,50),\n                  crs = st_crs(pcod_df))   \n\nspde_model =  inla.spde2.pcmatern(mesh,\n                                  prior.sigma = c(1, 0.5),\n                                  prior.range = c(100, 0.5))\n\n\n\n\n\n\n\n\n\n\ncreate time index and the grouped variable\nTo use the RW2 model the covariate has to be groupes:\n\ndepth_r$depth_group = inla.group(values(depth_r$depth_scaled))\n\nwe also define a time index from 1 to in the data frame\n\npcod_sf = pcod_sf %&gt;%\n     mutate(time_idx = match(year, c(2003, 2004, 2005)),\n         id = 1:nrow(.)) # Observation id for CV\n\n\n\n\n\n\n\nWarning Task\n\n\n\nImplement the model in inlabru\n\nDefine the components\nDefine the formula\nDefine the likelihood model using the bru_obs() function\nRun the model\n\n\n\n\nClick here to see the solution\n\n# Model components\ncmp_spat = ~ Intercept(1) + \n  covariate(depth_r$depth_group, model = \"rw2\", scale.model = TRUE)+\n  trend(time_idx, model = \"iid\")+\n  space(geometry, model = spde_model)\n\n# Linear predictor\nformula_spat = present ~ Intercept  + trend + space + covariate\n\n# Observational model\nlik_spat = bru_obs(formula = formula_spat, \n              data = pcod_sf, \n              family = \"binomial\")\n\n# Fit Model \nfit_spat = bru(cmp_spat,lik_spat)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that there are some survey locations in certain years that fall outside the depth raster region. inlabru will input these missing covariate values using the nearest available value. This can be computationally expensive, but you can avoid it by supplying a raster layer that encompasses all of your data points (e.g., by pre-imputing these missing values with your preferred method of choice).\nOne way of doing this in the inlabru framework is to use the bru_fill_missing() function:\n\n# Select the raster of interest\ndepth_orig = depth_r$depth_group\nre &lt;- extend(depth_orig, ext(depth_orig)*1.05)\n# Convert to an sf spatial object\nre_df &lt;- re %&gt;% stars::st_as_stars() %&gt;%  st_as_sf(na.rm=F)\n# fill in missing values using the original raster \nre_df$depth_group =  bru_fill_missing(depth_orig,re_df,re_df$depth_group)\n# rasterize\ndepth_filled &lt;- stars::st_rasterize(re_df) %&gt;% rast()\nplot(depth_filled)\n\n\n\n\n\n\n\n\n\n\nWe have now fit the model and want to check the results.\n\n\n\n\n\n\nWarning Task\n\n\n\nIs the effect of depth significant? Does it seem important to have a non-linear model?\nInspect the estimated time effect and\nUse the predict function to inspect the estimated spatial effect.\nUse the depth_r raster to define the points in space where to predict\n\npxl = st_as_sf(data.frame(crds(depth_r)), coords = c(\"x\",\"y\") ,\n               crs  = st_crs(pcod_sf))\n\n\n\n\nClick here to see the solution\n\n# covariate effect\n\np_cov = fit_spat$summary.random$covariate %&gt;%\n  ggplot() + geom_ribbon(aes(ID, ymin = `0.025quant`, ymax = `0.975quant` )) +\n  geom_line(aes(ID,mean)) + ggtitle(\"Covariate effect\")\n\n# time effect\np_time = fit_spat$summary.random$trend %&gt;%\n  ggplot() + geom_errorbar(aes(ID, ymin = `0.025quant`, ymax = `0.975quant` )) +\n  geom_point(aes(ID,mean)) + ggtitle(\"Time effect\")\n\n#space effect\npred_space = predict(fit_spat, pxl, ~ space)\n\np_space_mean = ggplot() + gg(pred_space, aes(color = mean))\np_space_sd = ggplot() + gg(pred_space, aes(color = sd))\n\n\n\n\n\n\nModel 2\n\nModel 2 - spatiotemporal field We consider a separable space time model with a linear predictor given by:\n\n\\[\n\\eta(s,t) = \\beta_0 + f_1(\\text{depth}(s)) + \\omega(s,t)\n\\]\n\n\\(f_1(\\text{depth}(s))\\) is a smooth covariate effect of depth (RW2)\n\\(\\omega(s,t)\\) is a space-time Mat√©rn spatial field with AR1 time component \\[\n\\omega(s,t) = \\phi\\ \\omega(s,t-1) + \\epsilon(s),\\qquad \\epsilon(s)\\sim\\text{GF}(\\sigma_{\\epsilon},\\rho_{\\epsilon})\n\\]\n\n\n\n\n\n\n\nWarning Task\n\n\n\nImplement this second model in inlabru\n\nDefine the components For the AR1 model use this following PC prior for the correlation parameter \\(\\phi\\)\n\n\n# PC prior for AR(1) correlation parameter\nh.spec &lt;- list(rho = list(prior = 'pc.cor0', param = c(0.5, 0.1)))\n\n\nDefine the formula\nDefine the likelihood model using the bru_obs() function\nRun the model (This model can take a couple of minutes to run)\n\n\n\n\nClick here to see the solution\n\n# Model components\ncmp_spat_ar1 = ~ Intercept(1) + \n  covariate(depth_filled$depth_group, model = \"rw2\", scale.model = TRUE)+\n  space_time(geometry,\n        group = time_idx ,\n        model = spde_model,\n        control.group = list(model = 'ar1',hyper = h.spec))\n\n# Linear predictor\nformula_spat_ar1 = present ~ .\n\n# Observational model\nlik_spat_ar1 = bru_obs(formula = formula_spat_ar1, \n              data = pcod_sf, \n              family = \"binomial\")\n\n# Fit Model \nfit_spat_ar1 = bru(cmp_spat_ar1,lik_spat_ar1)\n\n\n\n\nNow we want to check the results\n\n\n\n\n\n\nWarning Task\n\n\n\nWhat is the estimated parameter \\(phi\\) in the auto-regressive part of the model?\nCheck the effect of the covariate.\nUse the predict function to inspect the estimated probability of presence. Use the same prediction points as before, but here you also need to use the `\n\n\n\nClick here to see the solution\n\n# autoregressive effect\n\n#fit_spat_ar1$summary.hyperpar\n\n#covariate\np_cov_ar1 = fit_spat_ar1$summary.random$covariate %&gt;%\n  ggplot() + geom_ribbon(aes(ID, ymin = `0.025quant`, ymax = `0.975quant` )) +\n  geom_line(aes(ID,mean)) + ggtitle(\"Covariate effect\")\n\n\n#space-time effect\n\ninv_logit = function(x){ exp(x) / (1 + exp(x))}\n\npxl_all = fm_cprod(pxl, data.frame(time_idx = 1:3))\npred_space_ar1 = predict(fit_spat_ar1, pxl_all, ~data.frame(logit_prob = Intercept  + \n                                                               covariate  + \n                                                               space_time ,\n                                                            prob = inv_logit(Intercept  + \n                                                               covariate  + \n                                                               space_time)))\n\n\np_ar1 = pred_space_ar1$prob %&gt;% ggplot() + geom_sf(aes(color = mean)) + \n  facet_wrap(.~time_idx) + scale_color_scico(direction = -1) +\n  theme_map\n\n\n\n\n\n\n\nModel Comparison\nNow we want to use the WAIC, DIC and MLIK to compare the models\n\n\n\n\n\n\nWarning Task\n\n\n\nCompare the scores, what is your conclusion?\n\n\n\nClick here to see the solution\n\nout= data.frame(Model = c(\"Model 1\", \"Model 2\"),\n  DIC = c(fit_spat$dic$dic, fit_spat_ar1$dic$dic),\n  WAIC = c(fit_spat$waic$waic, fit_spat_ar1$waic$waic),\n  MLIK = c(fit_spat$mlik[1], fit_spat_ar1$mlik[1]))"
  },
  {
    "objectID": "day4_practical_6.html#model-check-and-comparison",
    "href": "day4_practical_6.html#model-check-and-comparison",
    "title": "Practical 6",
    "section": "Model check and comparison",
    "text": "Model check and comparison"
  },
  {
    "objectID": "day4_practical_6.html#model-checking-for-linear-models",
    "href": "day4_practical_6.html#model-checking-for-linear-models",
    "title": "Practical 6",
    "section": "Model Checking for Linear Models",
    "text": "Model Checking for Linear Models\nIn this exercise we will:\n\nLearn about some model assessments techniques available in INLA\nConduct posterior predictive model checking\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nRecall a simple linear regression model with Gaussian observations\n\\[\ny_i\\sim\\mathcal{N}(\\mu_i, \\sigma^2), \\qquad i = 1,\\dots,N\n\\]\nwhere \\(\\sigma^2\\) is the observation error, and the mean parameter \\(\\mu_i\\) is linked to the linear predictor through an identity function:\n\\[\n\\eta_i = \\mu_i = \\beta_0 + \\beta_1 x_i\n\\] where \\(x_i\\) is a covariate and \\(\\beta_0, \\beta_1\\) are parameters to be estimated.\n\nSimulate example data\nWe simulate data from a simple linear regression model\n\n\nCode\nbeta = c(2,0.5)\nsd_error = 0.1\n\nn = 100\nx = rnorm(n)\ny = beta[1] + beta[2] * x + rnorm(n, sd = sd_error)\n\ndf = data.frame(y = y, x = x)  \n\n\n\n\nFitting the linear regression model with inlabru\nNow we fit a simple linear regression model in inalbru by defining (1) the model components, (2) the linear predictor and (3) the likelihood.\n\n# Model components\ncmp =  ~ -1 + beta_0(1) + beta_1(x, model = \"linear\")\n# Linear predictor\nformula = y ~ Intercept + beta_1\n# Observational model likelihood\nlik =  bru_obs(formula = y ~.,\n            family = \"gaussian\",\n            data = df)\n# Fit the Model\nfit.lm = bru(cmp, lik)\n\n\n\nResiduals analysis\nA common way for model diagnostics in regression analysis is by checking residual plots. In a Bayesian setting residuals can be defined in multiple ways depending on how you account for posterior uncertainty. Here, we will adopt a Bayesian approach by generating samples from the posterior distribution of the model parameters and then draw samples from the residuals defined as:\n\\[\nr_i = y_i - x_i^T\\beta\n\\]\nWe can use the predict function to achieve this:\n\nres_samples &lt;- predict(\n  fit.lm,         # the fitted model\n  df,             # the original data set\n  ~ data.frame(   \n    res = y-(beta_0 + beta_1)  # compute the residuals\n  ),\n  n.samples = 1000   # draw 1000 samples\n)\n\nThe resulting data frame contains the posterior draw of the residuals mean for which we can produce some diagnostics plots , e.g.\n\n\nResiduals checks for Linear Model\nggplot(res_samples,aes(y=mean,x=1:100))+geom_point() +\nggplot(res_samples,aes(y=mean,x=x))+geom_point()\n\n\n\n\n\nBayesian residual plots: the left panel is the residual index plot; the right panel is the plot of the residual versus the covariate x\n\n\n\n\nWe can also compare these against the theoretical quantiles of the Normal distribution as follows:\n\n\nQQPlot for Linear Model\narrange(res_samples, mean) %&gt;%\n  mutate(theortical_quantiles = qnorm(1:100 / (1+100))) %&gt;%\n  ggplot(aes(x=theortical_quantiles,y= mean)) + \n  geom_ribbon(aes(ymin = q0.025, ymax = q0.975), fill = \"grey70\")+\n  geom_abline(intercept = mean(res_samples$mean),\n              slope = sd(res_samples$mean)) +\n  geom_point() +\n  labs(x = \"Theoretical Quantiles (Normal)\",\n       y= \"Sample Quantiles (Residuals)\") \n\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive Checks\nNow, instead of generating samples from the mean, we will account for the observational process uncertainty by:\n\nSampling \\(y^{1k}_i\\sim\\pi(y_i|\\mathbf{y})\\) \\(k = 1,\\dots,M;~i = 1,\\ldots,100\\) using generate() (here we will draw \\(M=500\\) samples)\n\n\nsamples =  generate(fit.lm, df,\n  formula = ~ {\n    mu &lt;- (beta_0 + beta_1)\n    sd &lt;- sqrt(1 / Precision_for_the_Gaussian_observations)\n    rnorm(100, mean = mu, sd = sd)\n  },\n  n.samples = 500\n) \n\n\nComparing some summaries of the simulated data with the one of the observed one\n\nHere we compare (i) the estimated posterior densities \\(\\hat{\\pi}^k(y|\\mathbf{y})\\) with the estimated data density and (ii) the samples means and 95% credible intervals against the observations.\n\n# Tidy format for plotting\nsamples_long = data.frame(samples) %&gt;% \n  mutate(id = 1:100) %&gt;% # i-th observation\n  pivot_longer(-id)\n\n# compute the mean and quantiles for the samples\ndraws_summaries = data.frame(mean_samples = apply(samples,1,mean),\nq25 = apply(samples,1,function(x)quantile(x,0.025)),  \nq975 = apply(samples,1,function(x)quantile(x,0.975)),\nobservations = df$y)  \n\np1 = ggplot() + geom_density(data = samples_long, \n                        aes(value, group = name),  color = \"#E69F00\") +\n  geom_density(data = df, aes(y))  +\n  xlab(\"\") + ylab(\"\") \n\np2 = ggplot(draws_summaries,aes(y=mean_samples,x=observations))+\n  geom_errorbar(aes(ymin = q25,\n                   ymax = q975), \n               alpha = 0.5, color = \"grey50\")+\ngeom_point()+geom_abline(slope = 1,intercept = 0,lty=2)+labs()\n\np1 +p2"
  },
  {
    "objectID": "day4_practical_6.html#sec-linmodel",
    "href": "day4_practical_6.html#sec-linmodel",
    "title": "Practical 6",
    "section": "GLM model checking",
    "text": "GLM model checking\nIn this exercise we will:\n\nLearn about some model assessments techniques available in INLA\nConduct posterior predictive model checking using CPO and PIT\n\nLibraries to load:\n\nlibrary(dplyr)\nlibrary(INLA)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(inlabru)     \n\nIn this exercise, we will use data on horseshoe crabs (Limulus polyphemus) where the number of satellites males surrounding a breeding female are counted along with the female‚Äôs color and carapace width.\n Download data set \nA possible model to study the factors that affect the number of satellites for female crabs is\n\\[\n\\begin{aligned}\ny_i&\\sim\\mathrm{Poisson}(\\mu_i), \\qquad i = 1,\\dots,N \\\\\n\\eta_i &= \\mu_i = \\beta_0 + \\beta_1 x_i + \\ldots\n\\end{aligned}\n\\]\nWe can explore the conditional means and variances given the female‚Äôs color:\n\ncrabs &lt;- read.csv(\"datasets/crabs.csv\")\n\n# conditional means and variances\ncrabs %&gt;%\n  summarise( Mean = mean(satell ),\n             Variance = var(satell),\n                     .by = color)\n\n   color     Mean  Variance\n1 medium 3.294737 10.273908\n2   dark 2.227273  6.737844\n3  light 4.083333  9.719697\n4 darker 2.045455 13.093074\n\n\nThe mean of the number of satellites vary by color which gives a good indication that color might be useful for predicting satellites numbers. However, notice that the mean is lower than its variance suggesting that overdispersion might be present and that a negative binomial model would be more appropriate for the data (we will cover this later).\nFitting the model\nFirst, lets begin fitting the Poisson model above using the carapace‚Äôs color and width as predictors. Since, color is a categorical variable in our model we need to create a dummy variable for it. We can use the model.matrix function to help us constructing the design matrix and then append this to our data:\n\ncrabs_df = model.matrix( ~  color , crabs) %&gt;%\n  as.data.frame() %&gt;%\n  select(-1) %&gt;%        # drop intercept\n  bind_cols(crabs) %&gt;%  # append to original data\n  select(-color)        # remove original color categorical variable\n\nThe new data set crabs_df contains a dummy variable for the different color categories (dark being the reference category). Then we can fit the model in inlabru as follows:\n\ncmp =  ~ -1 + beta0(1) +  colordarker +\n       colorlight + colormedium +\n       w(weight, model = \"linear\")\n\nlik =  bru_obs(formula = satell ~.,\n            family = \"poisson\",\n            data = crabs_df)\n\nfit_pois = bru(cmp, lik)\n\nsummary(fit_pois)\n\ninlabru version: 2.13.0.9011 \nINLA version: 25.09.19 \nComponents: \nLatent components:\nbeta0: main = linear(1)\ncolordarker: main = linear(colordarker)\ncolorlight: main = linear(colorlight)\ncolormedium: main = linear(colormedium)\nw: main = linear(weight)\nObservation models: \n  Family: 'poisson'\n    Tag: &lt;No tag&gt;\n    Data class: 'data.frame'\n    Response class: 'integer'\n    Predictor: satell ~ .\n    Additive/Linear: TRUE/TRUE\n    Used components: effects[beta0, colordarker, colorlight, colormedium, w], latent[] \nTime used:\n    Pre = 0.871, Running = 0.202, Post = 0.00776, Total = 1.08 \nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant   mode kld\nbeta0       -0.501 0.196     -0.885   -0.501     -0.117 -0.501   0\ncolordarker -0.008 0.180     -0.362   -0.008      0.345 -0.008   0\ncolorlight   0.445 0.176      0.101    0.445      0.790  0.445   0\ncolormedium  0.248 0.118      0.017    0.248      0.479  0.248   0\nw            0.001 0.000      0.000    0.001      0.001  0.001   0\n\nDeviance Information Criterion (DIC) ...............: 917.12\nDeviance Information Criterion (DIC, saturated) ....: 561.74\nEffective number of parameters .....................: 5.01\n\nWatanabe-Akaike information criterion (WAIC) ...: 929.70\nEffective number of parameters .................: 16.51\n\nMarginal log-Likelihood:  -489.43 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\n\nModel assessment and model choice\nNow that we have fitted the model we would like to carry some model assessments. In a Bayesian setting, this is often based on posterior predictive checks. To do so, we will use the CPO and PIT - two commonly used Bayesian model assessment criteria based on the posterior predictive distribution.\n\n\n\n\n\n\nNotePosterior predictive model checking\n\n\n\nThe posterior predictive distribution for a predicted value \\(\\hat{y}\\) is\n\\[\n\\pi(\\hat{y}|\\mathbf{y}) = \\int_\\theta \\pi(\\hat{y}|\\theta)\\pi(\\theta|\\mathbf{y})d\\theta.\n\\]\nThe probability integral transform (PIT) introduced by Dawid (1984) is defined for each observation as:\n\\[\n\\mathrm{PIT}_i = \\pi(\\hat{y}_i \\leq y_i |\\mathbf{y}{-i})\n\\]\nThe PIT evaluates how well a model‚Äôs predicted values match the observed data distribution. It is computed as the cumulative distribution function (CDF) of the observed data evaluated at each predicted value. If the model is well-calibrated, the PIT values should be approximately uniformly distributed. Deviations from this uniform distribution may indicate issues with model calibration or overfitting.\nAnother metric we could used to asses the model fit is the conditional predictive ordinate (CPO) introduced by Pettit (1990), and deÔ¨Åned as:\n\\[\n\\text{CPO}_i = \\pi(y_i| \\mathbf{y}{-i})\n\\]\nThe CPO measures the density of the observed value of \\(y_i\\) when model is fit using all data but \\(y_i\\). CPO provides a measure of how well the model predicts each individual observation while taking into account the rest of the data and the model. Large values indicate a better fit of the model to the data, while small values indicate a bad fitting of the model\n\n\nTo compute PIT and CPO we can either:\n\nask inlabru to compute them by set options = list(control.compute = list(cpo = TRUE)) in the bru() function arguments.\nset this as default in inlabru global option using the bru_options_set function.\n\nHere we will do the later and re-run the model\n\nbru_options_set(control.compute = list(cpo = TRUE))\n\nfit_pois = bru(cmp, lik)\n\nNow we can produce histograms and QQ plots to assess for uniformity in the PIT values which can be accessed through inlabru_model$cpo$pit :\n\nPlotR Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfit_pois$cpo$pit %&gt;%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_pois$cpo$pit))),\n       fit_pois$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_pois$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n\n\n\n\nBoth Q-Q plots and histogram of the PIT values suggest a not so great model fit. For the CPO values, usually the following summary of the CPO is often used:\n\\[\n-\\sum_{i=1}^n \\log (\\text{CPO}\\_i)\n\\]\nThis quantities is useful when comparing different models - a smaller values indicate a better model fit. CPO values can be accessed by typing inlabru_model$cpo$cpo.\n\n\n\n\n\n\nWarning Task\n\n\n\nThe model assessment above suggests that a Poisson model might not be the most appropriate model, likely due to the overdispersion we detected previously. Fit a Negative binomial to relax the Poisson model assumption that the conditional mean and variance are equal. Then, compute the CPO summary statistic and PIT QQ plot to decide which model gives the better fit.\n\n\nTake hint\n\nTo specify a negative binomial model you only need to change the family distribution to family =  \"nbinomial\".\n\n\n\n\nClick here to see the solution\n\npar(mfrow=c(1,2))\n\n# Fit the negative binomial model\n\nlik_nbinom =  bru_obs(formula = satell ~.,\n            family = \"nbinomial\",\n            data = crabs_df)\n\nfit_nbinom = bru(cmp, lik_nbinom)\n\n# PIT checks\n\nfit_nbinom$cpo$pit %&gt;%\n  hist(main = \"Histogram of PIT values\")\n\nqqplot(qunif(ppoints(length(fit_nbinom$cpo$pit))),\n       fit_nbinom$cpo$pit,\n       main = \"Q-Q plot for Unif(0,1)\",\n       xlab = \"Theoretical Quantiles\",\n       ylab = \"Sample Quantiles\")\n\nqqline(fit_nbinom$cpo$pit,\n       distribution = function(p) qunif(p),\n       prob = c(0.1, 0.9))\n\n\n\n\n\n\n\n# CPO comparison\n\ndata.frame( CPO = c(-sum(log(fit_pois$cpo$cpo)),\n                    -sum(log(fit_nbinom$cpo$cpo))),\n          Model = c(\"Poisson\",\"Negative Binomial\"))\n\n       CPO             Model\n1 465.4061           Poisson\n2 379.3340 Negative Binomial\n\n# Overall, we can see that the negative binomial model provides a better fit to the data."
  },
  {
    "objectID": "day5_practical_8.html",
    "href": "day5_practical_8.html",
    "title": "Practical 8 - Distance Sampling",
    "section": "",
    "text": "Aim of this practical:\nIn this practical we are going to look at distance sampling models.\nDownload Practical 8 R script"
  },
  {
    "objectID": "day5_practical_8.html#the-data",
    "href": "day5_practical_8.html#the-data",
    "title": "Practical 8 - Distance Sampling",
    "section": "The data",
    "text": "The data\nIn the next exercise, we will explore data from a combination of several NOAA shipboard surveys conducted on pan-tropical spotted dolphins in the Gulf of Mexico. The data set is available in inlabru (originally obtained from the dsm R package) and contains the following information:\n\nA total of 47 observations of groups of dolphins were detected. The group size was recorded, as well as the Beaufort sea state at the time of the observation.\nTransect width is 16 km, i.e.¬†maximal detection distance 8 km (transect half-width 8 km).\n\nWe can load and visualize the data as follows:\n\nmexdolphin &lt;- mexdolphin_sf\n\nmexdolphin$depth &lt;- mexdolphin$depth %&gt;% mutate(depth=scale(depth)%&gt;%c())\n\nggplot() + geom_sf(data = mexdolphin$points, color = \"red\" ) +\n\n  geom_sf(data = mexdolphin$samplers) +\n\n  geom_sf(data = mexdolphin$ppoly, alpha = 0)"
  },
  {
    "objectID": "day5_practical_8.html#the-workflow",
    "href": "day5_practical_8.html#the-workflow",
    "title": "Practical 8 - Distance Sampling",
    "section": "The workflow",
    "text": "The workflow\nTo model the density of spotted dolphins we take a thinned point process model of the form:\n\n\n\nWhen fitting a distance sampling model we need to fulfill the following tasks:\n\nBuild the mesh\nDefine the SPDE representation of the spatial GF. This includes defining the priors for the range and sd of the spatial GF\nDefine the components of the linear predictor. This includes the spatial GF and all eventual covariates\nDefine the observation model using the bru_obs() function\nRun the model using the bru() function\n\n\nBuilding the mesh\nThe first task is to build the mesh that covers the area of interest. For this purpose we use the function fm_mesh_2d. To do so, we need to define the area of interest. We can either use a predefined boundary or create a non convex hull surrounding the location of the specie sightseeings\n\nnon-covex hulldomain boundary\n\n\n\nboundary0 = fm_nonconvex_hull(mexdolphin$points,convex = -0.1)\nmesh_0 = fm_mesh_2d(boundary = boundary0,\n                          max.edge = c(30, 150), # The largest allowed triangle edge length.\n                          cutoff = 15,\n                          crs = fm_crs(mexdolphin$points))\nggplot() + gg(mesh_0)\n\n\n\n\n\n\n\n\n\n\nThe mexdolphin object contains a predefined region of interest which can be accessed through mexdolphin$ppoly\n\nmesh_1 = fm_mesh_2d(boundary = mexdolphin$ppoly,\n                    max.edge = c(30, 150),\n                    cutoff = 15,\n                    crs = fm_crs(mexdolphin$points))\n\nggplot() + gg(mesh_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning Task\n\n\n\nLook at the documentation for the fm_mesh_2d function typing\n\n?fm_mesh_2d\n\nplay around with the different options and create different meshes. You can compare these against a pre-computed mesh available by typing plot(mexdolphin$mesh)\nThe rule of thumb is that your mesh should be:\n\nfine enough to well represent the spatial variability of your process, but not too fine in order to avoid computation burden\nthe triangles should be regular, avoid long and thin triangles.\nThe mesh should contain a buffer around your area of interest (this is what is defined in the offset option) in order to avoid boundary artefact in the estimated variance.\n\n\n\n\n\nDefine the SPDE representation of the spatial GF\nTo define the SPDE representation of the spatial GF we use the function inla.spde2.pcmatern. This takes as input the mesh we have defined and the PC-priors definition for \\(\\rho\\) and \\(\\sigma\\) (the range and the marginal standard deviation of the field).\nPC priors Gaussian Random field are defined in (Fuglstad et al.¬†2018). From a practical perspective for the range \\(\\rho\\) you need to define two paramters \\(\\rho_0\\) and \\(p_{\\rho}\\) such that you believe it is reasonable that\n\\[\nP(\\rho&lt;\\rho_0)=p_{\\rho}\n\\]\nwhile for the marginal variance \\(\\sigma\\) you need to define two parameters \\(\\sigma_0\\) and \\(p_{\\sigma}\\) such that you believe it is reasonable that\n\\[\nP(\\sigma&gt;\\sigma_0)=p_{\\sigma}\n\\]\n\n\n\n\n\n\nTip Question\n\n\n\nTake a look at the code below and select which of the following statements about the specified Matern PC priors are true.\n\nspde_model &lt;- inla.spde2.pcmatern(mexdolphin$mesh,\n  prior.sigma = c(2, 0.01),\n  prior.range = c(50, 0.01)\n)\n\n\n there is probability of 0.01 that the spatial range is greater or equal than 50 the probability that the spatial range is smaller than 50 is very small the probability that the marginal standard deviation is smaller than 2 is very small there is probability of 0.99 that the marginal standard deviation is less or equal than 2\n\n\n\n\n\nDefine the components of the linear predictor\nWe have now defined a mesh and a SPDE representation of the spatial GF. We now need to define the model components.\nFirst, we need to define the detection function. Here, we will define a half-normal detection probability function. This must take distance as its first argument and the linear predictor of the sigma parameter as its second:\n\nhn &lt;- function(distance, sigma) {\n  exp(-0.5 * (distance / sigma)^2)\n\n}\n\nWe need to now separately define the components of the model including the SPDE model, the Intercept, the effect of depth and the detection function parameter sigma.\n\ncmp &lt;- ~ space(main = geometry, model = spde_model) +\n  sigma(1,\n    prec.linear = 1,\n    marginal = bm_marginal(qexp, pexp, dexp, rate = 1 / 8)\n  ) +\n  Intercept(1)\n\n\n\n\n\n\n\nNote\n\n\n\nTo control the prior distribution for the sigma parameter, we use a transformation mapper that converts a latent variable into an exponentially distributed variable with expectation 8 (this is a somewhat arbitrary value, but motivated by the maximum observation distance W)\nThe marginal argument in the sigma component specifies the transformation function taking N(0,1) to Exponential(1/8).\n\n\nThe formula, which describes how these components are combined to form the linear predictor\n\\[\n\\log \\color{red}{\\tilde{\\lambda}(s)} = \\overbrace{\\log \\lambda (s)}^{\\beta_0 + \\xi(s)} + \\overbrace{\\log \\color{red}{g(d(s))}}^{-0.5~d(\\mathbf{s})^2\\sigma^{-2}}\n\\]\n\n\n\n\n\n\nWarning Task\n\n\n\nComplete the code below to define the formula\n\neta &lt;- ... + log(2)\n\n\n\n\nClick here to see the solution\n\n\nCode\neta &lt;- geometry + distance ~ space +\n\n  log(hn(distance, sigma)) +\n\n  Intercept + log(2)\n\n\n\n\n\nHere, the log(2) offset in the predictor takes care of the two-sided detections\n\n\nDefine the observation model\ninlabru has support for latent Gaussian Cox processes through the cp likelihood family. To fit a point process model recall that we need to approximate the integral in using a numerical integration scheme as:\n\\[\n\\approx\\exp\\left(-\\sum_{k=1}^{N_k}w_k\\lambda(s_k)\\right)\\prod_{i=1}^n \\lambda(\\mathbf{s}_i)\n\\]\nThus, we first create our integration scheme using the fm_int function by specifying integration domains for the spatial and distance dimensions.\nHere we use the same points to define the SPDE approximation and to approximate the integral in ?@eq-thinned_pp, so that the integration weight and SPDE weights are consistent with each other. We also need to explicitly integrate over the distance dimension so we use the fm_mesh_1d() to create mesh over the samplers (which are the transect lines in this dataset, so we need to tell inlabru about the strip half-width).\n\n# build integration scheme\n\ndistance_domain &lt;-  fm_mesh_1d(seq(0, 8,\n                              length.out = 30))\n\nips = fm_int(list(geometry = mexdolphin$mesh,\n                  distance = distance_domain),\n             samplers = mexdolphin$samplers)\n\nNow, we just need to supply the sf object as our data and the integration scheme ips:\n\nlik = bru_obs(\"cp\",\n              formula = eta,\n              data = mexdolphin$points,\n              ips = ips)\n\nThen we fit the model, passing both the components and the observational model\n\nfit = bru(cmp, lik)\n\n\n\n\n\n\n\nNote\n\n\n\ninlabru supports a shortcut for defining the integration points using the domain and samplers argument of bru_obs(). This domain argument expects a list of named domains with inputs that are then internally passed to fm_int() to build the integration scheme. The samplers argument is used to define subsets of the domain over which the integral should be computed. An equivalent way to define the same model as above is:\n\nlik = bru_obs(formula = eta,\n              data = mexdolphin$points,\n              family = \"cp\",\n              domain = list(\n                geometry = mesh,\n                distance = fm_mesh_1d(seq(0, 8, length.out = 30))),\n              samplers = mexdolphin$samplers)"
  },
  {
    "objectID": "day5_practical_8.html#visualize-model-results",
    "href": "day5_practical_8.html#visualize-model-results",
    "title": "Practical 8 - Distance Sampling",
    "section": "Visualize model Results",
    "text": "Visualize model Results\n\nPosterior summaries\nWe can use the fit$summary.fixed and summary.hyperpar to obtain posterior summaries of the model parameters.\n\n\n                  mean 0.025quant 0.975quant\nsigma            -0.05      -0.46       0.36\nIntercept        -8.16      -9.29      -7.34\nRange for space 295.48     110.54     673.68\nStdev for space   0.81       0.42       1.39\n\n\nLook at the SPDE parameter posteriors as follows:\n\nplot( spde.posterior(fit, \"space\", what = \"range\")) +\nplot( spde.posterior(fit, \"space\", what = \"log.variance\"))\n\n\n\n\n\n\n\n\n\n\nModel predictions\nWe now want to extract the estimated posterior mean and sd of spatial GF.\n\n\n\n\n\n\nWarning Task\n\n\n\nDefine a prediction grid using function fm_pixel(). Then compute the prediction for both the spatial GF and the linear predictor (spatial GF + intercept)\n\n\n\nClick here to see the solution\n\n\nCode\npxl &lt;- fm_pixels(mexdolphin$mesh, dims = c(200, 100), mask = mexdolphin$ppoly)\n\npr.int = predict(fit, pxl, ~data.frame(spatial = space,\n\n                                      lambda = exp(Intercept + space)))\n\n\n\n\n\nFinally, we can plot the maps of the spatial effect\n\nggplot() + geom_sf(data = pr.int$spatial,aes(color = mean)) + scale_color_scico() + ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\nggplot() + geom_sf(data = pr.int$spatial,aes(color = sd)) + scale_color_scico() + ggtitle(\"Posterior sd\")\n\n\n\n\n\n\n\n\nNote The posterior sd is lowest at the observation points. Note how the posterior sd is inflated around the border, this is the ‚Äúborder effect‚Äù due to the SPDE representation.\n\n\n\n\n\n\nWarning Task\n\n\n\nUsing the predictions stored in pr.int, produce a map of the posterior mean intensity.\n\n\nTake hint\n\nRecall that the predicted intensity is given by \\(\\lambda(s) = \\exp(\\beta_0+\\xi(s))\\)\n\n\n\n\nClick here to see the solution\n\n\nCode\nggplot() +\n\n  geom_sf(data = pr.int$lambda,aes(color = mean)) +\n\n  scale_color_scico(palette = \"imola\") +\n\n  ggtitle(\"Posterior mean\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe can predict the detection function in a similar fashion.Here, we should make sure that it doesn‚Äôt try to evaluate the effects of components that can‚Äôt be evaluated using the given input data.\n\ndistdf &lt;- data.frame(distance = seq(0, 8, length.out = 100))\n\ndfun &lt;- predict(fit, distdf, ~ hn(distance, sigma))\n\nplot(dfun)"
  },
  {
    "objectID": "day5_practical_8.html#abundance-estimates",
    "href": "day5_practical_8.html#abundance-estimates",
    "title": "Practical 8 - Distance Sampling",
    "section": "Abundance estimates",
    "text": "Abundance estimates\nThe mean expected number of animals can be computed by integrating the intensity over the region of interest as follows:\n\npredpts &lt;- fm_int(mexdolphin$mesh, mexdolphin$ppoly)\nLambda &lt;- predict(fit, predpts, ~ sum(weight * exp(space + Intercept)))\nLambda\n\n     mean       sd   q0.025     q0.5   q0.975   median sd.mc_std_err\n1 243.885 51.92043 169.0426 235.5735 366.4596 235.5735      3.798775\n  mean.mc_std_err\n1        5.951798\n\n\nTo fully propagate the uncertainty on the expected number animals we can draw Monte Carlo samples from the fitted model as follows (this could take a couple of minutes):\n\nNs &lt;- seq(50, 450, by = 1)\n\nNest &lt;- predict(fit, predpts,\n  ~ data.frame(\n    N = Ns,\n    density = dpois(\n      Ns,\n      lambda = sum(weight * exp(space + Intercept))\n    )\n  ),\n  n.samples = 2000\n)\n\nWe can compare this with a simpler ‚Äúplug-in‚Äù approximation:\n\nNest &lt;- dplyr::bind_rows(\n  cbind(Nest, Method = \"Posterior\"),\n  data.frame(\n    N = Nest$N,\n    mean = dpois(Nest$N, lambda = Lambda$mean),\n    mean.mc_std_err = 0,\n    Method = \"Plugin\"\n  )\n)\n\nThen, we can visualize the result as follows:\n\nggplot(data = Nest) +\n  geom_line(aes(x = N, y = mean, colour = Method)) +\n  geom_ribbon(\n    aes(\n      x = N,\n      ymin = mean - 2 * mean.mc_std_err,\n      ymax = mean + 2 * mean.mc_std_err,\n      fill = Method,\n    ),\n    alpha = 0.2\n  ) +\n  geom_line(aes(x = N, y = mean, colour = Method)) +\n  ylab(\"Probability mass function\")"
  },
  {
    "objectID": "slides/slides_2.html#outline",
    "href": "slides/slides_2.html#outline",
    "title": "Lecture 1",
    "section": "Outline",
    "text": "Outline\n\nWhat are INLA and inlabru?\nWhy the Bayesian framework?\nWhich model are inlabru-friendly?\nWhat are Latent Gaussian Models?\nHow are they implemented in inlabru?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#what-is-inla-what-is-inlabru",
    "href": "slides/slides_2.html#what-is-inla-what-is-inlabru",
    "title": "Lecture 1",
    "section": "What is INLA? What is inlabru?",
    "text": "What is INLA? What is inlabru?\nThe short answer:\n\nINLA is a fast method to do Bayesian inference with latent Gaussian models and inlabru is an R-package that implements this method with a flexible and simple interface.\n\nThe (much) longer answer:\n\nRue, H., Martino, S. and Chopin, N. (2009), Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71: 319-392.\nVan Niekerk, J., Krainski, E., Rustand, D., & Rue, H. (2023). A new avenue for Bayesian inference with INLA. Computational Statistics & Data Analysis, 181, 107692.\nLindgren, F., Bachl, F., Illian, J., Suen, M. H., Rue, H., & Seaton, A. E. (2024). inlabru: software for fitting latent Gaussian models with non-linear predictors. arXiv preprint arXiv:2407.00791.\nLindgren, F., Bolin, D., & Rue, H. (2022). The SPDE approach for Gaussian and non-Gaussian fields: 10 years and still running. Spatial Statistics, 50, 100599.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#where",
    "href": "slides/slides_2.html#where",
    "title": "Lecture 1",
    "section": "Where?",
    "text": "Where?\n\n\n\n Website-tutorials\n\n\ninlabru https://inlabru-org.github.io/inlabru/\nR-INLA https://www.r-inla.org/home\n\n\n\n\n\n\n Discussion forums\n\n\ninlabru https://github.com/inlabru-org/inlabru/discussions\nR-INLA https://groups.google.com/g/r-inla-discussion-group\n\n\n\n\n\n\n Books\n\n\n\nBlangiardo, M., & Cameletti, M. (2015). Spatial and spatio-temporal Bayesian models with R-INLA. John Wiley & Sons.\nG√≥mez-Rubio, V. (2020). Bayesian inference with INLA. Chapman and Hall/CRC.\nKrainski, E., G√≥mez-Rubio, V., Bakka, H., Lenzi, A., Castro-Camilo, D., Simpson, D., ‚Ä¶ & Rue, H. (2018). Advanced spatial modeling with stochastic partial differential equations using R and INLA. Chapman and Hall/CRC.\nWang, X., Yue, Y. R., & Faraway, J. J. (2018). Bayesian regression modeling with INLA. Chapman and Hall/CRC.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#so-why-should-you-use-inlabru",
    "href": "slides/slides_2.html#so-why-should-you-use-inlabru",
    "title": "Lecture 1",
    "section": "So‚Ä¶ Why should you use inlabru?",
    "text": "So‚Ä¶ Why should you use inlabru?\n\nWhat type of problems can we solve?\nWhat type of models can we use?\nWhen can we use inlabru?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#so-why-should-you-use-inlabru-1",
    "href": "slides/slides_2.html#so-why-should-you-use-inlabru-1",
    "title": "Lecture 1",
    "section": "So‚Ä¶ Why should you use inlabru?",
    "text": "So‚Ä¶ Why should you use inlabru?\n\nWhat type of problems can we solve?\nWhat type of models can we use?\nWhen can we use inlabru?\n\n\nTo give proper answers to these questions, we need to start at the very beginning‚Ä¶",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-core",
    "href": "slides/slides_2.html#the-core",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-core-1",
    "href": "slides/slides_2.html#the-core-1",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something.\nWe have questions.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-core-2",
    "href": "slides/slides_2.html#the-core-2",
    "title": "Lecture 1",
    "section": "The core",
    "text": "The core\n\nWe have observed something.\nWe have questions.\nWe want answers!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#how-do-we-find-answers",
    "href": "slides/slides_2.html#how-do-we-find-answers",
    "title": "Lecture 1",
    "section": "How do we find answers?",
    "text": "How do we find answers?\nWe need to make choices:\n\n\n\nBayesian or frequentist?\nHow do we model the data?\nHow do we compute the answer?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#how-do-we-find-answers-1",
    "href": "slides/slides_2.html#how-do-we-find-answers-1",
    "title": "Lecture 1",
    "section": "How do we find answers?",
    "text": "How do we find answers?\nWe need to make choices:\n\nBayesian or frequentist?\nHow do we model the data?\nHow do we compute the answer?\n\nThese questions are not independent.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist",
    "href": "slides/slides_2.html#bayesian-or-frequentist",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no ‚Äútrue but unknown‚Äù parameters !",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist-1",
    "href": "slides/slides_2.html#bayesian-or-frequentist-1",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no ‚Äútrue but unknown‚Äù parameters!\nEvery parameter is described by a probability distribution!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#bayesian-or-frequentist-2",
    "href": "slides/slides_2.html#bayesian-or-frequentist-2",
    "title": "Lecture 1",
    "section": "Bayesian or frequentist?",
    "text": "Bayesian or frequentist?\nIn this course we embrace the Bayesian perspective\n\nThere are no ‚Äútrue but unknown‚Äù parameters!\nEvery parameter is described by a probability distribution!\nEvidence from the data is used to update the belief we had before observing the data!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-i",
    "href": "slides/slides_2.html#some-more-details-i",
    "title": "Lecture 1",
    "section": "Some more details I",
    "text": "Some more details I\nWe define as the linear predictor the mean (or a function of the mean) of our observations given the model components.\n\nIn this case \\(E(y_i|\\beta_0, \\beta_i) = \\eta_i = \\beta_0 + \\beta_1 x_i\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-i-1",
    "href": "slides/slides_2.html#some-more-details-i-1",
    "title": "Lecture 1",
    "section": "Some more details I",
    "text": "Some more details I\nWe define as the linear predictor the mean (or a function of the mean) of our observations given the model components.\n\nIn this case \\(E(y_i|\\beta_0, \\beta_i) =\\eta_i =  \\color{red}{\\boxed{\\beta_0}} +  \\color{red}{\\boxed{\\beta_1 x_i}}\\)\nThis model has two components!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-ii",
    "href": "slides/slides_2.html#some-more-details-ii",
    "title": "Lecture 1",
    "section": "Some more details II",
    "text": "Some more details II\nGiven the linear predictor \\(\\eta\\) the observations are independent of each other!\n\nThis means that all dependencies in the observations are accounted for by the components!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#some-more-details-ii-1",
    "href": "slides/slides_2.html#some-more-details-ii-1",
    "title": "Lecture 1",
    "section": "Some more details II",
    "text": "Some more details II\nGiven the linear predictor \\(\\eta\\) the observations are independent of each other!\n\nThe observation model (likelihood) can be written as:\n\\[\n\\pi(\\mathbf{y}|\\eta,\\sigma^2) = \\prod_{i = 1}^n\\pi(y_i|\\eta_i,\\sigma^2)\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit",
    "title": "Lecture 1",
    "section": "Let‚Äôs formalize this a bit‚Ä¶",
    "text": "Let‚Äôs formalize this a bit‚Ä¶\nThe elements of an inlabru-friendly statistical model are:\n\nThe observational model\n\n\\[\n    \\begin{aligned}\n    y_i|\\eta_i, \\sigma^2 & \\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\\\\n    E(y_i|\\eta_i, \\sigma^2) & = \\eta_i\n    \\end{aligned}\n\\]\nNote: We assume that, given the linear predictor \\(\\eta\\), the data are independent of each other!\nData dependence is expressed through the components of the linear predictor.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-1",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-1",
    "title": "Lecture 1",
    "section": "Let‚Äôs formalize this a bit‚Ä¶",
    "text": "Let‚Äôs formalize this a bit‚Ä¶\nThe elements of an inlabru-friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\[\nE(y_i|\\eta_i,\\sigma^2) = \\eta_i = \\beta_0 + \\beta_1x_i\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-2",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-2",
    "title": "Lecture 1",
    "section": "Let‚Äôs formalize this a bit‚Ä¶",
    "text": "Let‚Äôs formalize this a bit‚Ä¶\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor\n\n\\[\nE(y_i|\\eta_i,\\sigma^2) = \\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\n\\]\nNote 1: These are the components of our model! These explain the dependence structure of the data.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-3",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-3",
    "title": "Lecture 1",
    "section": "Let‚Äôs formalize this a bit‚Ä¶",
    "text": "Let‚Äôs formalize this a bit‚Ä¶\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\(\\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\\)\nA prior for the model components \\(\\textbf{u}\\) \\[\n\\mathbf{u} = \\{\\beta_0, \\beta_1\\}\\sim\\mathcal{N}(0,\\mathbf{Q}^{-1})\n\\]\n\nNote: These always have a Gaussian prior and are used to explain the dependencies among observations!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#lets-formalize-this-a-bit-4",
    "href": "slides/slides_2.html#lets-formalize-this-a-bit-4",
    "title": "Lecture 1",
    "section": "Let‚Äôs formalize this a bit‚Ä¶",
    "text": "Let‚Äôs formalize this a bit‚Ä¶\nThe elements of a inlabru friendly statistical model are:\n\nThe observational model \\(y_i|\\eta_i,\\sigma^2\\sim\\mathcal{N}(\\eta_i,\\sigma^2),\\qquad i = 1,\\dots,n\\)\nA model for the linear predictor \\(\\eta_i = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1x_i} }\\)\nA prior for the model components \\(\\mathbf{u} = \\{\\beta_0, \\beta_1\\}\\sim\\mathcal{N}(0,\\mathbf{Q}^{-1})\\)\nA prior for the non-Gaussian parameters \\(\\theta\\) \\[\n\\theta = \\sigma^2\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-1",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-1",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-2",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-2",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure\nStage 3 The hyperparameters",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#latent-gaussian-models-lgm-3",
    "href": "slides/slides_2.html#latent-gaussian-models-lgm-3",
    "title": "Lecture 1",
    "section": "Latent Gaussian Models (LGM)",
    "text": "Latent Gaussian Models (LGM)\n\n\n\n\nThe observation model: \\[\n\\pi(\\mathbf{y}|\\eta,\\theta) = \\prod_{i=1}^{n}\\pi(y_i|\\eta_i,\\theta)\n\\]\nLinear predictor \\(\\eta_i = \\beta_0 + \\beta_1 x_i\\)\nLatent Gaussian field \\(\\pi(\\mathbf{u}|\\theta)\\)\nThe hyperparameters: \\(\\pi(\\theta)\\)\n\n\n\n\n\nStage 1 The data generating process\nStage 2 The dependence structure\nStage 3 The hyperparameters\n\n\n\n\nQ: What are we interested in?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-posterior-distribution",
    "href": "slides/slides_2.html#the-posterior-distribution",
    "title": "Lecture 1",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\n\n\nposterior\n\n\n\nA\n\nPrior\n belief\n\n\n\nC\n\nBayes Theorem\n &\n Bayesian Computations\n\n\n\nA-&gt;C\n\n\n\n\n\nB\n\nObservation\n model\n\n\n\nB-&gt;C\n\n\n\n\n\nD\n\nPosterior\n distribution\n\n\n\nC-&gt;D\n\n\n\n\n\n\n\n\n\n\n\\[\n\\color{red}{\\pi(\\mathbf{u},\\theta|\\mathbf{y})}\\propto \\color{blue}{\\pi(\\mathbf{y}|\\mathbf{u},\\theta)}\\color{green}{\\pi(\\mathbf{u}|\\theta)\\pi(\\theta)}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-posterior-distribution-1",
    "href": "slides/slides_2.html#the-posterior-distribution-1",
    "title": "Lecture 1",
    "section": "The posterior distribution",
    "text": "The posterior distribution\n\n\n\n\n\n\n\nposterior\n\n\n\nC\n\nBayes Theorem\n &\n Bayesian Computations\n\n\n\nD\n\nPosterior\n distribution\n\n\n\nC-&gt;D\n\n\n\n\n\nE\n\nBayesian Computation are hard!!\n Here is where\n INLA\n comes in!!!\n\n\n\nE-&gt;C\n\n\n\n\n\nA\n\nPrior\n belief\n\n\n\nA-&gt;C\n\n\n\n\n\nB\n\nObservation\n model\n\n\n\nB-&gt;C",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression",
    "href": "slides/slides_2.html#inlabru-for-linear-regression",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model \\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-1",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-1",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_i x_i}}\n\\end{aligned}\n\\]\n\n\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-2",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-2",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\color{red}{\\boxed{\\beta_0 + \\beta_i x_i}}\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-3",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-3",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_i|\\eta_i, \\sigma^2}} & \\color{red}{\\boxed{\\sim \\mathcal{N}(\\eta_i,\\sigma^2)}}\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-4",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-4",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\beta_0 + \\beta_i x_i\n\\end{aligned}\n\\]\n\n\nlibrary(inlabru)\nPallid[1:3,c(\"w\",\"tl\")]\n\n      w    tl\n1 2.239  95.9\n2 2.948  95.0\n3 3.402 108.0\n\n\n\nThe code\n\n# define model components\ncmp =  ~ -1 + beta0(1) + beta1(tl, model = \"linear\")\n\n# define model predictor\neta = w ~ beta0 + beta1\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"gaussian\",\n              data = Pallid)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-linear-regression-5",
    "href": "slides/slides_2.html#inlabru-for-linear-regression-5",
    "title": "Lecture 1",
    "section": "inlabru for linear regression",
    "text": "inlabru for linear regression",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#real-datasets-are-more-complicated",
    "href": "slides/slides_2.html#real-datasets-are-more-complicated",
    "title": "Lecture 1",
    "section": "Real datasets are more complicated!",
    "text": "Real datasets are more complicated!\nData can have several dependence structures: temporal, spatial,‚Ä¶\nUsing a Bayesian framework:\n\nBuild (hierarchical) models to account for potentially complicated dependency structures in the data.\nAttribute uncertainty to model parameters and latent variables using priors.\n\nTwo main challenges:\n\nNeed computationally efficient methods to calculate posteriors (this is where INLA helps!).\nSelect priors in a sensible way (we‚Äôll talk about this)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-good-news",
    "href": "slides/slides_2.html#the-good-news",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! üòÉ\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-1",
    "href": "slides/slides_2.html#the-good-news-1",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! üòÉ\n\nStage 1: What is the distribution of the responses?\n\nGaussian response? (temperature, rainfall, fish weight ‚Ä¶)\nCount data? (people infected with a disease in each area)\nPoint pattern? (locations of trees in a forest)\nBinary data? (yes/no response, binary image)\nSurvival data? (recovery time, time to death)\n‚Ä¶ (many more examples!!)\n\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-2",
    "href": "slides/slides_2.html#the-good-news-2",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! üòÉ\n\nStage 1: What is the distribution of the responses?\n\nWe assume data to be conditionally independent given the model components and some hyperparameters\nThis means that all dependencies in data are explained in Stage\n\n\n\n\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-3",
    "href": "slides/slides_2.html#the-good-news-3",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! üòÉ\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\n\nHere we can have:\n\nFixed effects for covariates\nUnstructured random effects (individual effects, group effects)\nStructured random effects (AR(1), regional effects, )\n‚Ä¶\n\nThese are linked to the responses in the likelihood through linear predictors.\n\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-good-news-4",
    "href": "slides/slides_2.html#the-good-news-4",
    "title": "Lecture 1",
    "section": "The good news!!",
    "text": "The good news!!\nIn many cases complicated spatio-temporal models are just special cases of the same model structure!! üòÉ\n\nStage 1: What is the distribution of the responses?\nStage 2: What are the model components? and what is their distribution?\nStage 3: What are our prior beliefs about the parameters controlling the components in the model?\nThe likelihood and the latent model typically have hyperparameters that control their behavior.\nThey can include:\n\nVariance of observation noise\nDispersion parameter in the negative binomial model\nVariance of unstructured effects\n‚Ä¶",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-second-good-news",
    "href": "slides/slides_2.html#the-second-good-news",
    "title": "Lecture 1",
    "section": "The second good news!",
    "text": "The second good news!\nNo matter how complicated your model is, the inlabru workflow is always the same üòÉ\n\n# Define model components\ncomps &lt;- component_1(...) +\n  component_2(...) + ...\n\n# Define the model predictor\npred &lt;- linear_function(component_1,\n                            component_2, ...)\n\n# Build the observation model\nlik &lt;- bru_obs(formula = pred,\n               family = ... ,\n               data = ... ,\n                ...)\n\n# Fit the model\nfit &lt;- bru(comps, lik, ...)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-second-good-news-1",
    "href": "slides/slides_2.html#the-second-good-news-1",
    "title": "Lecture 1",
    "section": "The second good news!",
    "text": "The second good news!\nNo matter how complicated your model is, the inlabru workflow is always the same üòÉ\n\n# Define model components\ncomps &lt;- component_1(...) +\n  component_2(...) + ...\n\n# Define the model predictor\npred &lt;- linear_function(component_1,\n                            component_2, ...)\n\n# Build the observation model\nlik &lt;- bru_obs(formula = pred,\n               family = ... ,\n               data = ... ,\n                ...)\n\n# Fit the model\nfit &lt;- bru(comps, lik, ...)\n\nNOTE we will see later that this function can also be non-linear‚Ä¶.üòÅ",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-tokyo-rainfall-data",
    "href": "slides/slides_2.html#the-tokyo-rainfall-data",
    "title": "Lecture 1",
    "section": "The Tokyo rainfall data",
    "text": "The Tokyo rainfall data\nOne example with time series: Rainfall over 1 mm in the Tokyo area for each calendar day during two years (1983-84)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-model",
    "href": "slides/slides_2.html#the-model",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\]\n\\[\nn_t = \\left\\{\n\\begin{array}{lr}\n1, & \\text{for}\\; 29\\; \\text{February}\\\\\n2, & \\text{other days}\n\\end{array}\\right.\n\\]\n\\[\ny_t =\n\\begin{cases}\n\\{0,1\\}, & \\text{for}\\; 29\\; \\text{February}\\\\\n\\{0,1,2\\}, & \\text{other days}\n\\end{cases}\n\\]\n\nthe likelihood has no hyperparameters",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-model-1",
    "href": "slides/slides_2.html#the-model-1",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\]\nStage 2 The latent field\n\\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\n\nprobability of rain depends on the day of the year \\(t\\)\n\\(\\beta_0\\) is an intercept\n\\(f(\\text{time}_t)\\) is a RW2 model (this is just a smoother). The smoothness is controlled by a hyperparameter \\(\\tau_f\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#the-model-2",
    "href": "slides/slides_2.html#the-model-2",
    "title": "Lecture 1",
    "section": "The model",
    "text": "The model\nStage 1 The observation model\n\\[\ny_t|\\eta_t\\sim\\text{Bin}(n_t, p_t),\\qquad \\eta_t = \\text{logit}(p_t),\\qquad i = 1,\\dots,366\n\\]\nStage 2 The latent field\n\\[\n\\eta_t = \\beta_0 + f(\\text{time}_t)\n\\]\nStage 3 The hyperparameters\n\nThe structured time effect is controlled by one parameter \\(\\tau_f\\).\nWe assign a prior to \\(\\tau_f\\) to finalize the model.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series",
    "href": "slides/slides_2.html#inlabru-for-time-series",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{f(\\text{time}_t)}}\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-1",
    "href": "slides/slides_2.html#inlabru-for-time-series-1",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\color{red}{\\boxed{\\eta_i}} & = \\color{red}{\\boxed{\\beta_0 + f(\\text{time}_t)}}\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-2",
    "href": "slides/slides_2.html#inlabru-for-time-series-2",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_t|\\eta_t}} & \\color{red}{\\boxed{\\sim \\text{Binomial}(n_t,p_t)}}\\\\\n\\text{logit}(p_t) = \\eta_i & = \\beta_0 + f(\\text{time}_t)\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-time-series-3",
    "href": "slides/slides_2.html#inlabru-for-time-series-3",
    "title": "Lecture 1",
    "section": "inlabru for time series",
    "text": "inlabru for time series\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Binomial}(n_t,p_t)\\\\\n\\text{logit}(p_t) = \\eta_i & = \\beta_0 + f(\\text{time}_t)\n\\end{aligned}\n\\]\n\n\nTokyo[1:3,]\n\n  y n time\n1 0 2    1\n2 0 2    2\n3 1 2    3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + time_effect(time, model = \"rw2\", cyclic = TRUE)\n\n# define model predictor\neta = y ~ beta0 + time_effect\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"binomial\",\n              Ntrials = n,\n              data = Tokyo)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#example-disease-mapping",
    "href": "slides/slides_2.html#example-disease-mapping",
    "title": "Lecture 1",
    "section": "Example: disease mapping",
    "text": "Example: disease mapping\nWe observed larynx cancer mortality counts for males in 544 district of Germany from 1986 to 1990 and want to make a model.\n\n\n\n\\(y_i\\): The count at location \\(i\\).\n\\(E_i\\): An offset; expected number of cases in district \\(i\\).\n\\(c_i\\): A covariate (level of smoking consumption) at \\(i\\)\n\\(\\boldsymbol{s}_i\\): spatial location \\(i\\) .",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#bayesian-disease-mapping",
    "href": "slides/slides_2.html#bayesian-disease-mapping",
    "title": "Lecture 1",
    "section": "Bayesian disease mapping",
    "text": "Bayesian disease mapping\n\nStage 1: We assume the responses are Poisson distributed:\n\n\\[\n    y_i \\mid \\eta_i \\sim \\text{Poisson}(E_i\\exp(\\eta_i)))\n\\]\n\nStage 2: \\(\\eta_i\\) is a linear function of three components: an intercept, a covariate \\(c_i\\), a spatially structured effect \\(\\omega\\) likelihood by\n\n\\[\n    \\eta_i = \\beta_0 + \\beta_1\\ c_i + \\omega_i\n\\]\n\nStage 3:\n\n\\(\\tau_{\\omega}\\): Precisions parameter for the random effects\n\n\n\nThe latent field is \\(\\boldsymbol{u} = (\\beta_0, \\beta_1, \\omega_1, \\omega_2,\\ldots, \\omega_n)\\), the hyperparameters are \\(\\boldsymbol{\\theta} = (\\tau_{\\omega})\\), and must be given a prior.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{\\beta_1\\ c_i}} + \\color{red}{\\boxed{\\omega_i}}\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"besag\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-1",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-1",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_t & \\sim \\text{Poisson}(E_i\\lambda_i)\\\\\n\\text{log}(\\lambda_i) = \\color{red}{\\boxed{\\eta_i}} & = \\color{red}{\\boxed{\\beta_0 + \\beta_1\\ c_i + \\omega_i}}\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"bym2\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-2",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-2",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y_i|\\eta_t}} & \\color{red}{\\boxed{\\sim \\text{Poisson}(E_i\\lambda_i)}}\\\\\n\\text{log}(\\lambda_i) = \\eta_i & = \\beta_0 + \\beta_1\\ c_i + \\omega_i\n\\end{aligned}\n\\]\n\n\ng = system.file(\"demodata/germany.graph\",\n                package=\"INLA\")\nGermany[1:3,]\n\n  region         E  Y  x region.struct\n1      1  7.965008  8 56             1\n2      2 22.836219 22 65             2\n3      3 22.094716 19 50             3\n\n\n\nThe code\n\n# define model component\ncmp =  ~ -1 + beta0(1) + beta1(x, model = \"linear\") +\n  space(region, model = \"bym2\", graph = g)\n\n# define model predictor\neta = Y ~ beta0 + beta1 + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              family = \"poisson\",\n              E = E,\n              data = Germany)\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-disease-mapping-3",
    "href": "slides/slides_2.html#inlabru-for-disease-mapping-3",
    "title": "Lecture 1",
    "section": "inlabru for disease mapping",
    "text": "inlabru for disease mapping",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics",
    "href": "slides/slides_2.html#bayesian-geostatistics",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\nEncounter probability of Pacific Cod (Gadus macrocephalus) from a trawl survey.\n\n\n\n\n\n\n\n\\(y(s)\\) Presence or absence in location \\(s\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-1",
    "href": "slides/slides_2.html#bayesian-geostatistics-1",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + f( x(s)) + \\omega(s)\n\\]\nStage 3 Hyperparameters",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-2",
    "href": "slides/slides_2.html#bayesian-geostatistics-2",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response\n\n\\[\n    y(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\] - Stage 2 Latent field model\n\\[\n    \\eta(s) = \\text{logit}(p(s)) = \\beta_0 + f( x(s)) + \\omega(s)\n\\] - A global intercept \\(\\beta_0\\) - A smooth effect of covariate \\(x(s)\\) (depth) - A Gaussian field \\(\\omega(s)\\) (will discuss this later..) - Stage 3 Hyperparameters",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#bayesian-geostatistics-3",
    "href": "slides/slides_2.html#bayesian-geostatistics-3",
    "title": "Lecture 1",
    "section": "Bayesian Geostatistics",
    "text": "Bayesian Geostatistics\n\nStage 1 Model for the response \\[\ny(s)|\\eta(s)\\sim\\text{Binom}(1, p(s))\n\\]\nStage 2 Latent field model \\[\n\\eta(s) = \\text{logit}(p(s)) = \\beta_0 + \\beta_1 x(s) + \\omega(s)\n\\]\nStage 3 Hyperparameters\n\nPrecision for the smooth function \\(f(\\cdot)\\)\nRange and sd in the Gaussian field \\(\\sigma_{\\omega}, \\tau_{\\omega}\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics",
    "href": "slides/slides_2.html#inlabru-for-geostatistics",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n\\[\n\\begin{aligned}\ny(s)|\\eta(s) & \\sim\\text{Binom}(1, p(s))\\\\\n\\eta(s) &  = \\color{red}{\\boxed{\\beta_0}} + \\color{red}{\\boxed{ f(x(s))}} + \\color{red}{\\boxed{ \\omega(s)}}\\\\\n\\end{aligned}\n\\]\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 √ó 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ‚Ñπ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') +\n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-1",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-1",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n\\[\n\\begin{aligned}\ny(s)|\\eta(s) & \\sim\\text{Binom}(1, p(s))\\\\\n\\color{red}{\\boxed{\\eta(s)}} &  = \\color{red}{\\boxed{\\beta_0 +  f(x(s)) +  \\omega(s)}}\\\\\n\\end{aligned}\n\\]\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 √ó 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ‚Ñπ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') +\n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-2",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-2",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics\n\n\nThe Model\n\\[\n\\begin{aligned}\n\\color{red}{\\boxed{y(s)|\\eta(s)}} & \\sim \\color{red}{\\boxed{\\text{Binom}(1, p(s))}}\\\\\n\\eta(s) &  = \\beta_0 +  f(x(s)) +  \\omega(s)\\\\\n\\end{aligned}\n\\]\n\n\ndf %&gt;% select(depth, present) %&gt;% print(n = 3)\n\nSimple feature collection with 2143 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 343.0617 ymin: 5635.893 xmax: 579.3681 ymax: 5839.019\nProjected CRS: +proj=utm +zone=9 +datum=WGS84 +no_defs +type=crs +units=km\n# A tibble: 2,143 √ó 3\n  depth present            geometry\n  &lt;dbl&gt;   &lt;dbl&gt;        &lt;POINT [km]&gt;\n1   201       1 (446.4752 5793.426)\n2   212       1 (446.4594 5800.136)\n3   220       0 (448.5987 5801.687)\n# ‚Ñπ 2,140 more rows\n\n\n\nThe code\n\n# define model component\ncmp = ~ -1 + Intercept(1) +  depth_smooth(log(depth), model='rw2') +\n  space(geometry, model = spde_model)\n\n# define model predictor\neta = present ~ Intercept + depth_smooth + space\n\n# build the observation model\nlik = bru_obs(formula = eta,\n              data = df,\n              family = \"binomial\")\n\n# fit the model\nfit = bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#inlabru-for-geostatistics-3",
    "href": "slides/slides_2.html#inlabru-for-geostatistics-3",
    "title": "Lecture 1",
    "section": "inlabru for geostatistics",
    "text": "inlabru for geostatistics",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_2.html#take-home-message",
    "href": "slides/slides_2.html#take-home-message",
    "title": "Lecture 1",
    "section": "Take home message!",
    "text": "Take home message!\n\nMany of the models you have used (and some you have never used but will learn about) are just special cases of the large class of Latent Gaussian models\ninlabru provides an efficient and unified way to fit all these models!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 1"
    ]
  },
  {
    "objectID": "slides/slides_4.html#motivation",
    "href": "slides/slides_4.html#motivation",
    "title": "Lecture 3",
    "section": "Motivation",
    "text": "Motivation\nData are often observed in time, and time dependence is often expected.\n\n\n\nObservations are correlated in time",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#motivation-1",
    "href": "slides/slides_4.html#motivation-1",
    "title": "Lecture 3",
    "section": "Motivation",
    "text": "Motivation\n\nSmoothing of the time effect",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#what-is-our-goal",
    "href": "slides/slides_4.html#what-is-our-goal",
    "title": "Lecture 3",
    "section": "What is our goal?",
    "text": "What is our goal?\n\nSmoothing of the time effect\n\n\n\nNote: We can use the same model to smooth covariate effects!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#what-is-our-goal-1",
    "href": "slides/slides_4.html#what-is-our-goal-1",
    "title": "Lecture 3",
    "section": "What is our goal?",
    "text": "What is our goal?\n\nSmoothing of the time effect\nPrediction\n\n\n\nWe can ‚Äúpredict‚Äù any unobserved data, not only future data, e.g.¬†gaps in the data etc.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#modelling-time-with-inla",
    "href": "slides/slides_4.html#modelling-time-with-inla",
    "title": "Lecture 3",
    "section": "Modelling time with INLA",
    "text": "Modelling time with INLA\nTime can be indexed over a\n\nDiscrete domain (e.g., years)\nContinuous domain",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#modelling-time-with-inla-1",
    "href": "slides/slides_4.html#modelling-time-with-inla-1",
    "title": "Lecture 3",
    "section": "Modelling time with INLA",
    "text": "Modelling time with INLA\nTime can be indexed over a\n\nDiscrete domain (e.g., years)\n\nMain models: RW1, RW2 and AR1\nNote: RW1 and RW2 are also used for smoothing covariates\n\nContinuous domain\n\nHere we use the so-called SPDE-approach (more on this later)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#example---height-of-lake-erie-in-time",
    "href": "slides/slides_4.html#example---height-of-lake-erie-in-time",
    "title": "Lecture 3",
    "section": "Example - Height of Lake Erie in time",
    "text": "Example - Height of Lake Erie in time\n\nGoal we want understand the pattern and predict into the future",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#random-walk-models",
    "href": "slides/slides_4.html#random-walk-models",
    "title": "Lecture 3",
    "section": "Random Walk models",
    "text": "Random Walk models\nRandom walk models encourage the mean of the linear predictor to vary gradually over time.\n\nThey do this by assuming that, on average, the time effect at each point is the mean of the effect at the neighbouring points.\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Walk of order 1 (RW1) we take the two nearest neighbours\nRandom Walk of order 2 (RW2) we take the four nearest neighbours",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#random-walks-of-order-1",
    "href": "slides/slides_4.html#random-walks-of-order-1",
    "title": "Lecture 3",
    "section": "Random walks of order 1",
    "text": "Random walks of order 1\nIdea: \\(\\longrightarrow\\ u_t = \\text{mean}(u_{t-1} , u_{t+1}) + \\text{Gaussian error with precision  } \\tau\\)\n\nDefinition\n\\[\n  \\pi(\\mathbf{u} \\mid \\tau) \\propto\n  \\exp\\!\\left(\n     -\\frac{\\tau}{2} \\sum_{t=1}^{T-1} (u_{t+1} - u_t)^2\n  \\right) = \\exp\\!\\left(-\\tfrac{1}{2} \\, \\mathbf{u}^{\\top} \\mathbf{Q}\\ \\mathbf{u}\\right)\n\\]\n\\[\n    \\mathbf{Q} = \\tau\n    \\begin{bmatrix}\n      1 & -1 &  &        &        &   \\\\\n      -1 & 2 & -1 &        &        &   \\\\\n         &    & \\ddots & \\ddots & \\ddots &   \\\\\n         &    &        & -1     & 2 & -1 \\\\\n         &    &        &        & -1 & 1\n    \\end{bmatrix}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#random-walks-of-order-1-1",
    "href": "slides/slides_4.html#random-walks-of-order-1-1",
    "title": "Lecture 3",
    "section": "Random walks of order 1",
    "text": "Random walks of order 1\nIdea: \\(\\longrightarrow\\ u_t = \\text{mean}(u_{t-1} , u_{t+1}) + \\text{Gaussian error with precision  } \\tau\\)\nDefinition\n\\[\n  \\pi(\\mathbf{u} \\mid \\tau) \\propto\n  \\exp\\!\\left(\n     -\\frac{\\tau}{2} \\sum_{t=1}^{T-1} (u_{t+1} - u_t)^2\n  \\right) = \\exp\\!\\left(-\\tfrac{1}{2} \\, \\mathbf{u}^{\\top} \\mathbf{Q}\\ \\mathbf{u}\\right)\n\\]\n\nRole of the precision parameter \\(\\tau\\) and prior distribution\nRW as intrinsic model",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#what-is-the-role-of-the-precision-parameter",
    "href": "slides/slides_4.html#what-is-the-role-of-the-precision-parameter",
    "title": "Lecture 3",
    "section": "What is the role of the precision parameter?",
    "text": "What is the role of the precision parameter?\n\n\\(\\tau\\) says how much \\(u_t\\) can vary around its mean\n\nSmall \\(\\tau\\) \\(\\rightarrow\\) large variation \\(\\rightarrow\\) less smooth effect\nLarge \\(\\tau\\) \\(\\rightarrow\\) small variation \\(\\rightarrow\\) smoother effect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe need to set a prior distribution for \\(\\tau\\).\nA common option is the so called PC-priors",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#penalized-complexity-pc-priors",
    "href": "slides/slides_4.html#penalized-complexity-pc-priors",
    "title": "Lecture 3",
    "section": "Penalized Complexity (PC) priors",
    "text": "Penalized Complexity (PC) priors\n\nPC priors are easily available in inlabru for many model parameters\n\n\n\nThey are built with two principles in mind:\n\nThe prior discourages overdispersion by penalizing deviation from a base model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA line is the base model\nWe want to penalize more complex models",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#penalized-complexity-pc-priors-1",
    "href": "slides/slides_4.html#penalized-complexity-pc-priors-1",
    "title": "Lecture 3",
    "section": "Penalized Complexity (PC) priors",
    "text": "Penalized Complexity (PC) priors\n\nPC prior are easily available in inlabru for many model parameters\nThey are built with two principle in mind:\n\nThe prior discourages overdispersion by penalizing deviation from a base model\nUser-defined scaling\n\n\n\n\n\\[\n\\begin{eqnarray}\n\\sigma = \\sqrt{1/\\tau} \\\\\n\\text{Prob}(\\sigma&gt;U) = \\alpha;\\\\ \\qquad U&gt;0, \\ \\alpha \\in (0,1)\n\\end{eqnarray}\n\\]\n\n\n\\(U\\) an upper limit for the standard deviation and \\(\\alpha\\) a small probability.\n\\(U\\) a likely value for the standard deviation and \\(\\alpha=0.5\\).",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#example",
    "href": "slides/slides_4.html#example",
    "title": "Lecture 3",
    "section": "Example",
    "text": "Example\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_i|\\eta_i, \\sigma^2 & \\sim \\mathcal{N}(\\eta_i,\\sigma^2)\\\\\n\\eta_i & = \\beta_0 + f(t_i)\\\\\nf(t_1),f(t_2),\\dots,f(t_n) &\\sim \\text{RW2}(\\tau)\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe code\n\ncmp = ~ Intercept(1) + \n  time(year, model = \"rw1\",\n       hyper = list(prec = \n                      list(prior = \"pc.prec\",\n                           param = c(0.5, 0.5))))",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#rw-as-intrinsic-models",
    "href": "slides/slides_4.html#rw-as-intrinsic-models",
    "title": "Lecture 3",
    "section": "RW as intrinsic models",
    "text": "RW as intrinsic models\nRW1 defines differences, not absolute levels:\n\nOnly the changes between neighbouring terms are modelled.\nMathematically, \\[\n(u_1,\\dots,u_n)\\text{ and }(u_1+a,\\dots,u_n+a)\n\\] produce identical likelihoods ‚Äî they‚Äôre indistinguishable.\n\n\nThis means:\n\nIf \\(u_t\\sim\\text{RW}1\\) then \\[\n\\eta_t = \\beta_0 + u_t = (\\beta_0+k) + (u_t-k) = \\beta_0^* + u_t^*  \n\\] so parameters are not well-defined\n\n\n\nSolution:\n\nSum to zero constraint \\(\\sum_{i = 1}^n u_i = 0\\)\nThis is included in the model be default",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#rw-as-intrinsic-models-1",
    "href": "slides/slides_4.html#rw-as-intrinsic-models-1",
    "title": "Lecture 3",
    "section": "RW as intrinsic models",
    "text": "RW as intrinsic models\n\ncmp1 = ~ Intercept(1) + time(year, model = \"rw1\", constr = TRUE)\ncmp2 = ~ Intercept(1) + time(year, model = \"rw1\", constr = FALSE)\nlik = bru_obs(formula = Erie~.,\n              data = lakes)\n\nfit1 = bru(cmp1,lik)\nfit2 = bru(cmp2,lik)\n\n\n\n[1] \"FIT1 - Intercept\"\n\n\n             mean    sd 0.025quant 0.5quant 0.975quant    mode kld\nIntercept 174.138 0.002    174.135  174.138    174.142 174.138   0\n\n\n[1] \"FIT2 - Intercept\"\n\n\n          mean     sd 0.025quant 0.5quant 0.975quant mode kld\nIntercept    0 31.623    -62.009        0     62.009    0   0\n\n\n[1] \"FIT1 - RW1 effect\"\n\n\n    ID   mean    sd 0.025quant 0.5quant 0.975quant   mode kld\n1 1918 -0.122 0.015     -0.153   -0.122     -0.090 -0.122   0\n2 1919  0.039 0.015      0.006    0.040      0.069  0.042   0\n\n\n[1] \"FIT2 - RW1 effect\"\n\n\n    ID    mean     sd 0.025quant 0.5quant 0.975quant    mode kld\n1 1918 174.017 31.623    112.008  174.017    236.025 174.017   0\n2 1919 174.176 31.623    112.168  174.176    236.185 174.176   0",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#random-walks-of-order-2",
    "href": "slides/slides_4.html#random-walks-of-order-2",
    "title": "Lecture 3",
    "section": "Random walks of order 2",
    "text": "Random walks of order 2\n\nJust like RW1, but now we consider 4 neighbours instead of 2\n\n\\[\nu_t = \\text{mean}(u_{t-2} ,u_{t-1} , u_{t+1}, u_{t+2} ) + \\text{some Gaussian error with precision  } \\tau\n\\]\n\nRW2 are smoother than RW1\nThe precision has the same role as for RW1",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#example-1",
    "href": "slides/slides_4.html#example-1",
    "title": "Lecture 3",
    "section": "Example",
    "text": "Example\n\n\n\ncmp1 = ~ Intercept(1) + \n  time(year, model = \"rw1\", \n       scale.model = T,\n       hyper = list(prec = \n                      list(prior = \"pc.prec\",\n                           param = c(0.3,0.5))))\n\ncmp2 = ~ Intercept(1) + \n  time(year, model = \"rw2\",\n       scale.model = T,\n       hyper = list(prec = \n                      list(prior = \"pc.prec\", \n                           param = c(0.3,0.5))))\n\n\nlik = bru_obs(formula = Erie~ ., \n              data = lakes)\n\nfit1 = bru(cmp1, lik)\nfit2 = bru(cmp2, lik)\n\n\n\n\n\n\n\n\n\n\n\nNOTE: the scale.model = TRUE option scales the \\(\\mathbf{Q}\\) matrix so the precision parameter has the same interpretation in both models.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#rw-models-as-smoothers-for-covariates",
    "href": "slides/slides_4.html#rw-models-as-smoothers-for-covariates",
    "title": "Lecture 3",
    "section": "RW models as smoothers for covariates",
    "text": "RW models as smoothers for covariates\n\nRW models are discrete models\nCovariates are often recorded as continuous values\nThe function inla.group() will bin covariate values into groups (default 25 groups)\n\n\ninla.group(x, n = 25, method = c(\"cut\", \"quantile\"))\n\n\nTwo ways to bin\n\ncut (default) splits the data using equal length intervals\nquantile uses equi-distant quantiles in the probability space.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#rw-models-as-smoothers-for-covariates---example",
    "href": "slides/slides_4.html#rw-models-as-smoothers-for-covariates---example",
    "title": "Lecture 3",
    "section": "RW models as smoothers for covariates - Example",
    "text": "RW models as smoothers for covariates - Example\n\n\nThe data are derived from an anthropometric study of 892 females under 50 years in three Gambian villages in West Africa.\nAge - Age of respondent (continuous)\ntriceps - Triceps skinfold thickness.\n\n\n\n\n\n\n\n\n\n\n\n\ntriceps$age_group = inla.group(triceps$age, n = 30)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#model-fit-and-results",
    "href": "slides/slides_4.html#model-fit-and-results",
    "title": "Lecture 3",
    "section": "Model fit and results",
    "text": "Model fit and results\n\ncmp = ~ Intercept(1) + cov(age_group, model = \"rw2\", scale.model =T)\nlik = bru_obs(formula = triceps ~.,\n              data = triceps)\n\nfit = bru(cmp, lik)\npred = predict(fit, triceps, ~ Intercept + cov)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#summary-rw-1-and-2-models",
    "href": "slides/slides_4.html#summary-rw-1-and-2-models",
    "title": "Lecture 3",
    "section": "Summary RW (1 and 2) models",
    "text": "Summary RW (1 and 2) models\n\nLatent effects suitable for smoothing and modelling temporal data.\nOne hyperparameter: the precision \\(\\tau\\)\n\nUse PC prior for \\(\\tau\\)\n\nIt is an intrinsic model\n\nThe precision matrix \\(\\mathbf{Q}\\) is rank deficient\nA sum-to-zero constraint is added to make the model identifiable!\n\nRW2 models are smoother than RW1",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#auto-regressive-models-of-order-1-ar1",
    "href": "slides/slides_4.html#auto-regressive-models-of-order-1-ar1",
    "title": "Lecture 3",
    "section": "Auto Regressive Models of order 1 (AR1)",
    "text": "Auto Regressive Models of order 1 (AR1)\nDefinition\n\\[\nu_t = \\phi u_{t-i} + \\epsilon_t; \\qquad \\phi\\in(-1,1), \\ \\epsilon_t\\sim\\mathcal{N}(0,\\tau^{-1})\n\\]\n\\[\n\\pi(\\mathbf{u}|\\tau)\\propto\\exp\\left(-\\frac{\\tau}{2}\\mathbf{u}^T\\mathbf{Q}\\mathbf{u}\\right)\n\\]\nwith\n\\[\n    \\mathbf{Q} =\n    \\begin{bmatrix}\n      1 & -\\phi &  &        &        &   \\\\\n      -\\phi & (1+\\phi^2) & -\\phi &        &        &   \\\\\n         &    & \\ddots & \\ddots & \\ddots &   \\\\\n         &    &        & -\\phi     & (1+\\phi^2) & -\\phi \\\\\n         &    &        &        & -\\phi & 1\n    \\end{bmatrix}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#ar1-hyperparameters-and-prior",
    "href": "slides/slides_4.html#ar1-hyperparameters-and-prior",
    "title": "Lecture 3",
    "section": "AR1: Hyperparameters and prior",
    "text": "AR1: Hyperparameters and prior\nThe AR1 model has two parameters\n\nThe precision \\(\\tau\\)\nThe autocorrelation (or persistence) parameter \\(\\phi\\in(0,1)\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#ar1-hyperparameters-and-prior-1",
    "href": "slides/slides_4.html#ar1-hyperparameters-and-prior-1",
    "title": "Lecture 3",
    "section": "AR1: Hyperparameters and prior",
    "text": "AR1: Hyperparameters and prior\nThe AR1 model has two parameters\n\nThe precision \\(\\tau\\)\n\nPC prior as before - Baseline \\(\\tau=0\\) pc.prec \\[\n\\text{Prob}(\\sigma &gt; u) = \\alpha\n\\]\n\nThe autocorrelation (or persistence) parameter $(-1,1)\n\nTwo choices of PC priors\n\n\n\n\n\nBaseline \\(\\phi = 0\\) pc.cor0\n\n\\[\n\\begin{eqnarray}\n\\text{Prob}(|\\rho| &gt; u) = \\alpha;\\\\\n-1&lt;u&lt;1;\\ 0&lt;\\alpha&lt;1\n\\end{eqnarray}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#ar1-hyperparameters-and-prior-2",
    "href": "slides/slides_4.html#ar1-hyperparameters-and-prior-2",
    "title": "Lecture 3",
    "section": "AR1: Hyperparameters and prior",
    "text": "AR1: Hyperparameters and prior\nThe AR1 model has two parameters\n\nThe precision \\(\\tau\\)\n\nPC prior as before - Baseline \\(\\tau=0\\) pc.prec \\[\n\\text{Prob}(\\sigma &gt; u) = \\alpha\n\\]\n\nThe autocorrelation (or persistence) parameter $(-1,1)\n\nTwo choices of PC priors\n\n\n\n\n\nBaseline \\(\\phi = 1\\) pc.cor1\n\n\\[\n\\begin{eqnarray}\n\\text{Prob}(\\rho &gt; u) = \\alpha;&\\\\\n-1&lt;u&lt;1;\\qquad &\\sqrt{\\frac{1-u}{2}}&lt;\\alpha&lt;1\n\\end{eqnarray}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#example---ar1-and-rw1-for-earthquakes-data",
    "href": "slides/slides_4.html#example---ar1-and-rw1-for-earthquakes-data",
    "title": "Lecture 3",
    "section": "Example - AR1 and RW1 for earthquakes data",
    "text": "Example - AR1 and RW1 for earthquakes data\n\n\nThe Model\n\\[\n\\begin{aligned}\ny_t|\\eta_t & \\sim \\text{Poisson}(\\exp(\\eta_t))\\\\\n\\eta_t & = \\beta_0 + u_t\\\\\n1.\\ u_t&\\sim \\text{RW}1(\\tau)\\\\\n2.\\ u_t&\\sim \\text{AR}1(\\tau, \\phi)\\\\\n\\end{aligned}\n\\]\n\nNumber of serious earthquakes per year\n\n\n\n\n\n\n\n\n\n\n\nhyper = list(prec = list(prior = \"pc.prec\", param = c(1,0.5)))\ncmp1 = ~ Intercept(1) + time(year, model = \"rw1\", scale.model = T,\n                             hyper = hyper)\ncmp2 = ~ Intercept(1) + time(year, model = \"ar1\",\n                             hyper = hyper)\n\n\nlik = bru_obs(formula = quakes ~ .,\n              family = \"poisson\",\n              data = df)\n\nfit1 = bru(cmp1, lik)\nfit2 = bru(cmp2, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#example---rw1-and-ar1",
    "href": "slides/slides_4.html#example---rw1-and-ar1",
    "title": "Lecture 3",
    "section": "Example - RW1 and AR1",
    "text": "Example - RW1 and AR1\nEstimated trend",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#example---rw1-and-ar1-1",
    "href": "slides/slides_4.html#example---rw1-and-ar1-1",
    "title": "Lecture 3",
    "section": "Example - RW1 and AR1",
    "text": "Example - RW1 and AR1\nPredictions",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_4.html#ar1-vs.-rw-models",
    "href": "slides/slides_4.html#ar1-vs.-rw-models",
    "title": "Lecture 3",
    "section": "AR1 vs.¬†RW models",
    "text": "AR1 vs.¬†RW models",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 3"
    ]
  },
  {
    "objectID": "slides/slides_9.html#motivation",
    "href": "slides/slides_9.html#motivation",
    "title": "Lecture 9",
    "section": "Motivation",
    "text": "Motivation\n\nMany real-world data sets vary in both space and time.\n\nExamples: rainfall, temperature, air pollution, crop yield, disease spread.\n\nWe often want to model how similarity (correlation) changes across both dimensions.\n\n\nThe good news üëç\n\nthe framework works with spatio-temporal data as well\n\nThe bad news üëé\n\nfully spatio-temporal models are complex\nmodel fitting can take a long time\ndifferent types of spatio-temporal data structures lead to different types of complexities",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#complexity-of-spatio-temporal-models",
    "href": "slides/slides_9.html#complexity-of-spatio-temporal-models",
    "title": "Lecture 9",
    "section": "Complexity of spatio-temporal models",
    "text": "Complexity of spatio-temporal models\n\nadditional dependencies: in space and time\nspatial and temporal behaviour can be independent on each other, or dependent: i.e.¬†properties of the spatial structure vary through time (or vice versa)\n\n\nSeveral options are available in inlabru\n\nAreal Model\nGeostatistical and Point Process models",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#the-spatio-temporal-framework",
    "href": "slides/slides_9.html#the-spatio-temporal-framework",
    "title": "Lecture 9",
    "section": "The Spatio-Temporal Framework",
    "text": "The Spatio-Temporal Framework\nWe define a stochastic process:\n\\[\nZ(s,t), \\quad s \\in \\mathcal{S} \\subset \\mathbb{R}^d, \\quad t \\in \\mathcal{T}\n\\]\nThe covariance function:\n\\[\nC((s_1,t_1),(s_2,t_2)) = \\text{Cov}[Z(s_1,t_1), Z(s_2,t_2)]\n\\]\nOur goal: specify \\(C(\\cdot)\\) to capture spatial, temporal, and joint dependencies.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#separable-and-non-separable-models",
    "href": "slides/slides_9.html#separable-and-non-separable-models",
    "title": "Lecture 9",
    "section": "Separable and non-separable models",
    "text": "Separable and non-separable models\n\n\nSeparable models \\[\nC((s_1,t_1),(s_2,t_2)) = C_S(s_1,s_2) \\times C_T(t_1,t_2)\n\\]\nInterpretation:\n\nSpace and time act independently.\nSpatial correlation not dependent on time lag.\n\nAdvantages: üòÄ\n\nSimpler estimation.\nFewer parameters \\(\\rightarrow\\) Easier computation.\n\nDisadvantages: üòî\n\nUnrealistic for evolving or propagating phenomena.\n\n\nNon-Separable models",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#separable-and-non-separable-models-1",
    "href": "slides/slides_9.html#separable-and-non-separable-models-1",
    "title": "Lecture 9",
    "section": "Separable and non-separable models",
    "text": "Separable and non-separable models\n\n\nSeparable models \\[\nC((s_1,t_1),(s_2,t_2)) = C_S(s_1,s_2) \\times C_T(t_1,t_2)\n\\]\nInterpretation:\n\nSpace and time act independently.\nSpatial correlation not dependent on time lag.\n\nAdvantages: üòÄ\n\nSimpler estimation.\nFewer parameters \\(\\rightarrow\\) Easier computation.\n\nDisadvantages: üòî\n\nUnrealistic for evolving or propagating phenomena.\n\n\nNon-Separable models\n\\[\nC((s_1,t_1),(s_2,t_2)) = f(|s_2-s_1|,|t_2-t_1|)\n\\] Advantages: üòÄ\n\nMore realistic for dynamic physical processes.\nCan model propagation or decay that varies over time.\n\nDisadvantages: üòî\n\nMore parameters ‚Üí complex estimation.\nNeed more data for estimation\nInterpretation may be less straightforward.\n\n\n\nWe here focus on separable models",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#spatio-temporal-models-for-areal-data",
    "href": "slides/slides_9.html#spatio-temporal-models-for-areal-data",
    "title": "Lecture 9",
    "section": "Spatio temporal models for areal data",
    "text": "Spatio temporal models for areal data\n\\(\\frac{\\text{Number of cases } Y_{st}}{\\text{Expected numer of cases }E_{st}}\\) in Ohio from 1968 to 1988 (simulated data!)\nSpace",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#spatio-temporal-models-for-areal-data-1",
    "href": "slides/slides_9.html#spatio-temporal-models-for-areal-data-1",
    "title": "Lecture 9",
    "section": "Spatio temporal models for areal data",
    "text": "Spatio temporal models for areal data\n\\(\\frac{\\text{Number of cases } Y_{st}}{\\text{Expected numer of cases }E_{st}}\\) in Ohio from 1968 to 1988\nTime",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#spatio-temporal-models-for-areal-data-2",
    "href": "slides/slides_9.html#spatio-temporal-models-for-areal-data-2",
    "title": "Lecture 9",
    "section": "Spatio temporal models for areal data",
    "text": "Spatio temporal models for areal data\n\\(\\frac{\\text{Number of cases } Y_{st}}{\\text{Expected numer of cases }E_{st}}\\) in Ohio from 1968 to 1988\nTime",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#spatio-temporal-models-for-areal-data-3",
    "href": "slides/slides_9.html#spatio-temporal-models-for-areal-data-3",
    "title": "Lecture 9",
    "section": "Spatio temporal models for areal data",
    "text": "Spatio temporal models for areal data\nThe observation model\n\\[\nY_{st}|\\lambda_{st}\\sim\\text{Poisson}(E_{st}\\lambda_{st})\n\\]\nWe are going to see 3 different models for the linear predictor \\(\\eta_{st} = \\log \\lambda_{st}\\)\n\nModel 1: \\(\\eta_{st} = \\beta_0 + u_s + v_t\\) with \\(u_s\\sim\\text{CAR}(\\tau)\\)\nHere the time effect \\(v_t\\) is constant w.r.t. space and the space effect \\(u_t\\) is constant w.r.t. time.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-1---implementation",
    "href": "slides/slides_9.html#model-1---implementation",
    "title": "Lecture 9",
    "section": "Model 1 - Implementation",
    "text": "Model 1 - Implementation\n\ncmp_1 = ~ Intercept(1) +\n  space(id_area, model = \"besag\", graph = g, scale.model = T) +\n  time(year, model  = \"rw2\", scale.model = T)\n\nlik = bru_obs(formula = Y~.,\n              data = out,\n              family = \"poisson\",\n              E = E )\nfit_1 = bru(cmp_1, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-1---implementation-1",
    "href": "slides/slides_9.html#model-1---implementation-1",
    "title": "Lecture 9",
    "section": "Model 1 - Implementation",
    "text": "Model 1 - Implementation\n\ncmp_1 = ~ Intercept(1) +\n  space(id_area, model = \"besag\", graph = g, scale.model = T) +\n  time(year, model  = \"rw2\", scale.model = T)\n\nlik = bru_obs(formula = Y~.,\n              data = out,\n              family = \"poisson\",\n              E = E )\nfit_1 = bru(cmp_1, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#spatio-temporal-models-for-areal-data-4",
    "href": "slides/slides_9.html#spatio-temporal-models-for-areal-data-4",
    "title": "Lecture 9",
    "section": "Spatio temporal models for areal data",
    "text": "Spatio temporal models for areal data\nThe observation model \\[\nY_{st}|\\lambda_{st}\\sim\\text{Poisson}(E_{st}\\lambda_{st})\n\\]\nWe are going to see 3 different models for the linear predictor \\(\\eta_{st} = \\log \\lambda_{st}\\)\n\nModel 1: \\(\\eta_{st} = \\beta_0 + u_s + v_t\\) with \\(u_s\\sim\\text{CAR}(\\tau)\\)\nHere the time effect \\(v_t\\) is constant w.r.t. space and the space effect \\(u_t\\) is constant w.r.t. time.\nModel 2: \\(\\eta_{st} = \\beta_0 + u_{st} + v_t\\) with \\(u_{st}\\sim\\text{CAR}(\\tau),\\ i = 1,\\dots,T\\)\nHere the spatial fields are different for each year, they are independent on each other.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-2---implementation",
    "href": "slides/slides_9.html#model-2---implementation",
    "title": "Lecture 9",
    "section": "Model 2 - Implementation",
    "text": "Model 2 - Implementation\n\ncmp_2 = ~ Intercept(1) + \n  space(id_area, model = \"besag\", graph = g, scale.model = T,\n                               replicate = id_time) +\n  time(year, model  = \"rw2\", scale.model = T)\n\nfit_2 = bru(cmp_2, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-2---implementation-1",
    "href": "slides/slides_9.html#model-2---implementation-1",
    "title": "Lecture 9",
    "section": "Model 2 - Implementation",
    "text": "Model 2 - Implementation\n\ncmp_2 = ~ Intercept(1) + \n  space(id_area, model = \"besag\", graph = g, scale.model = T,\n                               replicate = id_time) +\n  time(year, model  = \"rw2\", scale.model = T)\n\nfit_2 = bru(cmp_2, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#spatio-temporal-models-for-areal-data-5",
    "href": "slides/slides_9.html#spatio-temporal-models-for-areal-data-5",
    "title": "Lecture 9",
    "section": "Spatio temporal models for areal data",
    "text": "Spatio temporal models for areal data\nThe observation model\n\\[\nY_{st}|\\lambda_{st}\\sim\\text{Poisson}(E_{st}\\lambda_{st})\n\\]\nWe are going to see 3 different models for the linear predictor \\(\\eta_{st} = \\log \\lambda_{st}\\)\n\nModel 1: \\(\\eta_{st} = \\beta_0 + u_s + v_t\\) with \\(u_s\\sim\\text{CAR}(\\tau)\\)\nHere the time effect \\(v_t\\) is constant w.r.t. space and the space effect \\(u_t\\) is constant w.r.t. time.\nModel 2: \\(\\eta_{st} = \\beta_0 + u_{st} + v_t\\) with \\(u_{st}\\sim\\text{CAR}(\\tau),\\ i = 1,\\dots,T\\)\nHere the spatial fields are different for each year, they are independent on each other.\nModel 3: \\(\\eta_{st} = \\beta_0 + u_{st} + v_t\\) with \\[\nu_{st} = \\phi u_{st-1} + w_{st}, \\text{ with } w_{st}\\sim\\text{CAR}(\\tau),\\ i = 1,\\dots,T\n\\]\n\nHere we are adding more temporal dependence in the spatial term, using a AR1 type model Here the spatial fields are different for each year, they are independent on each other.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-3---implementation",
    "href": "slides/slides_9.html#model-3---implementation",
    "title": "Lecture 9",
    "section": "Model 3 - Implementation",
    "text": "Model 3 - Implementation\n\ncmp_3 = ~ Intercept(1) + \n  space(id_area, model = \"besag\", graph = g, scale.model = T,\n                               group = id_time, control.group = list(model = \"ar1\")) +\n  time(year, model  = \"rw2\", scale.model = T)\n\nfit_3 = bru(cmp_3, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-3---implementation-1",
    "href": "slides/slides_9.html#model-3---implementation-1",
    "title": "Lecture 9",
    "section": "Model 3 - Implementation",
    "text": "Model 3 - Implementation\n\ncmp_3 = ~ Intercept(1) + \n  space(id_area, model = \"besag\", graph = g, scale.model = T,\n                               group = id_time, control.group = list(model = \"ar1\")) +\n  time(year, model  = \"rw2\", scale.model = T)\n\nfit_3 = bru(cmp_3, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#spatio-temporal-models-for-areal-data-6",
    "href": "slides/slides_9.html#spatio-temporal-models-for-areal-data-6",
    "title": "Lecture 9",
    "section": "Spatio temporal models for areal data",
    "text": "Spatio temporal models for areal data\nThe observation model\n\\[\nY_{st}|\\lambda_{st}\\sim\\text{Poisson}(E_{st}\\lambda_{st})\n\\]\n\nModel 1: \\(\\eta_{st} = \\beta_0 + u_s + v_t\\) with \\(u_s\\sim\\text{CAR}(\\tau)\\)\nModel 2: \\(\\eta_{st} = \\beta_0 + u_{st} + v_t\\) with \\(u_{st}\\sim\\text{CAR}(\\tau),\\ i = 1,\\dots,T\\)\nModel 3: \\(\\eta_{st} = \\beta_0 + u_{st} + v_t\\) with \\[\nu_{st} = \\phi u_{st-1} + w_{st}, \\text{ with } w_{st}\\sim\\text{CAR}(\\tau),\\ i = 1,\\dots,T\n\\]\n\nCompare\n\nDifferences between Model 2 and 3 are not evident when estimating the past, but they would give different predictions\nModel 3 in this case seem to better fit the data (more about these measures later)\n\n\n\n\n\n    model      DIC     WAIC\n1 Model 1 14825.56 20915.22\n2 Model 2 12362.26 12272.09\n3 Model 3 11971.38 11957.08\n\n\n\n\n\n                     mean    sd 0.025quant 0.975quant\nPrecision for space 14.22  1.41      11.63      17.16\nGroupRho for space   0.88  0.01       0.85       0.91\nPrecision for time  68.98 29.01      27.58     139.62",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#spatio-temporal-models-for-areal-data-in-inlabru",
    "href": "slides/slides_9.html#spatio-temporal-models-for-areal-data-in-inlabru",
    "title": "Lecture 9",
    "section": "Spatio temporal models for areal data in inlabru",
    "text": "Spatio temporal models for areal data in inlabru\n\nModels can be stuck together using either\n\nreplicate - the different slices are independent but share the hyperparameter\ngroup - different dependence structures are implemented\n\n\nTypes of group model\n\nnames(inla.models()$group)\n\n[1] \"exchangeable\"    \"exchangeablepos\" \"ar1\"             \"ar\"             \n[5] \"rw1\"             \"rw2\"             \"besag\"           \"iid\"            \n\n\n\nNote The group and replicate features can be used for more than space-time modeling!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#continuous-space-models-1",
    "href": "slides/slides_9.html#continuous-space-models-1",
    "title": "Lecture 9",
    "section": "Continuous space models",
    "text": "Continuous space models\n(Simulated data)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#spatio-temporal-models-for-areal-data-7",
    "href": "slides/slides_9.html#spatio-temporal-models-for-areal-data-7",
    "title": "Lecture 9",
    "section": "Spatio temporal models for areal data",
    "text": "Spatio temporal models for areal data\nThe observation model\n\\[\nY_t(s)|\\mu_t(s)\\sim\\mathcal{N}(\\mu_t(s),\\sigma_y^2); \\qquad \\eta_t(s) =  \\mu_t(s)\n\\]\nWe are going to see 3 different models for the linear predictor \\(\\eta_{st}\\)\n\nModel 1: \\(\\eta_t(s) = \\beta_0 + u(s) + v_t\\) with \\(u(s)\\sim\\text{GF}(\\rho,\\sigma_u)\\)\nHere the time effect \\(v_t\\) is constant w.r.t. space and the space effect \\(u(s)\\) is constant w.r.t. time.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-1---implementation-2",
    "href": "slides/slides_9.html#model-1---implementation-2",
    "title": "Lecture 9",
    "section": "Model 1 - Implementation",
    "text": "Model 1 - Implementation\nDefine mesh and spde model\n\n\n\nborder_simplified = st_simplify(border, dTolerance = 25)\n\nmesh = fm_mesh_2d(boundary = border_simplified,\n  max.edge = c(30, 90), cutoff = 15,\n   offset = c(-0.05, -0.25),\n  crs = st_crs(border))\n\nspde = inla.spde2.pcmatern(mesh = mesh, \n  prior.range = c(200, 0.5), # P(range &lt; 100) = 0.5\n  prior.sigma = c(1, 0.5)) # P(sigma &gt; 1) = 0.5\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\nCoordinate system is in Km (this is recommended always!)\nThe size of the domain is ca \\(630\\times488 \\text{ Km}^2\\) we use this as a guideline to define the prior for the range.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-1---implementation-3",
    "href": "slides/slides_9.html#model-1---implementation-3",
    "title": "Lecture 9",
    "section": "Model 1 - Implementation",
    "text": "Model 1 - Implementation\nFit the model\n\n\n\ncmp = ~ Intercept(1) + \n  time(time, model = \"rw2\") +\n  space(geometry, model = spde)\n\nlik = bru_obs(formula =  Y~ .,\n              data = dd)\n\nfit_1 = bru(cmp, lik)\n\n# plot time effect\n\np = fit_1$summary.random$time %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID, \n                  ymin = `0.025quant`,\n                  ymax = `0.975quant`), \n              alpha = 0.5) +\n  geom_line(aes(ID, mean))",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-1---implementation-4",
    "href": "slides/slides_9.html#model-1---implementation-4",
    "title": "Lecture 9",
    "section": "Model 1 - Implementation",
    "text": "Model 1 - Implementation\nSpace-time Predictions\n\npxl = fm_pixels(mesh, mask = border)\n\npxl_time = fm_cprod(pxl, data.frame(time = seq(1,12)))\n\npred_1 = predict(fit_1, pxl_time, ~ data.frame(mu = Intercept + space + time,\n                                      time_effect = time,\n                                      space_effect = space))",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#spatio-temporal-models-for-areal-data-8",
    "href": "slides/slides_9.html#spatio-temporal-models-for-areal-data-8",
    "title": "Lecture 9",
    "section": "Spatio temporal models for areal data",
    "text": "Spatio temporal models for areal data\nThe observation model\n\\[\nY_t(s)|\\mu_t(s)\\sim\\mathcal{N}(\\mu_t(s),\\sigma_y^2); \\qquad \\eta_t(s) =  \\mu_t(s)\n\\]\nWe are going to see 3 different models for the linear predictor \\(\\eta_{st}\\)\n\nModel 1: \\(\\eta_t(s) = \\beta_0 + u(s) + v_t\\) with \\(u(s)\\sim\\text{GF}(\\rho,\\sigma_u)\\)\nHere the time effect \\(v_t\\) is constant w.r.t. space and the space effect \\(u(s)\\) is constant w.r.t. time.\nModel 2: \\(\\eta_t(s) = \\beta_0 + u_t(s) + v_t\\) with \\(u_t(s)\\sim\\text{GF}(\\rho,\\sigma_u),\\ t = 1,\\dots,T\\)\nHere spatial fields are different for each year, are independent and share range and sd.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-2---implementation-2",
    "href": "slides/slides_9.html#model-2---implementation-2",
    "title": "Lecture 9",
    "section": "Model 2 - Implementation",
    "text": "Model 2 - Implementation\nFit the model\n\n\n\ncmp = ~ Intercept(1) + \n  time(time, model = \"rw2\") +\n  space(geometry, model = spde, \n        replicate = time)\n\nlik = bru_obs(formula =  Y~ .,\n              data = dd)\n\nfit_2 = bru(cmp, lik)\n\n# plot time effect\n\np = fit_2$summary.random$time %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID, \n                  ymin = `0.025quant`,\n                  ymax = `0.975quant`), \n              alpha = 0.5) +\n  geom_line(aes(ID, mean))",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-2---implementation-3",
    "href": "slides/slides_9.html#model-2---implementation-3",
    "title": "Lecture 9",
    "section": "Model 2 - Implementation",
    "text": "Model 2 - Implementation\nSpace-time Predictions\n\npred_2 = predict(fit_2, pxl_time, ~ data.frame(mu = Intercept + space + time,\n                                      time_effect = time,\n                                      space_effect = space))",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#spatio-temporal-models-for-areal-data-9",
    "href": "slides/slides_9.html#spatio-temporal-models-for-areal-data-9",
    "title": "Lecture 9",
    "section": "Spatio temporal models for areal data",
    "text": "Spatio temporal models for areal data\nThe observation model\n\\[\nY_t(s)|\\mu_t(s)\\sim\\mathcal{N}(\\mu_t(s),\\sigma_y^2); \\qquad \\eta_t(s) =  \\mu_t(s)\n\\]\nWe are going to see 3 different models for the linear predictor \\(\\eta_{st}\\)\n\nModel 1: \\(\\eta_t(s) = \\beta_0 + u(s) + v_t\\) with \\(u(s)\\sim\\text{GF}(\\rho,\\sigma_u)\\)\nHere the time effect \\(v_t\\) is constant w.r.t. space and the space effect \\(u(s)\\) is constant w.r.t. time.\nModel 2: \\(\\eta_t(s) = \\beta_0 + u_t(s) + v_t\\) with \\(u_t(s)\\sim\\text{GF}(\\rho,\\sigma_u),\\ t = 1,\\dots,T\\)\nHere spatial fields are different for each year, are independent and share range and sd.\nModel 3: \\(\\eta_t(s) = \\beta_0 + u_t(s) + v_t\\) with \\[\nu_t(s) = \\phi\\ u_{t-1}(s)  + \\omega(s),\\text{ with }  \\omega(s)\\sim\\text{GF}(\\rho,\\sigma_u)\n\\]\nHere we are adding more temporal dependence in the spatial term, using a AR1 type model Here the spatial fields are different for each year, they are independent on each other.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-3---implementation-2",
    "href": "slides/slides_9.html#model-3---implementation-2",
    "title": "Lecture 9",
    "section": "Model 3 - Implementation",
    "text": "Model 3 - Implementation\nFit the model\n\n\n\nh.spec &lt;- list(rho = list(prior = 'pc.cor1', \n                          param = c(0, 0.9)))\ncmp = ~ Intercept(1) + \n  time(time, model = \"rw2\") +\n  space(geometry, model = spde, \n        group = time,\n        control.group = list(model = 'ar1', \n                             hyper = h.spec))\n\nlik = bru_obs(formula =  Y~ .,\n              data = dd)\n\nfit_3 = bru(cmp, lik)\n\n# plot time effect\n\np = fit_3$summary.random$time %&gt;% \n  ggplot() + \n  geom_ribbon(aes(ID, \n                  ymin = `0.025quant`,\n                  ymax = `0.975quant`), \n              alpha = 0.5) +\n  geom_line(aes(ID, mean))",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#model-3---implementation-3",
    "href": "slides/slides_9.html#model-3---implementation-3",
    "title": "Lecture 9",
    "section": "Model 3 - Implementation",
    "text": "Model 3 - Implementation\nSpace-time Predictions\n\npred_3 = predict(fit_3, pxl_time, ~ data.frame(mu = Intercept + space + time,\n                                      time_effect = time,\n                                      space_effect = space))",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#adding-covariates-to-space-time-models",
    "href": "slides/slides_9.html#adding-covariates-to-space-time-models",
    "title": "Lecture 9",
    "section": "Adding covariates to space-time models",
    "text": "Adding covariates to space-time models\n\nCovariates that only vary in space\n\nAltitude, bathimetry,‚Ä¶\n\nCovariates that vary in space and time\n\nTemperature, precipitation, ‚Ä¶",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-covariates",
    "href": "slides/slides_9.html#space-covariates",
    "title": "Lecture 9",
    "section": "Space covariates",
    "text": "Space covariates\n\nThey should be stored in a terra, SpatRaster object.\n\n\nlibrary(terra)\nlibrary(tidyterra)\n\n\ncov_space\n\nclass       : SpatRaster \nsize        : 147, 147, 1  (nrow, ncol, nlyr)\nresolution  : 4.465128, 3.11244  (x, y)\nextent      : -465.4573, 190.9165, 7031.885, 7489.413  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=23 +south +datum=WGS84 +units=km +no_defs \nsource(s)   : memory\nname        :      last \nmin value   : -2.090229 \nmax value   :  3.701765 \n\nggplot()  + geom_spatraster(data = cov_space) + geom_sf(data = dd) + scale_fill_scico()",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-covariates---implementation",
    "href": "slides/slides_9.html#space-covariates---implementation",
    "title": "Lecture 9",
    "section": "Space covariates - Implementation",
    "text": "Space covariates - Implementation\nSetting up and running the model\n\ncmp1 = ~ Intercept(1) +\n  time(time, model = \"rw2\") +\n  space(geometry, model = spde) +\n  cov(cov_space,  model = \"linear\")\n\nlik1 = bru_obs(formula =  Y~ .,\n              data = dd_1)\n\nfit_cov1 = bru(cmp1, lik1)\n\nLook at the results\n\nround(fit_cov1$summary.fixed,2)\n\n           mean   sd 0.025quant 0.5quant 0.975quant  mode kld\nIntercept -0.06 0.19      -0.45    -0.06       0.33 -0.06   0\ncov       -1.57 0.08      -1.73    -1.57      -1.42 -1.57   0",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-time-covariates",
    "href": "slides/slides_9.html#space-time-covariates",
    "title": "Lecture 9",
    "section": "Space-time covariates",
    "text": "Space-time covariates\nSome covariates change in both space and time.\nThere are several ways of doing this‚Ä¶.many quite confusing ü§™\nHere is one recipe !",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-time-covariates-1",
    "href": "slides/slides_9.html#space-time-covariates-1",
    "title": "Lecture 9",
    "section": "Space-time covariates",
    "text": "Space-time covariates\n\nHave the covariate as a multilier spatraster object\n\n\n# The covariate\ncov_space_time\n\nclass       : SpatRaster \nsize        : 147, 147, 12  (nrow, ncol, nlyr)\nresolution  : 4.465128, 3.11244  (x, y)\nextent      : -465.4573, 190.9165, 7031.885, 7489.413  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=utm +zone=23 +south +datum=WGS84 +units=km +no_defs \nsource(s)   : memory\nnames       :         1,         2,         3,         4,         5,         6, ... \nmin values  : -2.090229, -0.906177, -2.774797, -3.360599, -3.181559, -1.916012, ... \nmax values  :  3.701765,  2.061383,  1.949362,  1.168580,  1.228917,  2.057855, ... \n\n\n\n# The data frame\ndd_1[1:3,]\n\nSimple feature collection with 3 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -103.4191 ymin: 7460.922 xmax: -103.4191 ymax: 7460.922\nProjected CRS: +proj=utm +zone=23 +south +datum=WGS84 +units=km +no_defs +type=crs\n# A tibble: 3 √ó 3\n      Y  time             geometry\n  &lt;dbl&gt; &lt;dbl&gt;         &lt;POINT [km]&gt;\n1 1.24      1 (-103.4191 7460.922)\n2 0.919     5 (-103.4191 7460.922)\n3 0.668     6 (-103.4191 7460.922)\n\n\nNote The layers names in the raster are the same as the elements of the column time in the data.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-time-covariates-2",
    "href": "slides/slides_9.html#space-time-covariates-2",
    "title": "Lecture 9",
    "section": "Space-time covariates",
    "text": "Space-time covariates\nImplementation\n\ncmp1 = ~ Intercept(1) +\n  time(time, model = \"rw2\") +\n  space(geometry, model = spde) +\n  cov(eval_spatial(cov_space_time, .data.,  time),  model = \"linear\")\n\nlik1 = bru_obs(formula =  Y~ .,\n              data = dd_1)\n\nfit_cov1 = bru(cmp1, lik1)\n\nround(fit_cov1$summary.fixed)\n\n          mean sd 0.025quant 0.5quant 0.975quant mode kld\nIntercept    0  0         -1        0          0    0   0\ncov         -1  0         -1       -1         -1   -1   0\n\n\nThe .data. indicates the data that are input in the bru_obs() function",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-time-point-processes",
    "href": "slides/slides_9.html#space-time-point-processes",
    "title": "Lecture 9",
    "section": "Space-time Point Processes",
    "text": "Space-time Point Processes\n\nFor space-time point processes one can also use the group and replicate features\nThe one difference is that one has to define the integration scheme in both space and time!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-time-point-processes-1",
    "href": "slides/slides_9.html#space-time-point-processes-1",
    "title": "Lecture 9",
    "section": "Space-time Point Processes",
    "text": "Space-time Point Processes\n\nDefine the component (with the group feature)\n\n\ncmp &lt;- ~ Intercept(1) +\n  space_time(geometry,\n             model = matern,\n    group = season,\n    ngroup = 4 )\n\nlik =  bru_obs(\n    geometry + season ~ .,\n    family = \"cp\",\n    data = mrsea$points,\n    samplers = mrsea$samplers,\n    domain = list(\n      geometry = mrsea$mesh,\n      season = seq_len(4)\n    ))\n\nfit &lt;- bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-time-point-processes-2",
    "href": "slides/slides_9.html#space-time-point-processes-2",
    "title": "Lecture 9",
    "section": "Space-time Point Processes",
    "text": "Space-time Point Processes\n\nDefine the component (with the group feature)\nDefine the likelihood model\n\n\ncmp &lt;- ~ Intercept(1) +\n  space_time(geometry,\n             model = matern,\n    group = season,\n    ngroup = 4 )\n\nlik =  bru_obs(\n    geometry + season ~ .,\n    family = \"cp\",\n    data = mrsea$points,\n    samplers = mrsea$samplers,\n    domain = list(\n      geometry = mrsea$mesh,\n      season = seq_len(4)\n    ))\n\nfit &lt;- bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-time-point-processes-3",
    "href": "slides/slides_9.html#space-time-point-processes-3",
    "title": "Lecture 9",
    "section": "Space-time Point Processes",
    "text": "Space-time Point Processes\n\nDefine the component (with the group feature)\nDefine the likelihood model\n\n\nWhere is our process defined (space and time)\n\n\ncmp &lt;- ~ Intercept(1) +\n  space_time(geometry,\n             model = matern,\n    group = season,\n    ngroup = 4 )\n\nlik =  bru_obs(\n    geometry + season ~ .,\n    family = \"cp\",\n    data = mrsea$points,\n    samplers = mrsea$samplers,\n    domain = list(\n      geometry = mrsea$mesh,\n      season = seq_len(4)\n    ))\n\nfit &lt;- bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-time-point-processes-4",
    "href": "slides/slides_9.html#space-time-point-processes-4",
    "title": "Lecture 9",
    "section": "Space-time Point Processes",
    "text": "Space-time Point Processes\n\nDefine the component (with the group feature)\nDefine the likelihood model\n\n\nWhere is our process defined (space and time)\nDomain of integration\n\n\ncmp &lt;- ~ Intercept(1) +\n  space_time(geometry,\n             model = matern,\n    group = season,\n    ngroup = 4 )\n\nlik =  bru_obs(\n    geometry + season ~ .,\n    family = \"cp\",\n    data = mrsea$points,\n    samplers = mrsea$samplers,\n    domain = list(\n      geometry = mrsea$mesh,\n      season = seq_len(4)))\n\nfit &lt;- bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-time-point-processes-5",
    "href": "slides/slides_9.html#space-time-point-processes-5",
    "title": "Lecture 9",
    "section": "Space-time Point Processes",
    "text": "Space-time Point Processes\n\nDefine the component (with the group feature)\nDefine the likelihood model\nRun the model (as usual)\n\n\ncmp &lt;- ~ Intercept(1) +\n  space_time(geometry,\n             model = matern,\n    group = season,\n    ngroup = 4 )\n\nlik =  bru_obs(\n    geometry + season ~ .,\n    family = \"cp\",\n    data = mrsea$points,\n    samplers = mrsea$samplers,\n    domain = list(\n      geometry = mrsea$mesh,\n      season = seq_len(4)))\n\nfit &lt;- bru(cmp, lik)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#space-time-point-processes-6",
    "href": "slides/slides_9.html#space-time-point-processes-6",
    "title": "Lecture 9",
    "section": "Space-time Point Processes",
    "text": "Space-time Point Processes\nResults\n\nppxl &lt;- fm_pixels(mrsea$mesh, mask = mrsea$boundary, format = \"sf\")\nppxl_all &lt;- fm_cprod(ppxl, data.frame(season = seq_len(4)))\n\nlambda1 &lt;- predict( fit,ppxl_all,\n  ~ data.frame(season = season, lambda = exp(space_time + Intercept)))",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_9.html#non-separable-models-in-inlabru",
    "href": "slides/slides_9.html#non-separable-models-in-inlabru",
    "title": "Lecture 9",
    "section": "Non-separable models in inlabru",
    "text": "Non-separable models in inlabru\n\nSome non-separable models are implemented in inlabru. See\n\n\nLindgrenn et al., A diffusion-based spatio-temporal extension of Gaussian Matern &gt; fields (2024) SORT 48",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 9"
    ]
  },
  {
    "objectID": "slides/slides_12.html#motivation",
    "href": "slides/slides_12.html#motivation",
    "title": "Lecture 12",
    "section": "Motivation",
    "text": "Motivation\n\nCount data are often modeled with Poisson or Negative Binomial models.\n\nThese assume zeros occur naturally from the count process.\n\nExample: for Poisson is \\(P(Y=0) = \\exp(\\lambda)\\)\n\nBut in real data, we often see too many zeros ‚Üí zero inflation.\nExamples:\n\nMany people have 0 doctor visits per year.\n\nMost customers have 0 claims.\n\nMany species are absent from samples.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#two-main-solutions",
    "href": "slides/slides_12.html#two-main-solutions",
    "title": "Lecture 12",
    "section": "Two main solutions",
    "text": "Two main solutions\nBoth handle excess zeros but use different assumptions about how zeros are generated.\n\nZero-Inflated Models\nHurdle Models",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#two-main-solutions-1",
    "href": "slides/slides_12.html#two-main-solutions-1",
    "title": "Lecture 12",
    "section": "Two main solutions",
    "text": "Two main solutions\nBoth handle excess zeros but use different assumptions about how zeros are generated.\n\nZero-Inflated Models\n\nTwo sources of zero: inflation process or count process\n\nHurdle Models",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#two-main-solutions-2",
    "href": "slides/slides_12.html#two-main-solutions-2",
    "title": "Lecture 12",
    "section": "Two main solutions",
    "text": "Two main solutions\nBoth handle excess zeros but use different assumptions about how zeros are generated:\n\nZero-Inflated Models\n\nTwo sources of zero: inflation process or count process\n\nHurdle Models\n\nOnle one source of zeros",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#zero-inflated-models",
    "href": "slides/slides_12.html#zero-inflated-models",
    "title": "Lecture 12",
    "section": "Zero-Inflated Models",
    "text": "Zero-Inflated Models\nAssume two processes:\n\nA binary process ‚Üí decides if the observation is a certain zero.\nA count process ‚Üí generates counts, including zeros.\n\n\\[\n\\begin{aligned}\n\\text{P}(Y=0) & = \\pi + (1-\\pi)f(0) & \\\\\n\\text{P}(Y=y) & = (1-\\pi)f(y)& y=1,2,\\dots\n\\end{aligned}\n\\]\n\nNote In zero-inflated models \\(f(y)\\) is typically discrete (ex Poisson, Binomial, ‚Ä¶)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#hurdle-models",
    "href": "slides/slides_12.html#hurdle-models",
    "title": "Lecture 12",
    "section": "Hurdle Models",
    "text": "Hurdle Models\nAlso have two parts, but different logic:\n\nA binary model determines if the observation crosses the hurdle (i.e.¬†zero vs.¬†positive).\nA truncated count model models only positive counts.\n\nZeros come only from the first process.\n\\[\n\\begin{aligned}\n\\text{P}(Y=0) & = \\pi & \\\\\n\\text{P}(Y=y) & = \\frac{f(y)}{1-f(0)}& y=1,2,\\dots\n\\end{aligned}\n\\]\n\nNote In hurdle models \\(f_Y(y)\\) can also be continuous (f.ex Gamma). In these cases: \\[\n\\begin{aligned}\n\\text{P}(Y=0) & = \\pi & \\\\\nf_Y(y) & = f(y)& y&gt;0\n\\end{aligned}\n\\] where \\(f(y)\\) is scaled so that it integrates to \\(1-\\pi\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#comparison-zero-inflated-vs.-hurdle-models",
    "href": "slides/slides_12.html#comparison-zero-inflated-vs.-hurdle-models",
    "title": "Lecture 12",
    "section": "Comparison: Zero-Inflated vs.¬†Hurdle Models",
    "text": "Comparison: Zero-Inflated vs.¬†Hurdle Models\n\n\n\n\n\n\n\n\nFeature\nZero-Inflated\nHurdle\n\n\n\n\nZeros come from\nTwo sources (inflation + count)\nOne source (hurdle only)\n\n\nPositive part\nIncludes zeros\nTruncated at zero\n\n\nCan use continuous distributions?\n‚ùå Typically count only\n‚úÖ Yes (Gamma, Lognormal)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#zero-inflated-models-in-inlabru-type-1",
    "href": "slides/slides_12.html#zero-inflated-models-in-inlabru-type-1",
    "title": "Lecture 12",
    "section": "Zero Inflated models in inlabru (Type 1)",
    "text": "Zero Inflated models in inlabru (Type 1)\n\nFour zero-inflated models are implemented\n\nPoisson (zeroinflatedpoisson1)\nBinomial (zeroinflatedbinomial1)\nNegative Binomial (zeroinflatednbinomial1)\nBetaBinomial (zeroinflatedbinomial1)\n\n\nTo get details about the distributions you can type\n\nINLA::inla.doc(\"zero inflated\")\n\n\nThe probability of the inflation process \\(\\pi\\) is a hyperparameter therefore cannot be modeled with covariates",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example-number-of-fishes-caught-by-fishermen-at-a-state-park.",
    "href": "slides/slides_12.html#example-number-of-fishes-caught-by-fishermen-at-a-state-park.",
    "title": "Lecture 12",
    "section": "Example: number of fishes caught by fishermen at a state park.",
    "text": "Example: number of fishes caught by fishermen at a state park.\n\n\n  nofish persons child count\n1      1       1     0     0\n2      0       1     0     0\n3      0       1     0     0",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example-two-models",
    "href": "slides/slides_12.html#example-two-models",
    "title": "Lecture 12",
    "section": "Example: two models",
    "text": "Example: two models\n\\[\n\\begin{eqnarray}\n\\text{Model 1:   }\\ & Y\\sim \\text{Poisson}(\\lambda)\\\\\n\\text{Model 2:   }\\ & Y\\sim \\text{NegBinomial}(n,p)\\\\\n\\text{Linear predictor:   }\\ & \\eta = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\\end{eqnarray}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example-two-models-1",
    "href": "slides/slides_12.html#example-two-models-1",
    "title": "Lecture 12",
    "section": "Example: two models",
    "text": "Example: two models\n\\[\n\\begin{eqnarray}\n\\text{Model 1:   }\\ & Y\\sim \\text{Poisson}(\\lambda)\\\\\n\\text{Model 2:   }\\ & Y\\sim \\text{NegBinomial}(n,p)\\\\\n\\text{Linear predictor:   }\\ & \\eta = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\\end{eqnarray}\n\\]\nNegative Binomial distribution\n\\[\n\\text{Prob}(Y=y) = \\frac{\\Gamma(y+n)}{\\Gamma(n)\\Gamma(y+1)}p^n(1-p)^y\n\\] with \\[\nE(Y) = \\mu = n\\frac{1-p}{p} \\qquad \\text{Var}(Y) = \\mu(1+\\frac{\\mu}{n})\n\\] and linear predictor \\(\\eta = log(\\mu)\\)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example-implementation",
    "href": "slides/slides_12.html#example-implementation",
    "title": "Lecture 12",
    "section": "Example: Implementation",
    "text": "Example: Implementation\n\n\nThe Model \\[\n\\begin{aligned}\n\\text{Model 1 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{Pois}(\\lambda(\\eta_i))\\\\\n\\text{Model 2 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{NegBin}(n(\\eta_i),p(\\eta_i))\\\\\n\\text{Linear Predictor }: \\eta_i & = \\color{red}{\\boxed{\\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}}}\n\\end{aligned}\n\\]\n\n\nzinb[1:3,]\n\n  nofish persons child count\n1      1       1     0     0\n2      0       1     0     0\n3      0       1     0     0\n\n\n\nThe code\n\n# define model components\ncmp = ~ Intercept(1) + fishing(nofish, model = \"linear\") + \n  persons(persons, model = \"linear\") + \n  child(child, model = \"linear\")\n\n# define model predictor\nformula = count ~ .\n\n# build the observation model\n# Poisson model\nlik_pois = bru_obs(formula,\n                   family = \"zeroinflatedpoisson1\",\n                   data = zinb) \n\n# Negative binomial model\nlik_nbin = bru_obs(formula,\n                   family = \"zeroinflatednbinomial1\",\n                   data = zinb) \n\n# fit the model\nfit_pois = bru(cmp, lik_pois)\nfit_nbin = bru(cmp, lik_nbin)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example-implementation-1",
    "href": "slides/slides_12.html#example-implementation-1",
    "title": "Lecture 12",
    "section": "Example: Implementation",
    "text": "Example: Implementation\n\n\nThe Model \\[\n\\begin{aligned}\n\\text{Model 1 }:  \\color{red}{\\boxed{y_i|\\eta_i}} & \\color{red}{\\boxed{\\sim \\pi1_{(y=0)} + (1-\\pi)\\text{Pois}(\\lambda(\\eta_i))}}\\\\\n\\text{Model 2 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{NegBin}(n(\\eta_i),p(\\eta_i))\\\\\n\\text{Linear Predictor }: \\eta_i & = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n\\end{aligned}\n\\]\n\n\nzinb[1:3,]\n\n  nofish persons child count\n1      1       1     0     0\n2      0       1     0     0\n3      0       1     0     0\n\n\n\nThe code\n\n# define model components\ncmp = ~ Intercept(1) + fishing(nofish, model = \"linear\") + \n  persons(persons, model = \"linear\") + \n  child(child, model = \"linear\")\n\n# define model predictor\nformula = count ~ .\n\n# build the observation model\n# Poisson model\nlik_pois = bru_obs(formula,\n                   family = \"zeroinflatedpoisson1\",\n                   data = zinb) \n\n# Negative binomial model\nlik_nbin = bru_obs(formula,\n                   family = \"zeroinflatednbinomial1\",\n                   data = zinb) \n\n# fit the model\nfit_pois = bru(cmp, lik_pois)\nfit_nbin = bru(cmp, lik_nbin)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example-implementation-2",
    "href": "slides/slides_12.html#example-implementation-2",
    "title": "Lecture 12",
    "section": "Example: Implementation",
    "text": "Example: Implementation\n\n\nThe Model \\[\n\\begin{aligned}\n\\text{Model 1 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{Pois}(\\lambda(\\eta_i))\\\\\n\\text{Model 2 }:  \\color{red}{\\boxed{y_i|\\eta_i}} & \\color{red}{\\boxed{\\sim \\pi1_{(y=0)} + (1-\\pi)\\text{NegBin}(n(\\eta_i),p(\\eta_i))}}\\\\\n\\text{Linear Predictor }: \\eta_i & = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n\\end{aligned}\n\\]\n\n\nzinb[1:3,]\n\n  nofish persons child count\n1      1       1     0     0\n2      0       1     0     0\n3      0       1     0     0\n\n\n\nThe code\n\nbru_options_set(control.compute = list(dic = T, waic = T, mlik = T))\n# define model components\ncmp = ~ Intercept(1) + fishing(nofish, model = \"linear\") + \n  persons(persons, model = \"linear\") + \n  child(child, model = \"linear\")\n\n# define model predictor\nformula = count ~ .\n\n# build the observation model\n# Poisson model\nlik_pois = bru_obs(formula,\n                   family = \"zeroinflatedpoisson1\",\n                   data = zinb) \n\n# Negative binomial model\nlik_nbin = bru_obs(formula,\n                   family = \"zeroinflatednbinomial1\",\n                   data = zinb) \n\n# fit the model\nfit_pois = bru(cmp, lik_pois)\nfit_nbin = bru(cmp, lik_nbin)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#results---fixed-effects",
    "href": "slides/slides_12.html#results---fixed-effects",
    "title": "Lecture 12",
    "section": "Results - Fixed effects",
    "text": "Results - Fixed effects\n\n\n[1] \"Poisson Model\"\n\n\n           mean   sd 0.025quant 0.975quant\nIntercept  0.22 0.15      -0.08       0.51\nfishing   -0.90 0.12      -1.13      -0.67\npersons    0.65 0.05       0.56       0.74\nchild     -0.95 0.10      -1.15      -0.76\n\n\n[1] \"Negative Binomial Model\"\n\n\n           mean   sd 0.025quant 0.975quant\nIntercept -0.80 0.31      -1.42      -0.18\nfishing   -0.59 0.25      -1.08      -0.10\npersons    0.94 0.12       0.71       1.17\nchild     -1.65 0.19      -2.03      -1.27",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#results---hyperparameters-and-scores",
    "href": "slides/slides_12.html#results---hyperparameters-and-scores",
    "title": "Lecture 12",
    "section": "Results - Hyperparameters and scores",
    "text": "Results - Hyperparameters and scores\nHyperparameters\n\n\n[1] \"Poisson Model\"\n\n\n                                                       mean   sd\nzero-probability parameter for zero-inflated poisson_1 0.48 0.04\n\n\n[1] \"Negative Binomial Model\"\n\n\n                                                         mean   sd\nsize for nbinomial_1 zero-inflated observations          0.56 0.10\nzero-probability parameter for zero-inflated nbinomial_1 0.06 0.06\n\n\nScores\n\n\n  Score Poisson    Nbin\n1   DIC 1111.36  795.44\n2  Mlik -579.97 -424.72\n3  WAIC 1207.12  798.86\n\n\n\nThere seems to be more overdispersion than excess zero..the negative binomial model better fits the data estimateing a small excess of zero.",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#comparing-estimated-distribution-for-a-specific-data-point",
    "href": "slides/slides_12.html#comparing-estimated-distribution-for-a-specific-data-point",
    "title": "Lecture 12",
    "section": "Comparing estimated distribution for a specific data point",
    "text": "Comparing estimated distribution for a specific data point\n\\[\n\\begin{eqnarray}\n\\text{Model 1:   }\\ & P(Y=0) &= \\pi + (1-\\pi)\\ \\text{Poisson}(\\lambda)\\\\\n\\text{Model 2:   }\\ & P(Y=0)&= \\pi + (1-\\pi)\\ \\text{NegBinom}(n,p)\\\\\n\\end{eqnarray}\n\\] We check the distribution for one person, non-fisher and no children",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#comparing-estimated-distribution---implementation",
    "href": "slides/slides_12.html#comparing-estimated-distribution---implementation",
    "title": "Lecture 12",
    "section": "Comparing estimated distribution - Implementation",
    "text": "Comparing estimated distribution - Implementation\n\ndf_pred = data.frame(nofish = 0, persons = 1, child = 0)\n\nprob_pois = predict(fit_pois, df_pred,\n        formula = ~ {\n          pi0 &lt;- zero_probability_parameter_for_zero_inflated_poisson_1\n          lambda = exp(Intercept +  fishing + persons + child)\n          yy = 0:20\n          list(\n            prob0 =  pi0 * c(1,rep(0,20)) + (1-pi0) * dpois(yy, lambda = lambda))\n          } )\n\nprob_nbin = predict(fit_nbin, df_pred,\n        formula = ~ {\n          pi0 &lt;- zero_probability_parameter_for_zero_inflated_nbinomial_1\n          size = exp(size_for_nbinomial_1_zero_inflated_observations)\n          mu = exp(Intercept +  fishing + persons + child)\n          p = size/(size + mu)\n          yy = 0:20\n          list(prob0 = pi0 * c(1,rep(0,20)) + (1-pi0) * dnbinom(yy,size = size, mu = mu))\n        })",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#comparing-estimated-distribution---implementation-1",
    "href": "slides/slides_12.html#comparing-estimated-distribution---implementation-1",
    "title": "Lecture 12",
    "section": "Comparing estimated distribution - Implementation",
    "text": "Comparing estimated distribution - Implementation\nNote: To get the names to input in predict use the bru_standardise_names() function:\n\ninlabru::bru_standardise_names(fit_pois)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#hurdle-models-in-inlabru",
    "href": "slides/slides_12.html#hurdle-models-in-inlabru",
    "title": "Lecture 12",
    "section": "Hurdle Models in inlabru",
    "text": "Hurdle Models in inlabru\nThere are two ways to implement hurdle models in inlabru\n\nStrategy 1 Use a modified version of the same distributions defined for the zero-inflated models\nStrategy 2 Use a ‚Äútwo-likelihood‚Äù trick",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#strategy-1---implemented-models",
    "href": "slides/slides_12.html#strategy-1---implemented-models",
    "title": "Lecture 12",
    "section": "Strategy 1 - Implemented models",
    "text": "Strategy 1 - Implemented models\nAvailable models (Type 0)\n\nPoisson (zeroinflatedpoisson0)\nBinomial (zeroinflatedbinomial0)\nNegative Binomial (zeroinflatednbinomial0)\nBetaBinomial (zeroinflatedbinomial0)\n\n\nAdvantages\n\nIt works exactly as the zero-inflated models\nNo need for extra coding\n\n\n\nDisadvantages\n\nCan only be used for the models that are already implemented\nThe probability of zero (\\(\\pi\\)) is fixed and cannot have covariates",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example-strategy-1-implementation",
    "href": "slides/slides_12.html#example-strategy-1-implementation",
    "title": "Lecture 12",
    "section": "Example Strategy 1: Implementation",
    "text": "Example Strategy 1: Implementation\n\n\nThe Model \\[\n\\begin{aligned}\n\\text{Model 1 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{TruncPois}(\\lambda(\\eta_i))\\\\\n\\text{Model 2 }:  y_i|\\eta_i & \\sim \\pi1_{(y=0)} + (1-\\pi)\\text{TruncNegBin}(n(\\eta_i),p(\\eta_i))\\\\\n\\text{Linear Predictor }: \\eta_i & = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i}\n\\end{aligned}\n\\]\n\n\nzinb[1:3,]\n\n  nofish persons child count\n1      1       1     0     0\n2      0       1     0     0\n3      0       1     0     0\n\n\n\nThe code\n\n# define model components\ncmp = ~ Intercept(1) + fishing(nofish, model = \"linear\") + \n  persons(persons, model = \"linear\") + \n  child(child, model = \"linear\")\n\n# define model predictor\nformula = count ~ .\n\n# build the observation model\n# Poisson model\nlik_pois = bru_obs(formula,\n                   family = \"zeroinflatedpoisson0\",\n                   data = zinb) \n\n# Negative binomial model\nlik_nbin = bru_obs(formula,\n                   family = \"zeroinflatednbinomial0\",\n                   data = zinb) \n\n# fit the model\nfit_pois0 = bru(cmp, lik_pois)\nfit_nbin0 = bru(cmp, lik_nbin)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example-strategy-1-results",
    "href": "slides/slides_12.html#example-strategy-1-results",
    "title": "Lecture 12",
    "section": "Example Strategy 1: Results",
    "text": "Example Strategy 1: Results",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#strategy-2---two-likelihood-trick",
    "href": "slides/slides_12.html#strategy-2---two-likelihood-trick",
    "title": "Lecture 12",
    "section": "Strategy 2 - Two-likelihood trick",
    "text": "Strategy 2 - Two-likelihood trick\nAdvantages\n\nMore flexible model\nCan model also the probability of zero \\(\\pi\\)\nCan also be used for continuous distributions with mass at 0\n\n\nDisadvantage\n\nSome more coding",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#hurdle-models-com",
    "href": "slides/slides_12.html#hurdle-models-com",
    "title": "Lecture 12",
    "section": "Hurdle models com",
    "text": "Hurdle models com\nThe Hurdle Model handles this by splitting the data-generating process into two parts:\n\nZero-hurdle component: This part is a binary model that estimates the probability of a zero count versus a non-zero count.\nPositive component: This part models the distribution of the positive (non-zero) values.\n\n\nThis idea can be used in inlabru by using two likelihood\n\nA binomial likelihood to model the 0/1 process\nA continuous positive density or a truncated count distribution for the positive part",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example---gamma-hurdle-model",
    "href": "slides/slides_12.html#example---gamma-hurdle-model",
    "title": "Lecture 12",
    "section": "Example - Gamma-Hurdle model",
    "text": "Example - Gamma-Hurdle model\nDaily precipitation in Parana state\n\n\n\\[\nz_{st} = \\left\\{\n\\begin{eqnarray}\n0 & \\text{ if } &\\text{ no rain on day } t \\text{ at station } s\\\\\n1 & \\text{ if } &\\text{  rain on day } t \\text{ at station } s\n\\end{eqnarray}\n\\right.\n\\]\n\\[\ny_{st} = \\left\\{\n\\begin{eqnarray}\n\\text{NA} \\text{ if } && \\text{ no rain on day } t  \\text{ at station } s\\\\\n\\text{rain amout} \\text{ if } && \\text{  rain on day } t  \\text{ at station } s\n\\end{eqnarray}\n\\right.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\nz_{st}\\sim \\text{Binom}(p_{st}, n_{st} = 1); \\qquad y_{st}|z_{st}=1\\sim\\text{Gamma}(a,b) \\ \\text{with}\\  E(y_{st}) = \\mu_{st} =  a/b\n\\] Where \\[\n\\begin{eqnarray}\n\\eta_t^1 & = \\text{logit}(p_t)  & = \\beta_0^B+u^B(s) \\\\\n\\eta_t^2 & = \\log(\\mu_t)& = \\beta_0^G+u^G(s)\n\\end{eqnarray}\n\\]",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example---gamma-hurdle-model-1",
    "href": "slides/slides_12.html#example---gamma-hurdle-model-1",
    "title": "Lecture 12",
    "section": "Example - Gamma-Hurdle model",
    "text": "Example - Gamma-Hurdle model\nImplementation\n\nCreate response for binary and Gamma process\n\n\ndf = df %&gt;%\n  mutate(z = ifelse(value==0,0,1),\n         y = ifelse(value==0,NA, value),)\n\n\nDefine components, observation models and run\n\n\ncmp = ~ -1 + Intercept_z(1) + Intercept_y(1) +\n  space_z(geometry, model = spde) +\n  space_y(geometry, model = spde) +\n  local_z(station, model = \"iid\")\n\nlik_z = bru_obs(formula = z ~ Intercept_z + space_z + local_z,\n                data = df %&gt;% filter(time==1),\n                family = \"binomial\")\n\nlik_y = bru_obs(formula = y ~ Intercept_y + space_y,\n                data =  df %&gt;% filter(time==1),\n                family = \"gamma\")\n\nfit = bru(cmp, lik_z, lik_y)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example---gamma-hurdle-model-2",
    "href": "slides/slides_12.html#example---gamma-hurdle-model-2",
    "title": "Lecture 12",
    "section": "Example - Gamma-Hurdle model",
    "text": "Example - Gamma-Hurdle model\nResults",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example---gamma-hurdle-model-3",
    "href": "slides/slides_12.html#example---gamma-hurdle-model-3",
    "title": "Lecture 12",
    "section": "Example - Gamma-Hurdle model",
    "text": "Example - Gamma-Hurdle model\nWe can:\n\nuse other components in both linear predictors \\(\\eta^1\\) and \\(\\eta^2\\)\nhave shared components between the two linear predictor\nuse other positive-continuous distribution instead of the Gamma (for example log-Normal, Weibull, etc.)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example---poisson--hurdle-model",
    "href": "slides/slides_12.html#example---poisson--hurdle-model",
    "title": "Lecture 12",
    "section": "Example - Poisson- Hurdle model",
    "text": "Example - Poisson- Hurdle model\nLet‚Äôs look again at the fishes examples. We now want to include covariates also in the model for the zero probability:\n\\[\n\\begin{eqnarray}\nP(Y = 0) = &\\pi\\\\\nP(Y=y) = & (1-\\pi) \\frac{f_Y(y)}{f_Y(0)},& \\qquad y =1,2,\\dots\n\\end{eqnarray}\n\\] We use the same ‚Äútrick‚Äù as before and we now have \\[\n\\begin{eqnarray}\nz\\sim\\text{Binomial}(\\pi,n); &\\qquad \\eta_1 = \\text{logit}(\\pi) = \\beta^1X\\\\\nz\\sim\\text{Truncated Poisson}(\\lambda); &\\qquad \\eta_2 =\\log(\\lambda) =  \\beta^2X\\\\\n\\end{eqnarray}\n\\] In inlabru the truncated Poisson distribution is called nzpoisson",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example---poisson--hurdle-model-1",
    "href": "slides/slides_12.html#example---poisson--hurdle-model-1",
    "title": "Lecture 12",
    "section": "Example - Poisson- Hurdle model",
    "text": "Example - Poisson- Hurdle model\nImplementation\n\nDefine response for binary and truncated poisson process\n\n\nzinb  = zinb %&gt;%\n  mutate(z = ifelse(count==0,1,0),\n         y = ifelse(count==0,NA, count))\n\n\nDefine components, observation models and run\n\n\ncmp = ~ -1 + Intercept_z(1) + Intercept_y(1) +\n  nofish_z(nofish, model = \"linear\") + persons_z(persons, model = \"linear\") +\n    nofish_y(nofish, model = \"linear\") + persons_y(persons, model = \"linear\")\n\nlik_z = bru_obs(formula = z~ Intercept_z  + nofish_z + persons_z,\n                data = zinb,\n                family = \"binomial\")\n\nlik_y = bru_obs(formula = y~ Intercept_y  + nofish_y + persons_y,\n                data = zinb,\n                family = \"nzpoisson\")\n\nfit = bru(cmp, lik_z, lik_y)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example---poisson--hurdle-model-2",
    "href": "slides/slides_12.html#example---poisson--hurdle-model-2",
    "title": "Lecture 12",
    "section": "Example - Poisson- Hurdle model",
    "text": "Example - Poisson- Hurdle model\nResults\n\nround(fit$summary.fixed,2)\n\n             mean   sd 0.025quant 0.5quant 0.975quant  mode kld\nIntercept_z  0.70 0.33       0.04     0.70       1.35  0.70   0\nnofish_z     0.23 0.28      -0.33     0.23       0.78  0.23   0\npersons_z   -0.18 0.12      -0.41    -0.18       0.04 -0.18   0\nIntercept_y  0.35 0.15       0.05     0.35       0.65  0.35   0\nnofish_y    -0.83 0.12      -1.07    -0.83      -0.59 -0.83   0\npersons_y    0.53 0.04       0.44     0.53       0.61  0.53   0",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example-binomial-hurdle-model",
    "href": "slides/slides_12.html#example-binomial-hurdle-model",
    "title": "Lecture 12",
    "section": "Example: Binomial-Hurdle model",
    "text": "Example: Binomial-Hurdle model\n\nIn inlabru there is no implementation for the truncated binomial (or negative binomial) distribution\nWe can ‚Äútrick‚Äù inlabru by using one of the implemented Type 0 models \\[\ny_i|\\eta_i  \\sim \\pi1_{(y=0)} + (1-\\pi)\\frac{f(y)}{f(0)}\n\\] with a fixed and small \\(\\pi\\) thus creating a truncated distribution!",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#example-binomial-hurdle-model-1",
    "href": "slides/slides_12.html#example-binomial-hurdle-model-1",
    "title": "Lecture 12",
    "section": "Example: Binomial-Hurdle model",
    "text": "Example: Binomial-Hurdle model\nImplementation\n\nDefine response and the components\n\n\n# Define response for binary and truncated neg. binomial\nzinb  = zinb %&gt;% mutate(z = ifelse(count==0,1,0),\n                        y = ifelse(count==0,NA, count))\n# define model components\ncmp = ~ -1 + Intercept_z(1) + Intercept_y(1) + \n  fishing_z(nofish, model = \"linear\") + fishing_y(nofish, model = \"linear\") + \n  persons_z(persons, model = \"linear\") + persons_y(persons, model = \"linear\") + \n  child_z(child, model = \"linear\") + child_y(child, model = \"linear\")\n\n\n\nBuild the likelihood for the binomial part\n\n\n# build the observation model\n# Poisson model\nlik_binom = bru_obs(z ~ Intercept_z + fishing_z + persons_z + child_z ,\n                   family = \"binomial\",\n                   data = zinb) \n\n\n\n\nDefine the type 0 Negbinomial with \\(\\pi\\) fixed to a small value\n\n\n# Negative binomial model\nlik_trunc_nbin = bru_obs(y ~ Intercept_y + fishing_y + persons_y + child_y ,\n                   family = \"zeroinflatednbinomial0\",\n                   data = zinb,\n                   control.family = list(hyper = list(theta = list(initial = -20, \n                                                                   fixed = TRUE))))\n\n\n\n\nRun the model\n\n\nfit_nbin = bru(cmp, lik_binom, lik_trunc_nbin)",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#summary---treating-many-zero-with-inlabru",
    "href": "slides/slides_12.html#summary---treating-many-zero-with-inlabru",
    "title": "Lecture 12",
    "section": "Summary - Treating many zero with inlabru",
    "text": "Summary - Treating many zero with inlabru\nThere are mainly two types of models to treat excess of zeros\n\nZero inflated models\nHurdle models",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-1",
    "href": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-1",
    "title": "Lecture 12",
    "section": "Summary - Treating many zero with inlabru",
    "text": "Summary - Treating many zero with inlabru\nThere are mainly two types of models to treat excess of zeros\n\nZero inflated models\n\nZeros come from two sources\nCan only be used for counts (Poisson, Binomial,‚Ä¶)\nFour likelihood are implemented in inlabru (Type1)\nThe probability of zero-inflation is constant\n\nHurdle models",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-2",
    "href": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-2",
    "title": "Lecture 12",
    "section": "Summary - Treating many zero with inlabru",
    "text": "Summary - Treating many zero with inlabru\nThere are mainly two types of models to treat excess of zeros\n\nZero inflated models\nHurdle models\n\nOnly one source of zeros\nPositive values can be both discrete and continuous\nTwo ways to imlement in inlabru",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  },
  {
    "objectID": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-3",
    "href": "slides/slides_12.html#summary---treating-many-zero-with-inlabru-3",
    "title": "Lecture 12",
    "section": "Summary - Treating many zero with inlabru",
    "text": "Summary - Treating many zero with inlabru\nThere are mainly two types of models to treat excess of zeros\n\nZero inflated models\nHurdle models\n\nOnly one source of zeros\nPositive values can be both discrete and continuous\nTwo ways to imlement in inlabru\n\nUse the implemented Type0 likelihoods\n\n\n\nProbability of zero is constant\n\n\nUse the ‚Äútwo-likelihood‚Äù approach\n\n\nProbability of zero can be modelled",
    "crumbs": [
      "Home",
      "Slides",
      "Lecture 12"
    ]
  }
]