---
title: "Lecture 13"
from: markdown+emoji
subtitle: "Introduction to Distance sampling with `inlabru`" 
format:
  revealjs:
    margin: 0
    logo:  NTNU_UofG.png
    theme: uofg_theme.scss
    header-includes: |
      <script src="custom.js" type="application/javascript"></script>
slide-number: "c/t"
title-slide-attributes:
#    data-background-image: images/trondheim3.png
    data-background-size: cover
    data-background-opacity: "0.55"
author:
  - name: Sara Martino
    #orcid: 0000-0002-6879-4412
    email: sara.martino@ntnu.no
    affiliations: Dept. of Mathematical Science, NTNU
  - name: Janine Illian
    #orcid: 0000-0002-6879-4412
    email: Janine.Illian@glasgow.ac.uk
    affiliations: University of Glasgow 
  - name: Jafet Belmont
    #orcid: 0000-0002-6879-4412
    email: Jafet.Belmont@glasgow.ac.uk
    affiliations: University of Glasgow   
        
# date: May 22, 2025
# bibliography: references.bib
embed-resources: true
execute:
  allow-html: true
  freeze: auto
  cache: false
---

```{r setup}
#| include: false
library(knitr)    # For knitting document and include_graphics function
library(ggplot2)  # For plotting
library(png) 
library(tidyverse)
library(INLA)
library(BAS)
library(patchwork)
library(inlabru)
library(sf)
library(scico)
library(gt)
```

# Distance Sampling

## Overview of Distance Sampling {.smaller background-color="#FFFFFF"}

-   Distance sampling is a family of related methods for estimating the abundance and spatial distribution of wild populations.

-   Distance sampling is based on the idea that animals further away from observers are harder to detect than animals that are nearer.

::::: columns
::: {.column width="40%"}
-   This idea is implemented in the model as a detection function that depends on distance.

    -   Animals at greater distances are harder to detect and the detection function therefore declines as distance increases.
:::

::: {.column width="60%"}
![](figures/distance-animation.gif){fig-align="center" width="623"}
:::
:::::

## Density surface models {.smaller auto-animate="true" background-color="#FFFFFF"}

Coupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.

-   Traditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data.

![](figures/dsm_approach.png){fig-align="center"}

## Density surface models {.smaller auto-animate="true" background-color="#FFFFFF"}

Coupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.

-   Traditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data.

-   This requires binning the data into counts based on some discretisation of space.

![](figures/discrete_DS.png){fig-align="center" width="544"}

## Density surface models {.smaller auto-animate="true" background-color="#FFFFFF"}

Coupling distance sampling data with spatial modelling allows maps of spatially varying density to be produced.

-   Traditionally, this is achieved in a two-stages approach by (i) using a detectability point estimates to create an offset vector to (ii) use within GLM or GAM for count response data.

-   A major downside to this approach is the **propagation of uncertainty** from the detection model to the second-stage spatial model.

::: fragment
-   **The goal**: one-stage distance sampling model, simultaneously estimating the detectability and the spatial distribution of animals using a *thinned point process framework*.

![](figures/inlabru_DS.png){fig-align="center" width="500"}
:::

# Thinned Point Process

## Thinned Point Process {.smaller}

::: {style="height: 50px;"}
:::

The LGCP is a flexible approach that can include spatial covariates to model the mean intensity and a mean-zero spatially structured random effect to account for unexplained heterogeneity not captured by the covariates.

-   To account for the imperfect detection of points we specify a thinning probability function $$
    g(s) = \text{Prob}(\text{a point at s is detected}|\text{a point is at s})
    $$

-   A key property of LGCP is that a realisation of a point process with intensity $\lambda(s)$ that is thinned by probability function $g(s)$, follows also a LGCP with intensity:

$$
\underbrace{\tilde{\lambda}(s)}_{\text{observed process}} = \underbrace{\lambda(s)}_{\text{true process}} \times \underbrace{g(s)}_{\text{thinning probability}}
$$

## Thinned Point Process {.smaller auto-animate="true" background-color="#FFFFFF"}

Lets visualize this on 1D: Intensity function with [points]{style="color:grey;"}

![](figures/densityrug-1.png){fig-align="center"}

## Thinned Point Process {.smaller auto-animate="true" background-color="#FFFFFF"}

Intensity (density) function with [points]{style="color:grey;"} and transect locations

![](figures/densityrugtrans-1.png){fig-align="center"}

## Thinned Point Process {.smaller auto-animate="true" background-color="#FFFFFF"}

::: incremental
-   Detection function $\color{red}{g(s)}$

-   Here $\color{red}{g(s) =1}$ on the transects (at x = 10,30 and 50).
:::

![](figures/detfun-1.png){fig-align="center"}

## Thinned Point Process {.smaller auto-animate="true" background-color="#FFFFFF"}

-   Detection function $\color{red}{g(s)}$ and [detected points]{style="color:grey;"}

![](figures/detfundets-1.png){fig-align="center"}

## Thinned Point Process {.smaller auto-animate="true" background-color="#FFFFFF"}

![](figures/thinning1-1.png){fig-align="center"}

## Thinned Point Process {.smaller auto-animate="true" background-color="#FFFFFF"}

![](figures/thinning2-1.png){fig-align="center"} The detection function describes the probability $\color{red}{p(s)}$ that an point is detected

## Thinned Point Process {.smaller auto-animate="true" background-color="#FFFFFF"}

![](figures/thinning3-1.png){fig-align="center"}

## Thinned Point Process {.smaller auto-animate="true" background-color="#FFFFFF"}

![](figures/thinning4-1.png){fig-align="center"}

Observations are from a thinned Poisson process with intensity $\lambda(s) \color{red}{p(s)}$

## Detection Function {.smaller auto-animate="true" background-color="#FFFFFF"}

::: {style="height: 50px;"}
:::

-   Standard distance sampling approaches specify $g(s)$ as a function that declines with increasing distance
    -   horizontal distance to the observer for *point transects*
    -   perpendicular distance to the transect line for *line transects*

![](figures/whale_watch.png){fig-align="center" width="398"}

## Detection Function {.smaller auto-animate="true" background-color="#FFFFFF"}

::: {style="height: 50px;"}
:::

-   Standard distance sampling approaches specify $g(s)$ as a function that declines with increasing distance
    -   horizontal distance to the observer for *point transects*
    -   [perpendicular distance to the transect line for *line transects*]{style="color:red;"}

![](figures/whale_watch.png){fig-align="center" width="448"}

## Detection Function {.smaller auto-animate="true" background-color="#FFFFFF"}

::: {style="height: 50px;"}
:::

-   Standard distance sampling approaches specify $g(s)$ as a function that declines with increasing distance
    -   horizontal distance to the observer for *point transects*
    -   perpendicular distance to the transect line for *line transects*
-   The thinning probability function is specified as a parametric family of functions.

::::: columns
::: {.column width="40%"}
**Half-normal**: $g(\mathbf{s}|\sigma) = \exp(-0.5 (d(\mathbf{s})/\sigma)^2)$

**Hazard-rate** :$g(\mathbf{s}|\sigma) = 1 - \exp(-(d(\mathbf{s})/\sigma)^{-1})$
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| fig-width: 4.6
#| fig-height: 3.5


library(ggplot2)
# Parameters
sigma <- 50
b <- 2
distances <- seq(0, 150, length.out = 200)

# Create data
df <- data.frame(
  distance = rep(distances, 2),
  probability = c(exp(-distances^2 / (2 * sigma^2)),
                  1 - exp(-(distances/sigma)^(-b))),
  type = rep(c("Half-normal", "Hazard-rate"), each = 200)
)

# Simple plot with linetypes
ggplot(df, aes(x = distance, y = probability, color = type, linetype = type)) +
  geom_line(size = 1) +
  labs(
    x = "Distance",
    y = "Detection Probability",
    color = "Type",
    linetype = "Type"
  ) +
  scale_color_manual(values = c("#2E86AB", "#E76F51")) +
  scale_linetype_manual(values = c("solid", "dashed")) +
  theme_minimal()
```
:::
:::::

## Detection Function {.smaller auto-animate="true" background-color="#FFFFFF"}

::: {style="height: 50px;"}
:::

-   Standard distance sampling approaches specify $g(s)$ as a function that declines with increasing distance
    -   horizontal distance to the observer for *point transects*
    -   perpendicular distance to the transect line for *line transects*
-   The thinning probability function is specified as a parametric family of functions.
-   The thinned-LGCP likelihood is given by:

$$
\pi(\mathbf{s_1},\ldots,\mathbf{s_m}) = \exp\left( |\Omega| - \int_{\mathbf{s}\in\Omega}\lambda(s)g(s)\text{d}s \right) \prod_{i=1}^m \lambda(\mathbf{s}_i)g(\mathbf{s}_i)
$$

-   To make $g(s)$ and $\lambda(s)$ identifiable, we assume intensity is constant with respect to distance from the observer.

    -   In practice this means we assume animals are uniformly distributed with respect to distance from the line

## Putting all the pieces together {.smaller auto-animate="true" background-color="#FFFFFF"}

::::: columns
::: {.column width="40%"}
![](figures/thinweeds0-1.png){fig-align="center"}
:::

::: {.column width="60%"}
-   The true point pattern $Y = \mathbf{s}_1,\ldots,\mathbf{s}_n$ are a realization of a Point process with intensity $\lambda(s)$
:::
:::::

## Putting all the pieces together {.smaller auto-animate="true" background-color="#FFFFFF"}

::::: columns
::: {.column width="40%"}
![](figures/thinweeds1-1.png){fig-align="center"}
:::

::: {.column width="60%"}
-   The true point pattern $Y = \mathbf{s}_1,\ldots,\mathbf{s}_n$ are a realization of a Point process with intensity $\lambda(s)$
-   We design a sampling survey to collect the data along transects
:::
:::::

## Putting all the pieces together {.smaller auto-animate="true" background-color="#FFFFFF"}

::::: columns
::: {.column width="40%"}
![](figures/thinweeds2-1.png){fig-align="center"}
:::

::: {.column width="60%"}
-   The true point pattern $Y = \mathbf{s}_1,\ldots,\mathbf{s}_n$ are a realization of a Point process with intensity $\lambda(s)$
-   We design a sampling survey to collect the data along transects
-   [detected points]{style="color:grey;"} are generated from the thinned PP with intensity $\color{red}{\tilde{\lambda}(s)}= \lambda(s)\color{red}{g(d(s))}$
    -   The **log intensity** $\log \color{red}{\tilde{\lambda}(s)} = \overbrace{\log \lambda (s)}^{\mathbf{x}'\beta + \xi(s)} + \overbrace{\log \color{red}{g(d(s))}}^{-0.5~d(\mathbf{s})^2\sigma^{-2}}$
:::
:::::

## Putting all the pieces together {.smaller auto-animate="true" background-color="#FFFFFF"}

::::: columns
::: {.column width="40%"}
![](figures/thinweeds2-1.png){fig-align="center"}
:::

::: {.column width="60%"}
-   The true point pattern $Y = \mathbf{s}_1,\ldots,\mathbf{s}_n$ are a realization of a Point process with intensity $\lambda(s)$
-   We design a sampling survey to collect the data along transects
-   [detected points]{style="color:grey;"} are generated from the thinned PP with intensity $\color{red}{\tilde{\lambda}(s)}= \lambda(s)\color{red}{g(d(s))}$
    -   The **log intensity** $\log \color{red}{\tilde{\lambda}(s)} = \overbrace{\log \lambda (s)}^{\mathbf{x}'\beta + \xi(s)} + \overbrace{\log \color{red}{g(d(s))}}^{-0.5~d(\mathbf{s})^2\sigma^{-2}}$
    -   The **encounter rate**, i.e. the number of observed animals within a distance $W$ follows $m \sim \text{Poisson} \left(\int_0^W \tilde{\lambda}(d)\text{d}d\right)$
:::
:::::

-   The pdf of detected *distances* is $\pi(d_1,\ldots,d_m|m) \propto \prod_{i=1}^m\dfrac{\tilde{\lambda}(d_i)}{\int_0^W \tilde{\lambda}(d)\text{d}d}$

## Putting all the pieces together {.smaller auto-animate="true" background-color="#FFFFFF"}

::::: columns
::: {.column width="40%"}
![](figures/thinweeds2-1.png){fig-align="center"}
:::

::: {.column width="60%"}
-   The true point pattern $Y = \mathbf{s}_1,\ldots,\mathbf{s}_n$ are a realization of a Point process with intensity $\lambda(s)$
-   We design a sampling survey to collect the data along transects
-   [detected points]{style="color:grey;"} are generated from the thinned PP with intensity $\color{red}{\tilde{\lambda}(s)}= \lambda(s)\color{red}{g(d(s))}$
    -   The **log intensity** $\log \color{red}{\tilde{\lambda}(s)} = \overbrace{\log \lambda (s)}^{\mathbf{x}'\beta + \xi(s)} + \overbrace{\log \color{red}{g(d(s))}}^{-0.5~d(\mathbf{s})^2\sigma^{-2}}$
    -   The **encounter rate**, i.e. the number of observed animals within a distance $W$ follows $m \sim \text{Poisson} \left(\int_0^W \tilde{\lambda}(d)\text{d}d\right)$
:::
:::::

-   The pdf of detected *distances* is $\pi(d_1,\ldots,d_m|m) \propto \prod_{i=1}^m\dfrac{ g(d_i)}{\int_o^W g(d) \text{d}d}$ if $\color{red}{\tilde{\lambda}(d_i)} = \lambda \color{red}{g(d_i)}$

## An approximation: Strips as lines

-   If the strips width ( $2W$ ) is narrow compared to study region ($\Omega$) we can treat them as lines.

    -   We need to adjust the intensity at a *point* $\mathbf{s}$ on the line to take account of the actual width of the strip

    -   Adjust the thinning probability to account for having collapsed all points onto the line.

## An approximation: Strips as lines {.smaller}

The intensity at a *point* $\mathbf{s}$ on the line becomes $2W\lambda(s)$ instead of $\lambda(s)$.

-   Let $\pi(d)$ be the probability that the point is at a distance $d$ from the line.

-   Let $p(d)$ be the probability that is detected given it is at $d$.

Then, the thinning probability becomes $\pi(d)\times p(d)$, assuming the points are uniformly distributed within the strip then $\pi(d) = 1/W$ (the density of distances is assumed to be constant on the interval $[0,W]$).

This updates our thinning intensity to

$$
\log \tilde{\lambda}(s) = \underbrace{\mathbf{x}'\beta + \xi(s)}_{\log \lambda(s)} + \log p(d) + \log \times(2/2W)
$$

-   Typically $p(d)$ is a non-linear function, that is where `inlabru` can help via a **Fixed point iteration scheme** (further details available in this [vignette](https://cran.r-project.org/web/packages/inlabru/vignettes/method.html))

::: notes
$$
\mathbb{E}[N_{\text{det}}] = 2W \lambda(s) \times \frac{1}{W} \int_0^W p(r)  dr = 2\lambda(s) \int_0^W p(r)  dr
$$
:::

# Example

## Example: Dolphins in the Gulf of Mexico {.smaller}

In the next example, we will explore data from a combination of several NOAA shipboard surveys conducted on pan-tropical spotted dolphins in the Gulf of Mexico.

-   A total of 47 observations of groups of dolphins were detected. The group size was recorded, as well as the Beaufort sea state at the time of the observation.

-   Transect width is 16 km, i.e. maximal detection distance 8 km (transect half-width 8 km).

```{r}
#| echo: false
#| message: false
#| eval: true
#| out-width: 100%

library(mapview)
mapviewOptions(basemaps = c( "OpenStreetMap.DE"))
mexdolphin <- mexdolphin_sf

mapview(mexdolphin$points,zcol="size")+
  mapview(mexdolphin$samplers)+
 mapview(mexdolphin$ppoly )

mesh = fm_mesh_2d(boundary = mexdolphin$ppoly,
                    max.edge = c(30, 150),
                    cutoff = 15,
                    crs = fm_crs(mexdolphin$points))

```

## Step 1: Define the SPDE representation: The mesh {.smaller auto-animate="true" background-color="#FFFFFF"}

First, we need to create the mesh used to approximate the random field. We can either:

1.  Create a nonconvex extension of the points using the `fm_mesh_2d` and `fm_nonconvex_hull` functions from the `fmesher` package:

```{r}
#| message: false
#| warning: false
library(fmesher)

boundary0 = fm_nonconvex_hull(mexdolphin$points,convex = -0.1)

mesh_0 = fm_mesh_2d(boundary = boundary0,
                          max.edge = c(30, 150), # The largest allowed triangle edge length.
                          cutoff = 15,
                          crs = fm_crs(mexdolphin$points))
```

::::: columns
::: {.column width="60%"}
```{r}
#| echo: false
  ggplot() + gg(mesh_0) +geom_sf(data=mexdolphin$points)
```
:::

::: {.column width="40%"}
-   `max.edge` for maximum triangle edge lengths
-   `cutoff` to avoid overly small triangles in clustered areas
:::
:::::

## Step 1: Define the SPDE representation: The mesh {.smaller auto-animate="true" background-color="#FFFFFF"}

First, we need to create the mesh used to approximate the random field. We can either:

2.  Use a pre-define `sf` boundary and specify this directly into the mesh construction via the `fm_mesh_2d` function

```{r}
#| message: false
#| warning: false
library(fmesher)


mesh_1 = fm_mesh_2d(boundary = mexdolphin$ppoly,
                    max.edge = c(30, 150),
                    cutoff = 15,
                    crs = fm_crs(mexdolphin$points))

```

::::: columns
::: {.column width="60%"}
```{r}
#| echo: false
  ggplot() + gg(mesh_1) +geom_sf(data=mexdolphin$points)
```
:::

::: {.column width="40%"}
-   `max.edge` for maximum triangle edge lengths
-   `cutoff` to avoid overly small triangles in clustered areas
:::
:::::

## Step 1: Define the SPDE representation: The mesh

::: incremental
-   All random field models need to be discretised for practical calculations.

-   The SPDE models were developed to provide a consistent model definition across a range of discretisations.

-   We use finite element methods with local, piecewise linear basis functions defined on a triangulation of a region of space containing the domain of interest.

-   Deviation from stationarity is generated near the boundary of the region.

-   The choice of region and choice of triangulation affects the numerical accuracy.
:::

## Step 1: Define the SPDE representation: The mesh

-   Too fine meshes $\rightarrow$ heavy computation

-   Too coarse mesh $\rightarrow$ not accurate enough

![](figures/mesh_res.png){fig-align="center"}

## Step 1: Define the SPDE representation: The mesh

**Some guidelines**

-   Create triangulation meshes with `fm_mesh_2d()`:

-   edge length should be around a third to a tenth of the spatial range

-   Move undesired boundary effects away from the domain of interest by extending to a smooth external boundary:

-   Use a coarser resolution in the extension to reduce computational cost (`max.edge=c(inner, outer)`), i.e., add extra, larger triangles around the border

## Step 1: Define the SPDE representation: The mesh

-   Use a fine resolution (subject to available computational resources) for the domain of interest (inner correlation range) and avoid small edges ,i.e., filter out small input point clusters (0 $<$ `cutoff` $<$ inner)

-   Coastlines and similar can be added to the domain specification in `fm_mesh_2d()` through the `boundary` argument.

-   simplify the border

## Step 1: Define the SPDE representation: The SPDE {background-color="#FFFFFF"}

We use the `inla.spde2.pcmatern` to define the SPDE model using PC priors through the following probability statements

::::: columns
::: {.column width="40%"}
-   $P(\rho < 50) = 0.1$

-   $P(\sigma > 2) = 0.1$
:::

::: {.column width="60%"}
```{r}
spde_model =  inla.spde2.pcmatern(
  mexdolphin$mesh,
  prior.sigma = c(2, 0.1),
  prior.range = c(50, 0.1)
)
```
:::
:::::

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center


dens_prior_range = function(rho_0, p_alpha)
{
  # compute the density of the PC prior for the
  # range rho of the Matern field
  # rho_0 and p_alpha are defined such that
  # P(rho<rho_0) = p_alpha
  rho = seq(0, rho_0*10, length.out =100)
  alpha1_tilde = -log(p_alpha) * rho_0
  dens_rho =  alpha1_tilde / rho^2 * exp(-alpha1_tilde / rho)
  return(data.frame(x = rho, y = dens_rho))
}

dens_prior_sd = function(sigma_0, p_sigma)
{
  # compute the density of the PC prior for the
  # sd sigma of the Matern field
  # sigma_0 and p_sigma are defined such that
  # P(sigma>sigma_0) = p_sigma
  sigma = seq(0, sigma_0*5, length.out =100)
  alpha2_tilde = -log(p_sigma)/sigma_0
  dens_sigma = alpha2_tilde* exp(-alpha2_tilde * sigma) 
  return(data.frame(x = sigma, y = dens_sigma))
}

ggplot() + geom_line(data = dens_prior_range(50,.1), aes(x,y))+ labs(y="",x="range",title="Prior for the range") +
ggplot() + geom_line(data = dens_prior_sd(1,.1), aes(x,y)) + labs(y="",x="sd",title="Prior for the sd")

```

## Step 2: Define the Detection function {.smaller background-color="#FFFFFF"}

We start by plotting the distances and histogram of frequencies in distance intervals.

```{r}
#| echo: false
#| fig-align: center
#| fig-height: 3
#| fig-width: 5
W <- 8
ggplot(mexdolphin$points) +
  geom_histogram(aes(x = distance),
    breaks = seq(0, W, length.out = 9),
    boundary = 0, fill = NA, color = "black"
  ) +
  geom_point(aes(x = distance), y = 0, pch = "|", cex = 4)

```

Then, we need to define a half-normal detection probability function. This must take distance as its first argument and the linear predictor of the sigma parameter as its second:

```{r}
# define detection function
hn <- function(distance, sigma) {
  exp(-0.5 * (distance / sigma)^2)
}
```

## Example: Dolphins in the Gulf of Mexico {.smaller background-color="#FFFFFF"}

::::: columns
::: {.column width="60%"}
**The LGCP Model**

$$
\begin{aligned}
p(\mathbf{y} | \lambda)  & \propto \exp \left( -\int_\Omega \lambda(\mathbf{s}) p(\mathbf{s}) \mathrm{d}\mathbf{s} \right) \prod_{i=1}^n \lambda(\mathbf{s}_i) p(\mathbf{s}_i)) \\
\eta(s) &  = \color{#FF6B6B}{\boxed{\beta_0}} + \color{#FF6B6B}{\boxed{ \omega(s)}} + \color{#FF6B6B}{\boxed{ \log p(s)}} \\
\end{aligned}
$$

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "1-7"

# define model component
cmp = ~ Intercept(1) + 
  space(main = geometry, model = spde_model) +
  sigma(1,
    prec.linear = 1,
    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)
  ) 

# define model predictor
eta  = geometry + distance ~ space +
  log(hn(distance, sigma)) +
  Intercept + log(2)

# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = mexdolphin$points,
              ips = ips)

# fit the model
fit = bru(cmp, lik)
```
:::

::: {.column width="40%"}
The **integration scheme**

```{r}
#| echo: true


# build integration scheme
distance_domain <-  fm_mesh_1d(seq(0, 8,
                              length.out = 30))
ips = fm_int(list(geometry = mesh,
                  distance = distance_domain),
             samplers = mexdolphin$samplers)
```

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4

ggplot()  + geom_sf(data = ips, aes(color = weight),size=0.5) + 
   scale_color_scico(palette = "roma") + gg(mesh) 

```
:::
:::::

::: notes
The samplers in this dataset are lines, not polygons, so we need to tell `inlabru` about the strip half-width, W, which in the case of these data is 8.

To control the prior distribution for the $\sigma$ parameter, we use a transformation mapper that converts a latent variable into an exponentially distributed variable with expectation 8

The `marginal` argument in the sigma component specifies the transformation function taking N(0,1) to Exponential(1/8).
:::

## Example: Dolphins in the Gulf of Mexico {.smaller background-color="#FFFFFF"}

::::: columns
::: {.column width="60%"}
**The LGCP Model**

$$
\begin{aligned}
p(\mathbf{y} | \lambda)  & \propto \exp \left( -\int_\Omega \lambda(\mathbf{s}) p(\mathbf{s}) \mathrm{d}\mathbf{s} \right) \prod_{i=1}^n \lambda(\mathbf{s}_i) p(\mathbf{s}_i)) \\
\color{#FF6B6B}{\boxed{\eta(s)}} &  = \color{#FF6B6B}{\boxed{\beta_0 +  \omega(s) +  \log p(s)}}\\
\end{aligned}
$$

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "9-12"

# define model component
cmp = ~ Intercept(1) + 
  space(main = geometry, model = spde_model) +
  sigma(1,
    prec.linear = 1,
    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)
  ) 

# define model predictor
eta  = geometry + distance ~ space +
  log(hn(distance, sigma)) +
  Intercept + log(2)

# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = mexdolphin$points,
              ips = ips)

# fit the model
fit = bru(cmp, lik)
```
:::

::: {.column width="40%"}
The **integration scheme**

```{r}
#| echo: true

# build integration scheme
distance_domain <-  fm_mesh_1d(seq(0, 8,
                              length.out = 30))
ips = fm_int(list(geometry = mesh,
                  distance = distance_domain),
             samplers = mexdolphin$samplers)
```

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4

ggplot()  + geom_sf(data = ips, aes(color = weight),size=0.5) + 
   scale_color_scico(palette = "roma") + gg(mesh) 

```
:::
:::::

::: notes
we need an offset due to the unknown direction of the detections
:::

## Example: Dolphins in the Gulf of Mexico {.smaller background-color="#FFFFFF"}

::::: columns
::: {.column width="60%"}
**The LGCP Model**

$$
\begin{aligned}
\color{#FF6B6B}{\boxed{p(\mathbf{y} | \lambda)}} & \propto \exp \left( -\int_\Omega \lambda(\mathbf{s}) p(\mathbf{s}) \mathrm{d}\mathbf{s} \right) \prod_{i=1}^n \lambda(\mathbf{s}_i) p(\mathbf{s}_i)) \\
\eta(s) &  = \beta_0 +  \omega(s) +  \log p(s) \\
\end{aligned}
$$

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "13-18"

# define model component
cmp = ~ Intercept(1) + 
  space(main = geometry, model = spde_model) +
  sigma(1,
    prec.linear = 1,
    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)
  ) 

# define model predictor
eta  = geometry + distance ~ space +
  log(hn(distance, sigma)) +
  Intercept + log(2)

# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = mexdolphin$points,
              ips = ips)

# fit the model
fit = bru(cmp, lik)
```
:::

::: {.column width="40%"}
The **integration scheme**

```{r}
#| echo: true

# build integration scheme
distance_domain <-  fm_mesh_1d(seq(0, 8,
                              length.out = 30))
ips = fm_int(list(geometry = mesh,
                  distance = distance_domain),
             samplers = mexdolphin$samplers)
```

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4

ggplot()  + geom_sf(data = ips, aes(color = weight),size=0.5) + 
   scale_color_scico(palette = "roma") + gg(mesh) 

```
:::
:::::

## Example: Dolphins in the Gulf of Mexico {.smaller background-color="#FFFFFF"}

::::: columns
::: {.column width="60%"}
**The LGCP Model**

$$
\begin{aligned}
\color{#FF6B6B}{\boxed{p(\mathbf{y} | \lambda)}} & \propto \exp \left( -\int_\Omega \lambda(\mathbf{s}) p(\mathbf{s}) \mathrm{d}\mathbf{s} \right) \prod_{i=1}^n \lambda(\mathbf{s}_i) p(\mathbf{s}_i)) \\
\eta(s) &  = \beta_0 +  \omega(s) +  \log p(s) \\
\end{aligned}
$$

**The code**

```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false
#| code-line-numbers: "20-21"

# define model component
cmp = ~ Intercept(1) + 
  space(main = geometry, model = spde_model) +
  sigma(1,
    prec.linear = 1,
    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)
  ) 

# define model predictor
eta  = geometry + distance ~ space +
  log(hn(distance, sigma)) +
  Intercept + log(2)

# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = mexdolphin$points,
              ips = ips)

# fit the model
fit = bru(cmp, lik)
```
:::

::: {.column width="40%"}
The **integration scheme**

```{r}
#| echo: true

# build integration scheme
distance_domain <-  fm_mesh_1d(seq(0, 8,
                              length.out = 30))
ips = fm_int(list(geometry = mesh,
                  distance = distance_domain),
             samplers = mexdolphin$samplers)
```

```{r}
#| echo: false
#| fig-width: 5
#| fig-height: 4

ggplot()  + geom_sf(data = ips, aes(color = weight),size=0.5) + 
   scale_color_scico(palette = "roma") + gg(mesh) 

```
:::
:::::

# Results

## Results: posterior summaries {.smaller background-color="#FFFFFF"}

```{r}
#| echo: false

# define model component
cmp = ~ Intercept(1) + 
  space(main = geometry, model = spde_model) +
  sigma(1,
    prec.linear = 1,
    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)
  ) 

# define model predictor
eta  = geometry + distance ~ space +
  log(hn(distance, sigma)) +
  Intercept + log(2)

# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = mexdolphin$points,
              ips = ips)

# fit the model
fit = bru(cmp, lik)
```

::::: columns
::: {.column width="40%"}
We can use the `fit$summary.fixed` and `summary.hyperpar` to obtain posterior summaries of the mdoel parameters.

```{r}
#| echo: false
rbind(fit$summary.fixed[,c(1,3,5)],
      fit$summary.hyperpar[,c(1,3,5)]) %>% gt(rownames_to_stub = TRUE) %>% fmt_number(decimals=2)

```
:::

::: {.column width="60%"}
The `spde.posterior` allow us to plot the posterior density of the Matern field parameters

```{r}
spde.posterior(fit, "space", what = "range") %>% plot()
```
:::
:::::

## Results: posterior summaries {.smaller background-color="#FFFFFF"}

```{r}
#| echo: false

# define model component
cmp = ~ Intercept(1) + 
  space(main = geometry, model = spde_model) +
  sigma(1,
    prec.linear = 1,
    marginal = bru_mapper_marginal(qexp, pexp, dexp, rate = 1 / 8)
  ) 

# define model predictor
eta  = geometry + distance ~ space +
  log(hn(distance, sigma)) +
  Intercept + log(2)

# build the observation model
lik = bru_obs("cp",
              formula = eta,
              data = mexdolphin$points,
              ips = ips)

# fit the model
fit = bru(cmp, lik)
```

::::: columns
::: {.column width="40%"}
We can use the `fit$summary.fixed` and `summary.hyperpar` to obtain posterior summaries of the model parameters.

```{r}
#| echo: false
rbind(fit$summary.fixed[,c(1,3,5)],
      fit$summary.hyperpar[,c(1,3,5)]) %>% gt(rownames_to_stub = TRUE) %>% fmt_number(decimals=2)

```
:::

::: {.column width="60%"}
The `spde.posterior` allow us to plot the posterior density of the Matern field parameters

```{r}
spde.posterior(fit, "space", what = "log.variance") %>% plot()
```
:::
:::::

## Results: predicted densities {.smaller background-color="#FFFFFF"}

::::: columns
::: {.column width="40%"}
To map the spatial intensity we first need to define a grid of points where we want to predict.

-   We do this using the function `fm_pixel()` which creates a regular grid of points covering the mesh
-   Then, we use the `predict` function which takes as input
    -   the fitted model (`fit`)
    -   the prediction points (`pxl`)
    -   the model components we want to predict (e.g., $e^{\beta_0 + \xi(s)}$)
:::

::: {.column width="60%"}
```{r}
#| message: false
#| warning: false
library(patchwork)
pxl <- fm_pixels(mesh, dims = c(200, 100), mask = mexdolphin$ppoly)
pr.int <- predict(fit, pxl, ~ data.frame(spatial = space,
                                      loglambda = Intercept + space,
                                      lambda = exp(Intercept + space)))
```

```{r}
#| eval: false
ggplot() +
  gg(pr.int$spatial, geom = "tile")
```

```{r}
#| echo: false
ggplot() +
  gg(pr.int$spatial, geom = "tile") +
  scale_fill_scico(palette = "roma")+
ggplot() +
  gg(pr.int$loglambda, geom = "tile") +
  scale_fill_scico(palette="imola",name=expression(log(lambda)))+
ggplot() +
gg(pr.int$loglambda, geom = "tile",aes(fill = sd)) +
  scale_fill_scico(name=expression(sd~log(lambda)))+
ggplot() +
  gg(pr.int$lambda, geom = "tile") +
  scale_fill_scico(name=expression(lambda))+ plot_layout(ncol=1)+ 
  plot_layout(ncol=2)
```
:::
:::::

## Results: predicted densities {.smaller background-color="#FFFFFF"}

::::: columns
::: {.column width="60%"}
We can also use the `predict` the detection function:

```{r}
#| echo: true
distdf <- data.frame(distance = seq(0, 8, length.out = 100))
dfun <- predict(fit, distdf, ~ hn(distance, sigma))
```
:::

::: {.column width="40%"}
:::

```{r}
plot(dfun)
```
:::::

## Results: predicted expected counts {.smaller background-color="#FFFFFF"}

We can look at the posterior for the mean expected number of dolphins. Remember, the number of dolphins over the whole domain $\Omega$ is $$
N(\Omega)\sim\text{Poisson}(E_{\Omega}), \text{ with } E_{\Omega} = \int_{\Omega}\lambda(s)ds
$$ so the expected mean number of dolphins is $$
E_{\Omega} = \int_{\Omega}\lambda(s)ds = \int_{\Omega}\exp\{\beta_0+\omega(s)\}ds
$$

```{r}
#| echo: true
predpts <- fm_int(mexdolphin$mesh, mexdolphin$ppoly)
Lambda <- generate(fit, predpts, ~ sum(weight * exp(space + Intercept)),
                   n.samples = 3000)

```

Estimate:

```{r}
m = mean(Lambda[1,])
data.frame(ll = Lambda[1,]) %>% ggplot() + geom_density(aes(x = ll, y = after_stat(density))) +
  geom_vline(aes(xintercept = mean(ll), color = "Posterior Mean")) + theme(legend.title = element_blank()) +
  ggtitle("Mean expected number of dolphins", subtitle = "Posterior distribution")

```

## Results: predicted expected counts {.smaller background-color="#FFFFFF"}

If want to predict the expected counts We can also get Monte Carlo samples for the expected number of dolphins as follows:

::::: columns
::: {.column width="60%"}
```{r}
Ns <- seq(50, 650, by = 1)

Nests = predict(fit, predpts,
             ~{lambda = sum(weight * exp(space + Intercept))
               Ns <- Ns
               list(dens = dpois(Ns, lambda))
             },n.samples = 3000)

Nests_gen = generate(fit, predpts,
             ~{lambda = sum(weight * exp(space + Intercept))
               Ns <- Ns
               list(dens = dpois(Ns, lambda))
             },n.samples = 10)

dim(sapply(Nests_gen, function(x)x$dens))
NN_gen = data.frame(Ns = Ns, 
                    sapply(Nests_gen, function(x)x$dens)) %>%
  pivot_longer(-Ns)

data.frame(ll = Lambda[1,]) %>% ggplot() + geom_density(aes(x = ll, y = after_stat(density))) +
  geom_vline(aes(xintercept = mean(ll), color = "Posterior Mean")) + theme(legend.title = element_blank()) +
  ggtitle("Mean expected number of dolphins", subtitle = "Posterior distribution")+ geom_line(data = Nests$dens %>% mutate(N = Ns), aes(Ns, mean))



ggplot() + geom_line(data = Nests$dens %>% mutate(N = Ns), aes(Ns, mean))  +
  geom_ribbon(data = Nests$dens %>% mutate(N = Ns), aes(Ns, ymin = q0.025, ymax = q0.975), alpha = 0.5) +
  geom_line(data = NN_gen, aes(Ns, value, group = name))



```
:::

::: {.column width="40%"}
```{r}
#| echo: false



```
:::
:::::
