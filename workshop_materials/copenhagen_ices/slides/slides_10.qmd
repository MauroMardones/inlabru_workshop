---
title: "Lecture 10"
from: markdown+emoji
subtitle: "Model Comparison and Model evaluation" 
format:
  revealjs:
    margin: 0
    logo:  NTNU_UofG.png
    theme: uofg_theme.scss
    header-includes: |
      <script src="custom.js" type="application/javascript"></script>
slide-number: "c/t"
title-slide-attributes:
#    data-background-image: images/trondheim3.png
    data-background-size: cover
    data-background-opacity: "0.55"
author:
  - name: Sara Martino
    #orcid: 0000-0002-6879-4412
    email: sara.martino@ntnu.no
    affiliations: Dept. of Mathematical Science, NTNU
  - name: Janine Illian
    #orcid: 0000-0002-6879-4412
    email: Janine.Illian@glasgow.ac.uk
    affiliations: University of Glasgow 
  - name: Jafet Belmont
    #orcid: 0000-0002-6879-4412
    email: Jafet.Belmont@glasgow.ac.uk
    affiliations: University of Glasgow   
        
# date: May 22, 2025
# bibliography: references.bib
embed-resources: true
execute:
  allow-html: true
  freeze: auto
---

```{r setup}
# #| include: false

knitr::opts_chunk$set(echo = FALSE,
                      message=FALSE,
                      warning=FALSE,
                      strip.white=TRUE,
                      prompt=FALSE,
                      fig.align="center",
                       out.width = "60%")

library(knitr)    # For knitting document and include_graphics function
library(ggplot2)  # For plotting
library(png)
library(tidyverse)
library(INLA)
library(BAS)
library(patchwork)
library(DAAG)
library(inlabru)
library(cowplot) # needs install.packages("magick") to draw images

```

## Motivation

<p style="font-size: 1.5em; text-align: center; margin-top: 150px;">

“All models are wrong, some models are useful” — George Box

</p>

. . .

1.  How do we check that our model fits our data? - **Model Validation**

. . .

2.  How do we choose the best model? - **Model Comparison**

. . .

But..it is not always easy to distinguish between the two!

# Bayesian model comparison

## Bayesian Model Comparison

-   There is *no golden standard*
-   It really depends what you want to do!
-   Basically two types
    -   Ones that look at the posterior probability of the data under the model
    -   Ones that look at how model the data fits the data
-   In general *it is not an easy task*
-   `inlabru` provides some options available

. . .

## Model Comparison/Validation in `inlabru`{.smaller}

-   Criteria of fit

    -   Marginal likelihood ⇒Bayes factors
    -   Deviance information criterion (DIC)
    -   Widely applicable information criterion (WAIC)

-   There are also some predictive checks for the model:

    -   Conditional predictive ordinate (CPO)
    -   Probability integral transform (PIT)

-   You can sample from the posterior using `generate()` and compute

    -   log-scores
    -   CRPS
    -   ...

# Criteria of fit

## Marginal likelihood

```{r}
#| echo: false
#| eval: true

x = rnorm(100)
y = 1 + 0.6 * x+ rnorm(100)
df = data.frame(x,y)
cmp = ~ Intercept(1) + cov(x, model = "linear")
lik = bru_obs(formula = y~.,
              data = df)
```

```{r}
#| echo: true
#| eval: true

# tell inlabru you want to compute mlik
bru_options_set(control.compute = list(mlik = TRUE))
fit = bru(cmp, lik)
# see the results
fit$mlik
```

<hr>

-   Calculates $\log(\pi(\mathbf{y}))$
-   Can calculate Bayes factors through differences in value
-   **NB:** Problematic for intrinsic models

## Deviance Information Criteria (DIC)

```{r}
#| echo: true
#| eval: true

# tell inlabru you want to compute DIC
bru_options_set(control.compute = list(dic = TRUE))
fit = bru(cmp, lik)
# see the results
fit$dic$dic
```

<hr>

-   Measure of complexity and fit.
-   Defined as: $\text{DIC} = \bar{D} + p_D$
    -   $\bar{D}$ is the posterior mean of the deviance
    -   $p_D$ is the effective number of parameters.
-   Smaller values indicate better trade-off between complexity and fit.

## Widely applicable information criterion (WAIC)

also known as *Watanabe–Akaike information criterion*.

```{r}
#| echo: true
#| eval: true

# tell inlabru you want to compute WAIC
bru_options_set(control.compute = list(waic = TRUE))
fit = bru(cmp, lik)
# see the results
fit$waic$waic
```

<hr>

-   similar to the DIC...but maybe better[^1]
-   Linked to the leave-one-out crossvalidation
-   Smaller values indicate better trade-off between complexity and fit

[^1]: See "Understanding predictive information criteria for Bayesian models" (2013) by Andrew Gelman, Jessica Hwang, and Aki Vehtari

# Posterior predictive checks

## Posterior predictive distribution

**Remember:** Our GLM is defined as: $$
\begin{eqnarray}
\pi(\mathbf{y}|\mathbf{u},\theta) & =\prod_i \pi(y_i|\mathbf{u},\theta)& \ \text{  likelihood}\\
\pi(\mathbf{u}|\theta)& &\ \text{  LGM}\\
\pi(\theta )& &\ \text{  hyperprior}\\
\end{eqnarray}
$$ using `inlabru` we estimate the *posterior distribution* $\pi(\mathbf{u},\theta|\mathbf{y})$.

The *posterior predictive distribution* for a new data $\hat{y}$ is then: $$
\pi(\hat{y}|\mathbf{y}) = \int\pi(y_i|\mathbf{u},\theta)\pi(\mathbf{u},\theta|\mathbf{y})\ d\mathbf{u}\ d\theta
$$

This ditribution can be used to check the model fit!

## Posterior predictive distribution {.smaller}

$$
\pi(\hat{y}|\mathbf{y}) = \int\pi(y_i|\mathbf{u},\theta)\pi(\mathbf{u},\theta|\mathbf{y})\ d\mathbf{u}\ d\theta
$$

**NOTE:** In general this is NOT computed by `inlabru` but needs to be approximated

-   Use generate to sample from the posterior $(\mathbf{u}^*_i,\theta^*_i)\sim\pi(\mathbf{u},\theta|\mathbf{y})$

-   Simulate a new datapoint $y^*_i\sim(y_i|\mathbf{u}^*_i,\theta^*_i)$

-   Use $y^*_1,\dots, y^*_N$ to approximate the posterior predictive distribution.

. . .

**BUT** `inlabru` computes automatically two quantities that are useful for model check!

-   Conditional predictive ordinate (CPO)
-   Probability integral transform (PIT)

## Conditional predictive ordinate (CPO)

Definition: $$
cpo_i = \pi(y^{obs}_i|\mathbf{y}_{-i})
$$

-   Introduced in Pettit (1990)[^2]
-   Measures fit through the predictive density
-   Can be used to compute the log-score as $$
    \text{Score} = -\sum \log(cpo_i)
    $$ lower score correspond to better models

[^2]: Pettit, L. I. 1990. “The Conditional Predictive Ordinate for the Normal Distribution.” Journal of the Royal Statistical Society. Series B (Methodological)

## Probability integral transform (PIT)

How to compute:

```{r}
#| echo: true
#| eval: true

# tell inlabru you want to compute DIC
bru_options_set(control.compute = list(cpo = TRUE))
fit = bru(cmp, lik)
# see the results
head(fit$cpo$cpo)

```

**Note** it is possible to check for possible fails in computed CPOs

```{r}
#| echo: true
#| eval: true

head(fit$cpo$failure)
```

## Probability integral transform (PIT)

Definition: $$
pit_i = \text{Prob}(\hat{y}_i<y_i|\mathbf{y}_{-i})
$$

-   Linked to leave-one-out cross-validation

-   $pit_i$ shows how well the ith data point is predicted by the rest of the data

-   Very small values indicate "suprising" observation under the model

-   For well-calibrated, the PIT values should be approximately uniformly distributed.

## Probability integral transform (PIT)

How to compute:

```{r}
#| echo: true
#| eval: true

# tell inlabru you want to compute DIC
bru_options_set(control.compute = list(cpo = TRUE))
fit = bru(cmp, lik)
# see the results
head(fit$cpo$pit)

```

## Good and Bad PIT plots

```{r}
#| echo: false
#| eval: true

n = 1000

df = data.frame(id = 1:n,
           Good = runif(n),
           Underdispersed = rbeta(n,shape1 = 2,shape2 = 2),
           Overdispersed = rbeta(n,shape1 = 0.5,shape2 = 0.5),
           Biased = rbeta(n,shape1 = 1,shape2 = 2)) %>% 
  pivot_longer(-id) %>%
  mutate(name = factor(name, 
                     levels = c("Good", "Underdispersed", "Overdispersed", "Biased")) ) 
df %>%
  ggplot() + geom_histogram(aes(x = value, y = ..density..), bins = 20) +
  facet_wrap(~name) + xlab("") + ylab("")
```

## Other scores {.smaller}

In the literature there are many proposed scores for evaluate predictions. For example:

-   Dawid-Sebastian score
-   Log-score
-   Continuous rank probility score (CRPS)
-   Brier score
-   ...

They all have their strength and wakness and which one is better depends on the goals of the model.

`inlabru` does not provide such scores automatcally, but they can be computed using simulations from the posterior distribution.

## Example: CRPS for Poisson data

Our model: $$
\begin{eqnarray}
y_i|\lambda_i & \sim \text{Poisson}(\lambda_i),&\ i = 1,\dots,N_{\text{data}}\\
\log(\lambda_i)  = \eta_i &= \beta_0 + \beta_1 x_i
\end{eqnarray}
$$

Simulate data and fit the model:

```{r}
#| echo: true
#| eval: true

df_pois <- data.frame(
  x = rnorm(50),
  y = rpois(length(x), exp(2 + 1 * x))
)
cmp = ~ Intercept(1) + cov(x, model = "linear")
lik = bru_obs(formula = y~.,
              family = "poisson",
              data = df_pois)
fit_pois <- bru(cmp, lik)

```

## Example: CRPS for Poisson data {.smaller}

The CRPS score is defined as: $$
\text{S}_{\text{CRPS}}(F_i, y_i) = \sum_{k=0}^\infty\left[\text{Prob}(Y_i\leq k|\mathbf{y})-I(y_i\leq k)\right]^2
$$

Computational algorithm:

1.  Simulate $\lambda^{(j)}\sim p(\lambda|\text{data}), j = 1,\dots, N_{\text{samples}}$ using `generate()` (size $N\times N_\text{samples}$).

2.  For each $i=1,\dots,N_{\text{data}}$, estimate $r_{ik}=\text{Prob}(Y\leq k|\text{data})-I(y_i\leq k)$ as $$
      \hat{r}_{ik} = \frac{1}{N_\text{samples}} \sum_{j=1}^{N_\text{samples}}
      \{
      \text{Prob}(Y\leq k|\lambda^{(j)}_i)-I(y_i\leq k)
      \} .
    $$

3.  Compute $$
      S_\text{CRPS}(F_i,y_i) = \sum_{k=0}^{K} \hat{r}_{ik}^2
    $$

## Example: CRPS for Poisson data {.smaller}

Implementation:

```{r}
#| echo: true
#| eval: true

# some large value, so that 1-F(K) is small
max_K <- ceiling(max(df_pois$y) + 4 * sqrt(max(df_pois$y)))
k <- seq(0, max_K)
kk <- rep(k, times = length(df_pois$y))
i <- seq_along(df_pois$y)
pred_pois <- generate(fit_pois, df_pois,
  formula = ~ {
    lambda <- exp(Intercept + x)
    ppois(kk, lambda = rep(lambda, each = length(k)))
  },
  n.samples = 2000
)
results <- data.frame(
  i = rep(i, each = length(k)),
  k = kk,
  Fpred = rowMeans(pred_pois),
  residuals =
    rowMeans(pred_pois) - (rep(df_pois$y, each = length(k)) <= kk)
)

crps_scores <-
  (results %>%
    group_by(i) %>%
    summarise(crps = sum(residuals^2), .groups = "drop") %>%
    pull(crps))
summary(crps_scores)
```

# ..but how about residuals??

## Residuals: Frequentist vs Bayesian {.smaller}

::::: columns
::: {.column width="50%"}
### :dart: Frequentist View

-   Model parameters are **fixed but unknown**.
-   Fitted values (predictions): $\hat{y}_i$ are point estimates
-   **Residuals**:\
    $$
    r_i = y_i - \hat{y}_i
    $$
-   Single number per data point.
-   Used for:
    -   Checking model fit / outliers
:::

::: {.column width="50%"}
### :crystal_ball: Bayesian View
:::
:::::

## Residuals: Frequentist vs Bayesian {.smaller}

::::: columns
::: {.column width="50%"}
### :dart: Frequentist View

-   Model parameters are **fixed but unknown**.
-   Fitted values (predictions): $\hat{y}_i$ are point estimates
-   **Residuals**:\
    $$
    r_i = y_i - \hat{y}_i
    $$
-   Single number per data point.
-   Used for:
    -   Checking model fit / outliers
:::

::: {.column width="50%"}
### :crystal_ball: Bayesian View

-   Parameters are **random variables** with posterior $\pi(\theta \mid y)$.
-   Predictions $\tilde{y}_i$ also have a **posterior distribution**.
-   No single “true” fitted value → residuals are **not uniquely defined**.
    -   $r_i^{(\text{mean})} = (y_i - E[\tilde{y}_i \mid y])$ (mean residual)\
    -   $r_i^{(\text{sample})} = (y_i - \tilde{y}^s)$ for posterior sample $s$
    -   Distribution of $y_i - \tilde{y}_i^{(s)}$ (posterior residuals)
:::
:::::

. . .

<hr>

A better option is to use posterior predictive checks[^3]

[^3]: See Gelman et. al (2020) "Bayesian Workflow"

## One example of posterior predictive checks

::::: columns
::: {.column width="50%"}
$$
y_i|\eta_i\sim\mathcal{N}(\eta_i, \sigma^2)
$$

-   **Model 1** $$
    \eta_i = \beta_0 + \beta_1 x_i
    $$
-   **Model 2** $$
    \eta_i = \beta_0 +  f(x_i)
    $$
:::

::: {.column width="50%"}
```{r}
#| echo: false
N = 100
df = data.frame(x = runif(100, 0 , 100)) %>%
  mutate(y = log(x) + rnorm(100, sd = 0.5))

# fit model 1
cmp = ~ Intercept(1) + cov(x, model = "linear")
lik = bru_obs(formula = y~.,
              data = df)
fit = bru(cmp, lik)

# fit model 2
df$x_group = inla.group(df$x, n = 30)
cmp1 = ~ Intercept(1) + cov(x_group, model = "rw2", scale.model = TRUE)
lik1 = bru_obs(formula = y~.,
              data = df)
fit1 = bru(cmp1, lik1)


p1 = predict(fit, df, ~ Intercept + cov)
p2 = predict(fit1, df, ~ Intercept + cov)

```

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 4
#| out-width: "100%"
df %>% ggplot() + geom_point(aes(x,y)) +
  geom_line(data = p1, aes(x,mean, color = "Model 1")) +
  geom_ribbon(data = p1, aes(x, ymin = q0.025 , ymax = q0.975,
                             fill = "Model 1"), alpha = 0.5) +
  geom_line(data = p2, aes(x_group,mean, color = "Model 2")) +
  geom_ribbon(data = p2, aes(x_group, ymin = q0.025 , ymax = q0.975,
                             fill = "Model 2"), alpha = 0.5) +
  xlab("") + ylab("") +
  
  scale_color_manual(
    name = "Model",        # shared legend title
    values = c("Model 1" = "steelblue", "Model 2" = "tomato")
  ) +
  scale_fill_manual(
    name = "Model",        # must match the color scale name
    values = c("Model 1" = "steelblue", "Model 2" = "tomato")
  ) 
  


```
:::
:::::

## One example of posterior predictive checks

1.  Sample $y^{1k}_i\sim\pi(y_i|\mathbf{y})$ $k = 1,\dots,M$ using `generate()`

```{r}
preds =  generate(fit, df,
  formula = ~ {
    mu <- (Intercept + cov)
    sd <- sqrt(1 / Precision_for_the_Gaussian_observations)
    rnorm(100, mean = mu, sd = sd)
  },
  n.samples = 500
)


mat <- round(preds,2)

# Add row and column names
rownames(mat) <- paste0("r", 1:nrow(mat))
colnames(mat) <- paste0("c", 1:ncol(mat))

# Extract the first 3 and last rows
mat_head <- head(mat, 2)
mat_tail <- tail(mat, 1)

# Create an ellipsis row (with row name)
ellipsis_row <- matrix("...", nrow = 1, ncol = ncol(mat))
rownames(ellipsis_row) <- "..."

# Combine them
mat_display <- rbind(mat_head, ellipsis_row, mat_tail)
mat_display = cbind(mat_display[,c(1:4)], rep("...",4), mat_display[,500])
rownames(mat_display) <- c("$y_1$",  "$y_3$", "...", "$y_N$")
colnames(mat_display) <- c("1", "2", "3", "4", "...", "M")

# Pretty-print table
knitr::kable(
  mat_display,
  align = "c",
  caption = ""
)%>% 
  kableExtra::add_header_above(c(" " = 1, "Samples" = ncol(mat_display))) %>%
  kableExtra::kable_styling(full_width = FALSE, position = "center")





```

2.  Compare some summaries of the simulated data with the one of the observed one

## One example of posterior predictive checks

Here we compare the estimated posterior densities $\hat{\pi}^k(y|\mathbf{y})$ with the estimated data density

```{r}
preds = data.frame(preds) %>% 
  mutate(id = 1:100) %>%
  pivot_longer(-id)

preds1 =  generate(fit1, df,
  formula = ~ {
    mu <- (Intercept + cov)
    sd <- sqrt(1 / Precision_for_the_Gaussian_observations)
    rnorm(100, mean = mu, sd = sd)
  },
  n.samples = 500
)
preds1 = data.frame(preds1) %>% 
  mutate(id = 1:100) %>%
  pivot_longer(-id)

p1 = ggplot() + geom_density(data = preds, 
                        aes(value, group = name),  color = "#E69F00") +
  geom_density(data = df, aes(y))  +
  xlab("") + ylab("") + ggtitle("Model 1")

p2 = ggplot() + geom_density(data = preds1, 
                        aes(value, group = name),  color = "#E69F00") +
  geom_density(data = df, aes(y))  +
  xlab("") + ylab("") + ggtitle("Model 2")
p1 + p2

```

. . .

This is just a simple example, but more complex checks can be computed with the same idea!

## Leave Group Our Cross-Validation

This is a new option for cross-validation in `inlabru` . . .

-   Leave-one-out cross-validation (LOOCV) is a very common technique to evaluate predictions from models
-   When data are correlated (as in spatial statistics) LOOCV might be too optimistic and overestimate model performances.

. . .

-   One possible solution is to then remove "chunck(s)" of data (for example one station and all its nearest neighbours)
-   This is the solution implemented in the `inla.group.cv()` function[^4]

[^4]: Adin & al (2024) Automatic cross-validation in structured models: Is it time to leave out leave-one-out?, Spat. Statistics and Liu & al. (2022) Leave-group-out cross-validation for latent Gaussian models

# Model validation for LGCP

## Model validation for LGCP

Point processes (and so LGCP) are different from all other spatial models:

-   The data are *presence* **and** *absence* of points
-   The likelihood depends on the data location **and** the integrated intensity over the whole domain

. . .

This makes model evaluation especially challenging. Especially cross-validation based measures are hard to define... why?

## Cross validation for point processes

::::: columns
::: {.column width="50%"}
-   In a point process empty areas are also "data"
-   We cannot just remove points as this will change the underlying intensity
-   We need to remove a whole subdomain in order to cross-validate
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 3.5
#| out-width: "95%"


# Simulate a 2D Non-Homogeneous Poisson Process via thinning
simulate_nhpp_2d <- function(lambda_fun, x_max, y_max, lambda_max) {
  # Step 1: simulate candidate points
  area <- x_max * y_max
  n_points <- rpois(1, lambda_max * area)
  x_cand <- runif(n_points, 0, x_max)
  y_cand <- runif(n_points, 0, y_max)
  
  # Step 2: compute acceptance probabilities
  accept_prob <- lambda_fun(x_cand, y_cand) / lambda_max
  
  # Step 3: thin
  keep <- runif(n_points) < accept_prob
  x <- x_cand[keep]
  y <- y_cand[keep]
  
  data.frame(x = x, y = y)
}

# Example intensity function: λ(x, y) = 100 * exp(-((x-0.5)^2 + (y-0.5)^2)/0.05)
lambda_fun <- function(x, y) {
  300 * exp(-((x - 0.5)^2 + (y - 0.5)^2) / 0.05)
}

# Simulate on [0,1] x [0,1]
set.seed(42)
points <- simulate_nhpp_2d(lambda_fun, x_max = 1, y_max = 1, lambda_max = 500)

# Plot result

xy = expand.grid(seq(0,1,length.out = 50), seq(0,1,length.out = 50)) %>%
  data.frame() %>%
  mutate(z = lambda_fun(Var1,Var2))
library(scico)
p1 = xy %>% ggplot() + geom_tile(aes(Var1, Var2, fill = z), alpha = 0.8) + 
  scale_fill_scico(direction = -1) + coord_equal() +
  geom_point(data = points, aes(x,y)) + xlab("") + ylab("") + theme(legend.position = "none")

b1 = data.frame(x = c(0.3,0.5,0.5,0.3),
                y = c(0.3,0.3,0.5,0.5))

points = points %>% mutate( sel = ifelse((x>.3& x<.5 & y>.3 & y<.5), 0,1))
p1 + geom_text(data = points %>% filter(sel==0), aes(x,y), label = "X", color = "red")
p1 + geom_polygon(data = b1, aes(x,y), fill = "white")

```
:::
:::::

## A warning note!

**Warning**:warning:

> Do not use WAIC and DIC as computed *today* by `inlabru` to compare LGCP models

-   WAIC is linked to leave-one-out crossvalidation therefore it is ill-defined for point processes
-   DIC is ok "in theory" but not the way it is computed today

. . .

Work is going on about this and measures of fit for LGCP will soon be available in `inlabru`

## Some ideas on what one can do

-   It is possible to define residuals for PP, for example as: $$
    \hat{R}_B = n(B)- \int_B\hat{\lambda}(s)\ ds
    $$ where
-   $n(B)$ is the number of observed points in $B$
-   $\hat{\lambda}(s)$ is the estimated intensity

These residuals can be used to evaluate the model.

. . .

This is still work in progress and at the moment not easily available... but it will be :grinning:

[Here](https://inlabru-org.github.io/inlabru/articles/2d_lgcp_residuals_sf.html) you can see some examples of computation and use.

# Conclusion

## Take home messages

-   Model check and model comparison are complex topics
-   There are no universal solutions, it all depends on which model characteristics you are interested in.
-   `inlabru` provides some easy to compute alternatives
-   LGCP require own tools to validate the model
    -   work is ongoing here... stay tuned :grinning:
