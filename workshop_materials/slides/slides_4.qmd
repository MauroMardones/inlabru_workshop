---
title: "Lecture 3"
from: markdown+emoji
subtitle: "Temporal Models and Smoothing" 
format:
  revealjs:
    margin: 0
    logo:  NTNU_UofG.png
    theme: uofg_theme.scss
    header-includes: |
      <script src="custom.js" type="application/javascript"></script>
slide-number: "c/t"
title-slide-attributes:
#    data-background-image: images/trondheim3.png
    data-background-size: cover
    data-background-opacity: "0.55"
author:
  - name: Sara Martino
    #orcid: 0000-0002-6879-4412
    email: sara.martino@ntnu.no
    affiliations: Dept. of Mathematical Science, NTNU
  - name: Janine Illian
    #orcid: 0000-0002-6879-4412
    email: Janine.Illian@glasgow.ac.uk
    affiliations: University of Glasgow 
  - name: Jafet Belmont
    #orcid: 0000-0002-6879-4412
    email: Janine.Illian@glasgow.ac.uk
    affiliations: University of Glasgow   
        
# date: May 22, 2025
# bibliography: references.bib
embed-resources: true
execute:
  allow-html: true
---

```{r setup}
# #| include: false

knitr::opts_chunk$set(echo = FALSE,
                      message=FALSE,
                      warning=FALSE,
                      strip.white=TRUE,
                      prompt=FALSE,
                      fig.align="center",
                       out.width = "60%")

library(knitr)    # For knitting document and include_graphics function
library(ggplot2)  # For plotting
library(png)
library(tidyverse)
library(INLA)
library(BAS)
library(patchwork)
library(DAAG)
library(inlabru)
library(cowplot) # needs install.packages("magick") to draw images

```

## Motivation {.smaller}

 Data are often observed in time, and time dependence is often expected.

```{r}
#| fig-width: 8
#| fig-height: 6
#| fig-align: center
lakes = as.data.frame(greatLakes ) %>% mutate(year = 1918:2009)
lakes %>% 
  pivot_longer(-year) %>%
  ggplot() + geom_point(aes(year, value ))+
  facet_wrap(.~name,scales = "free" ) + xlab("") + ylab("")
```

. . . 



 - Observations are correlated in time
 - We also have correlations between the time series (will look at the later...)


## Motivation {auto-animate="true"}



1. Smoothing of the time effect 



```{r}
lakes = as.data.frame(greatLakes ) %>% mutate(year = 1918:2009)

cmp = ~ -1 + Intercept(1) + time(year, model = "rw2", 
                                 scale.model = T,
                                 hyper = list(prec = list(prior = "pc.prec", param = c(1, 0.01))))

lik = bru_obs(formula = Erie~.,
              data = lakes)
out = bru(cmp, lik)
pred = predict(out, lakes, ~ Intercept + time)

pred %>% ggplot() + 
  geom_point(data = lakes, aes(year, Erie)) + ylab("") + xlab("")
```


## What is our goal? {auto-animate="true"}

1. Smoothing of the time effect 

```{r}

pred %>% ggplot() + geom_line(aes(year, mean)) +
  geom_ribbon(aes(year, ymin = q0.025, ymax = q0.975), alpha = 0.4 ) + 
  geom_point(data = lakes, aes(year, Erie)) + ylab("") + xlab("")
```

. . .

**Note:** We can use the same model to smooth covariate effects!

## What is our goal? {auto-animate="true"}


1. Smoothing of the time effect 

2. Prediction 

```{r}


question <- readPNG("figures/question.png", native = TRUE)


p1 = pred %>% ggplot() + geom_line(aes(year, mean)) +
  geom_ribbon(aes(year, ymin = q0.025, ymax = q0.975), alpha = 0.4 ) + 
  geom_point(data = lakes, aes(year, Erie)) + ylab("") + xlab("") + xlim(1919,2030)
ggdraw() +  draw_plot(p1) + draw_image(question, scale = .4, y = 0.15, x=0.35)

```
. . . 

We can  "predict" any unobserved data, does not have to be in the future  

## Modeling time with INLA{auto-animate="true"}

Time can be indexed over a 

- Discrete domain (e.g., years) 

- Continuous domain


## Modeling time with INLA{auto-animate="true"}

Time can be indexed over a 

- Discrete domain (e.g., years) 
    
  - Main models:  RW1,  RW2 and AR1
  
  - **Note:** RW1 and RW2  are also used for smoothing covariates
    
- Continuous domain
  
  - Here we use the so-called SPDE-approach


# Discrete time modelling

## Example - (log) Number of Air Passengers in time
```{r}

data("AirPassengers")
df = data.frame(y = log(as.numeric(AirPassengers)),
                year = rep(1949:1960, each =12),
                month = rep(1:12, 12)) %>%
  mutate(date = as.Date(paste(year, "-" , month, "-01", sep="")))



df_preds = data.frame(y = NA,
                      year = rep(1961,12),
                      month = 1:12) %>%
  mutate(date = as.Date(paste(year, "-" , month, "-01", sep="")))

df= rbind(df, df_preds) %>%
  mutate(time = seq_along(y))
df %>% ggplot() + geom_point(aes(date, y)) + 
  ylab("log(n.passenger)") + xlab("Time")




if(0)
{
cmp = ~ Intercept(1)+year(year, model = "linear") + time(time, model = "rw2")
lik = bru_obs(formula = y~.,
              family = "gaussian",
              data = df)

fit = bru(cmp, lik)

preds = predict(fit, df, ~Intercept+year + time)



cmp2 = ~ Intercept(1)+year(year, model = "linear") + seas(month, model = "rw2", cyclic = T)
lik2 = bru_obs(formula = y~.,
              family = "gaussian",
              data = df)

fit2 = bru(cmp2, lik2)




preds2 = predict(fit2, df, ~Intercept+year + seas)


ppreds = rbind(cbind(preds, model = 1),
               cbind(preds2, model = 2))
ppreds %>% 
  filter(!is.na(y)) %>%
  ggplot() + geom_line(aes(time, mean, group = model, color = factor(model))) + 
   geom_ribbon(aes(time, ymin = q0.025, ymax = q0.975, group = model, fill = factor(model) ), alpha =  0.5) 

}

```


**Goal** we want understand the pattern and predict into the future
    
## Random Walk models {.smaller}


Random walk models  encourage the mean of the linear predictor 
to vary gradually over time.

. . . 

They do this by assuming that, on average, thetime effect at each point is the mean of the effect at the neighbouring points.


```{r}
# Example random walk data
dd <- data.frame(
  time = 0:6,
  y = (-3:3)/2 # example path
)

# Define a normal distribution at time = 3
dens <- data.frame(y = seq(-3, 3, length.out = 200))
dens$density <- dnorm(dens$y, mean = 0, sd = .2)

# scale density so it looks like a vertical shape at x = 3
scale_factor <- 0.5
dens$x <- 3 + dens$density * scale_factor

ggplot(dd, aes(time, y)) +
  # grey background
 
  
  # points
  geom_point(size = 2) +
  
  # connecting line between neighbors t=2 and t=4
  geom_line(data = dd %>% filter(time %in% c(2,4)), aes(time, y), color = "black") +
  
  # normal density "red blob"
  geom_polygon(data = dens,
               aes(x, y), fill = "red", alpha = 0.8) +
  
  # predicted mean point
  geom_point(aes(x = 3, y = 0), color = "blue", size = 3) +
  ylim(-2,2) + 
  # labels
  labs(title = "First Order Random Walk",
       x = "Time",
       y = "Mean Response")

```



. . . 

 - Random Walk of order 1 (RW1) we take the two nearest neighbors 
 
 - Random Walk of order 2 (RW2) we take the four nearest neighbors 
 
 
 
 
## Random walks of order 1{auto-animate="true"}
**Idea:** $\longrightarrow\ u_t = \text{mean}(u_{t-1} , u_{t+1}) + \text{Gaussian error with precision  } \tau$

. . .

**Definition**

$$
  \pi(\mathbf{u} \mid \tau) \propto
  \exp\!\left(
     -\frac{\tau}{2} \sum_{t=1}^{T-1} (u_{t+1} - u_t)^2
  \right) = \exp\!\left(-\tfrac{1}{2} \, \mathbf{u}^{\top} \mathbf{Q}\ \mathbf{u}\right),
$$
where  the precision is
$\mathbf{Q} = \tau\mathbf{R}$ with

$$
    \mathbf{R} =
    \begin{bmatrix}
      1 & -1 &  &        &        &   \\
      -1 & 2 & -1 &        &        &   \\
         &    & \ddots & \ddots & \ddots &   \\
         &    &        & -1     & 2 & -1 \\
         &    &        &        & -1 & 1
    \end{bmatrix}
$$


## Random walks of order 1{auto-animate="true"}
**Idea:** $\longrightarrow\ u_t = \text{mean}(u_{t-1} , u_{t+1}) + \text{Gaussian error with precision  } \tau$


**Definition**

$$
  \pi(\mathbf{u} \mid \tau) \propto
  \exp\!\left(
     -\frac{\tau}{2} \sum_{t=1}^{T-1} (u_{t+1} - u_t)^2
  \right) = \exp\!\left(-\tfrac{1}{2} \, \mathbf{u}^{\top} \mathbf{Q}\ \mathbf{u}\right)
$$

  1. Role of the precision parameter $\tau$ and prior distribution
  2. RW as intrinsic model



## What is the role of the precision parameter?

- $\tau$ says how much $u_t$ can vary around its mean

  - Small $\tau$ $\rightarrow$ large variation $\rightarrow$ less smooth effect
  - Large $\tau$ $\rightarrow$ small variation $\rightarrow$ smoother effect
  
. . .  

```{r}
#| fig-align: center
#| out-width: 40%

cmp = ~ -1 +  time(year, model = "rw2", constr = FALSE,
                            hyper = list(prec = list(initial = 0, fixed = T)))
cmp2 = ~ -1 +  time(year, model = "rw2", constr = FALSE,
                            hyper = list(prec = list(initial = 20, fixed = T)))
                                        

lik = bru_obs(formula = Erie~.,
              data = lakes)
out = bru(cmp, lik)
out2=bru(cmp2, lik)
ggplot() + geom_point(data = lakes, aes(year, Erie)) +
  geom_line(data= out$summary.random$time, aes(ID, mean, color = "small precision")) +
  geom_line(data= out2$summary.random$time, aes(ID, mean, color = "large precision")) +
  ylab("") + xlab("") + theme(legend.title = element_blank())

```

. . .

We need to set a _prior distribution_ for $\tau$.

A common option are the so called _PC-priors_


## Penalized Complexity (PC) priors{ auto-animate="true"}

- PC priors are available in `inlabru` for many model parameters

. . . 


- They are built with two principles in mind:

   1. The prior discourage overdispersion by penalizing distance from a _base model_

::::: columns
::: {.column width="60%"}

```{r precs}
#| out-width: 100%
cmp = ~ -1 +  time(year, model = "rw2", constr = FALSE,
                            hyper = list(prec = list(initial = 10, fixed = T)))
cmp2 = ~ -1 +  time(year, model = "rw2", constr = FALSE,
                            hyper = list(prec = list(initial = 20, fixed = T)))
                                        

lik = bru_obs(formula = Erie~.,
              data = lakes)
out = bru(cmp, lik)
out2=bru(cmp2, lik)
ggplot() + geom_point(data = lakes, aes(year, Erie)) +
  geom_line(data= out$summary.random$time, aes(ID, mean, color = "small precision")) +
  geom_line(data= out2$summary.random$time, aes(ID, mean, color = "large precision")) +
  ylab("") + xlab("") + theme(legend.title = element_blank(), legend.position = "none")

```


:::
::: {.column width="40%"}

- A line is the _base model_

- We want to penalize more complex models

:::
:::::

   




  


## Penalized Complexity (PC) priors{ auto-animate="true"}

- PC prior are available in `inlabru` for many model parameters

- They are built with two principles in mind:

   1. The prior discourage overdispersion by penalizing distance from a _base model_
   2. User-defined scaling
   
$$
\text{Prob}\left(\sqrt{\frac{1}{\tau}}>U\right) = \text{Prob}(\sigma>U) = \alpha; \qquad U>0, \ \alpha \in (0,1)
$$

  - $U$  an upper limit for the standard deviation and $\alpha$  a small probability.

  - $U$  a likely value for the standard deviation and $\alpha=0.5$.


  



## Example

::: columns
::: {.column width="50%"}
**The Model**
$$
\begin{aligned}
y_i|\eta_i, \sigma^2 & \sim \mathcal{N}(\eta_i,\sigma^2)\\
\eta_i & = \beta_0 + f(t_i)\\
f(t_1),f(t_2),\dots,f(t_n) &\sim \text{RW2}(\tau)
\end{aligned}
$$
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out-width: 100%

lakes %>% ggplot() + geom_point(aes(year, Erie)) + xlab("") + ylab("")

```
:::
:::

::: columns
::: {.column width="60%"}
**The code**
```{r}
#| echo: true
#| eval: false
#| warning: false
#| message: false

cmp = ~ Intercept(1) + 
  time(year, model = "rw1",
       hyper = list(prec = 
                      list(prior = "pc.prec",
                           param = c(0.5, 0.5))))
```
:::
::: {.column width="40%"}

```{r}
#| fig-align: center
#| out-width: 100%
#| eval: true

cmp0 = ~ -1 +  time(year, model = "rw2", constr = FALSE,
                   scale.model = TRUE, 
                    hyper = list(prec = list(prior = "pc.prec", param = c(0.3,0.5))))
lik = bru_obs(formula = Erie~.,
              data = lakes)
out_pc0 = bru(cmp0, lik)


prior = data.frame(x = seq(0,9,0.01),
                   y = INLA::inla.pc.dprec(seq(0,9,0.01), 
                                           u  = 0.3, alpha = 0.5))
posterior = inla.tmarginal(function(x)1/sqrt(x),
                           out_pc0$marginals.hyperpar$`Precision for time`)

ggplot() + geom_line(data = prior, aes(x,y, color = "prior")) +
  geom_line(data = posterior, aes(x,y, color = "posterior"))

```
:::
:::


## RW as intrinsic models{.smaller}

RW1 defines differences, not absolute levels:

- Only the *changes* between neighbouring terms are modelled.

- The model has no information about the global mean (intercept).

- Mathematically,
$$
(u_1,\dots,u_n)\text{ and }(u_1+a,\dots,u_n+a)
$$
produce identical likelihoods — they’re indistinguishable.

. . .

This means:

- The precision matrix $\mathbf{Q}$ is singular.

- Posterior inference is not well-defined unless we fix the overall level.

. . .


 Solution:
  
  - Sum to zero constraint $\sum_{i = 1}^n u_i = 0$
  - This is included in the model by default
  

## Random walks of order 2

- Just like RW1, but now we consider 4 neighbours instead of 2
$$
u_t = \text{mean}(u_{t-2} ,u_{t-1} , u_{t+1}, u_{t+2} ) + \text{some Gaussian error with precision  } \tau
$$
- RW2 are smoother than RW1

- The precision has the same role as for RW1

# Example

::: columns
::: {.column width="60%"}
```{r}
#| echo: true
cmp1 = ~ Intercept(1) + 
  time(year, model = "rw1", 
       scale.model = T,
       hyper = list(prec = 
                      list(prior = "pc.prec",
                           param = c(0.3,0.5))))

cmp2 = ~ Intercept(1) + 
  time(year, model = "rw2",
       scale.model = T,
       hyper = list(prec = 
                      list(prior = "pc.prec", 
                           param = c(0.3,0.5))))


lik = bru_obs(formula = Erie~ ., 
              data = lakes)

fit1 = bru(cmp1, lik)
fit2 = bru(cmp2, lik)


```

:::
::: {.column width="40%"}
```{r}
#| echo: false
#| out-width: 100%
pred1 = predict(fit1, lakes, ~ Intercept + time)
pred2 = predict(fit2, lakes, ~ Intercept + time)

ggplot()+ geom_line(data = pred1, aes(year, mean, color = "rw1")) +
  geom_line(data = pred2, aes(year, mean, color = "rw2")) + ylab("") + xlab("") +
  theme(legend.title = element_blank(), legend.position = "bottom")

```

:::
:::

. . .

**NOTE:** the `scale.model = TRUE` option scales the $\mathbf{Q}$ matrix so that the precision parameter has the same interpretation in both models. 

## Summary RW (1 and 2) models

- Latent effects suitable for smoothing and modeling temporal data.

- Have one hyperparameter: the precision $\tau$
  
  - Use PC prior for $\tau$  

- Are an _intrinsic_ model

  - The precision matrix $\mathbf{Q}$ is rank deficient
  
  - A sum-to-zero constraint is added to make the model identifiable!

- RW2 models are smoother than RW1

## Auto Regressive Models of order 1 (AR1)

**Definition**

$$
u_t = \phi u_{t-i} + \epsilon_t; \qquad \phi\in(-1,1), \ \epsilon_t\sim\mathcal{N}(0,\tau^{-1})
$$
$$
\pi(\mathbf{u}|\tau)\propto\exp\left(-\frac{\tau}{2}\mathbf{u}^T\mathbf{Q}\mathbf{u}\right)
$$
with 
$$
    \mathbf{Q} =
    \begin{bmatrix}
      1 & -\phi &  &        &        &   \\
      -\phi & (1+\phi^2) & -\phi &        &        &   \\
         &    & \ddots & \ddots & \ddots &   \\
         &    &        & -\phi     & (1+\phi^2) & -\phi \\
         &    &        &        & -\phi & 1
    \end{bmatrix}
$$

## AR1: Hyperparameters and prior{auto-animate="true"}

The AR1 model has two parameters

  - The precision $\tau$
  - The autocorrelation (or persistence) parameter $\phi\in(0,1)$
  
## AR1: Hyperparameters and prior{auto-animate="true"}

The AR1 model has two parameters

  - The precision $\tau$
    - PC prior as before - Baseline $\tau=0$ `pc.prec`
$$
\text{Prob}(\sigma > u) = \alpha
$$
    
    
  - The autocorrelation (or persistence) parameter $\phi\in(-1,1)
    
    - Two choices of PC priors

::::: columns
::: {.column width="50%"}  

Baseline $\phi = 0$ `pc.cor0`

$$
\begin{eqnarray}
\text{Prob}(|\rho| > u) = \alpha;\\
-1<u<1;\ 0<\alpha<1
\end{eqnarray}
$$

:::
::: {.column width="50%"}  

```{r}
#| out-width: 100%
#| fig-align: "center"
data.frame(xx = seq(-1,1,0.001)) %>%
  mutate(yy1 = inla.pc.dcor0(xx, u = 0.07, alpha= 0.5),
  yy2 = inla.pc.dcor0(xx, u = 0.4, alpha= 0.5),
  yy3 = inla.pc.dcor0(xx, u = 0.7, alpha= 0.5)) %>%
  pivot_longer(-xx)%>%
  ggplot()  + geom_line(aes(xx,value, group = name, color = name)) + 
  xlab("") + ylab("")+ 
  scale_color_discrete(name="",
                         breaks=c("yy1", "yy2", "yy3"),
                         labels = list(
      expression(u == 0.07 ~ "," ~ alpha == 0.5),
      expression(u == 0.4~ "," ~ alpha == 0.5),
      expression(u == 0.7~ "," ~ alpha == 0.5)))+
  ylim(c(0,5))


```
:::
::::::


## AR1: Hyperparameters and prior{auto-animate="true"}

The AR1 model has two parameters

  - The precision $\tau$
    - PC prior as before - Baseline $\tau=0$ `pc.prec`
$$
\text{Prob}(\sigma > u) = \alpha
$$
    
    
    
 - The autocorrelation (or persistence) parameter $\phi\in(-1,1)
    
    - Two choices of PC priors

::::: columns
::: {.column width="50%"}  

Baseline $\phi = 1$ `pc.cor1`


$$
\begin{eqnarray}
\text{Prob}(\rho > u) = \alpha;&\\
-1<u<1;\qquad &\sqrt{\frac{1-u}{2}}<\alpha<1
\end{eqnarray}
$$
:::
::: {.column width="50%"}  

```{r}
#| out-width: 100%
#| fig-align: "center"
pp = data.frame(xx = seq(-1,1,0.001)) %>%
  mutate(yy1 = inla.pc.dcor1(xx, u = 0.07, alpha= 0.7),
  yy2 = inla.pc.dcor1(xx, u = 0.4, alpha= 0.8),
  yy3 = inla.pc.dcor1(xx, u = 0.2, alpha= 0.7)) %>%
  pivot_longer(-xx)
pp %>%
  ggplot()  + geom_line(aes(xx,value, group = name, color = name)) + 
  xlab("") + ylab("")+ scale_color_discrete(name="",
                         breaks=c("yy1", "yy2", "yy3"),
                         labels = list(
      expression(u == 0.07 ~ "," ~ alpha == 0.7),
      expression(u == 0.4~ "," ~ alpha == 0.8),
      expression(u == 0.7~ "," ~ alpha == 0.7)))+
  ylim(c(0,5))


```
:::
::::::

  
  